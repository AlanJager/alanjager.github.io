<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>花の様に</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="花の様に">
<meta property="og:url" content="http://hanayo.cn/page/4/index.html">
<meta property="og:site_name" content="花の様に">
<meta property="og:locale">
<meta property="article:author" content="Alan Jager">
<meta property="article:tag" content="ブログ">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="花の様に" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">花の様に</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://hanayo.cn"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Go-through-KVM-code-due-to-a-tdp-page-fault" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/07/15/Go-through-KVM-code-due-to-a-tdp-page-fault/" class="article-date">
  <time class="dt-published" datetime="2022-07-15T02:23:14.000Z" itemprop="datePublished">2022-07-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/virtualization/">virtualization</a>►<a class="article-category-link" href="/categories/virtualization/kvm/">kvm</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/07/15/Go-through-KVM-code-due-to-a-tdp-page-fault/">Go through KVM code due to a tdp_page_fault</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="What-happened"><a href="#What-happened" class="headerlink" title="What happened?"></a>What happened?</h2><p>Our CI/CD system ran integration test for every pull request but suddenly it met performance issue. Usually one round of integration test need 1h but this time almost all test do not finished after 1h 20min. </p>
<p>After check the codebase and test on lastest release stable branch, its more likely that the system met performance issue.</p>
<p>Before starting trip of “dig out the root cause”, check big picture of this CI/CD system architecture.</p>
<h2 id="Prepare-from-perf"><a href="#Prepare-from-perf" class="headerlink" title="Prepare from perf"></a>Prepare from perf</h2><p>Because integration test runs on virtual machine memory, check hypervisor’s performance might gave more details. So use perf to collect run time data for analysis.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;brendangregg&#x2F;FlameGraph  # or download it from github</span><br><span class="line">cd FlameGraph</span><br><span class="line">perf record -F 99 -a -g -- sleep 60</span><br><span class="line">perf script | .&#x2F;stackcollapse-perf.pl &gt; out.perf-folded</span><br><span class="line">.&#x2F;flamegraph.pl out.perf-folded &gt; perf.svg</span><br></pre></td></tr></table></figure>

<p>Check the flame graph</p>
<img src="/2022/07/15/Go-through-KVM-code-due-to-a-tdp-page-fault/2022-07-15-kvm-figure1.png" class=""> 


<h2 id="Start-from-tdp-page-fault"><a href="#Start-from-tdp-page-fault" class="headerlink" title="Start from tdp_page_fault"></a>Start from tdp_page_fault</h2><p>Abviously cpu spend lots of time to handle tdp_page_fault</p>
<p>find definition from <code>linux/arch/x86/kvm/mmu.c</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu)</span><br><span class="line">&#123;</span><br><span class="line">	struct kvm_mmu *context &#x3D; &amp;vcpu-&gt;arch.mmu;</span><br><span class="line"></span><br><span class="line">	context-&gt;base_role.word &#x3D; 0;</span><br><span class="line">	context-&gt;base_role.guest_mode &#x3D; is_guest_mode(vcpu);</span><br><span class="line">	context-&gt;base_role.smm &#x3D; is_smm(vcpu);</span><br><span class="line">	context-&gt;base_role.ad_disabled &#x3D; (shadow_accessed_mask &#x3D;&#x3D; 0);</span><br><span class="line">	context-&gt;page_fault &#x3D; tdp_page_fault;</span><br><span class="line">	context-&gt;sync_page &#x3D; nonpaging_sync_page;</span><br><span class="line">	context-&gt;invlpg &#x3D; nonpaging_invlpg;</span><br><span class="line">	context-&gt;update_pte &#x3D; nonpaging_update_pte;</span><br><span class="line">	context-&gt;shadow_root_level &#x3D; kvm_x86_ops-&gt;get_tdp_level(vcpu);</span><br><span class="line">	context-&gt;root_hpa &#x3D; INVALID_PAGE;</span><br><span class="line">	context-&gt;direct_map &#x3D; true;</span><br><span class="line">	context-&gt;set_cr3 &#x3D; kvm_x86_ops-&gt;set_tdp_cr3;</span><br><span class="line">	context-&gt;get_cr3 &#x3D; get_cr3;</span><br><span class="line">	context-&gt;get_pdptr &#x3D; kvm_pdptr_read;</span><br><span class="line">	context-&gt;inject_page_fault &#x3D; kvm_inject_page_fault;</span><br></pre></td></tr></table></figure>

<p><code>kvm_vcpu</code> <code>page_fault</code> point to <code>tdp_page_fault</code> when mmu field of <code>kvm_vcpu</code> is initializing.</p>
<p>from kvm vcpu setup <code>arch/x86/kvm/mmu.c</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">static void init_kvm_mmu(struct kvm_vcpu *vcpu)</span><br><span class="line">&#123;</span><br><span class="line">	if (mmu_is_nested(vcpu))</span><br><span class="line">		init_kvm_nested_mmu(vcpu);</span><br><span class="line">	else if (tdp_enabled)</span><br><span class="line">		init_kvm_tdp_mmu(vcpu);</span><br><span class="line">	else</span><br><span class="line">		init_kvm_softmmu(vcpu);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>from kvm vcpu setup <code>arch/x86/kvm/mmu.c</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">void kvm_mmu_setup(struct kvm_vcpu *vcpu)</span><br><span class="line">&#123;</span><br><span class="line">	MMU_WARN_ON(VALID_PAGE(vcpu-&gt;arch.mmu.root_hpa));</span><br><span class="line"></span><br><span class="line">	init_kvm_mmu(vcpu);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>from kvm vcpu setup <code>arch/x86/kvm/x86.c</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">int kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)</span><br><span class="line">&#123;</span><br><span class="line">	kvm_vcpu_mtrr_init(vcpu);</span><br><span class="line">	vcpu_load(vcpu);</span><br><span class="line">	kvm_vcpu_reset(vcpu, false);</span><br><span class="line">	kvm_mmu_setup(vcpu);</span><br><span class="line">	vcpu_put(vcpu);</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>vcpu is created by </p>
<p><code>kvm_vm_ioctl_create_vcpu(struct kvm *kvm, u32 id)</code></p>
<p>check mmu in vcpu structure:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line"> * Paging state of the vcpu</span><br><span class="line"> *</span><br><span class="line"> * If the vcpu runs in guest mode with two level paging this still saves</span><br><span class="line"> * the paging mode of the l1 guest. This context is always used to</span><br><span class="line"> * handle faults.</span><br><span class="line"> *&#x2F;</span><br><span class="line">struct kvm_mmu mmu;</span><br></pre></td></tr></table></figure>

<h2 id="Find-more-by-pf-interception"><a href="#Find-more-by-pf-interception" class="headerlink" title="Find more by pf_interception"></a>Find more by pf_interception</h2><p>combine to flage graph, pf_interception is before tdp_page_fault,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">static int pf_interception(struct vcpu_svm *svm)</span><br><span class="line">&#123;</span><br><span class="line">	u64 fault_address &#x3D; __sme_clr(svm-&gt;vmcb-&gt;control.exit_info_2);</span><br><span class="line">	u64 error_code &#x3D; svm-&gt;vmcb-&gt;control.exit_info_1;</span><br><span class="line"></span><br><span class="line">	return kvm_handle_page_fault(&amp;svm-&gt;vcpu, error_code, fault_address,</span><br><span class="line">			static_cpu_has(X86_FEATURE_DECODEASSISTS) ?</span><br><span class="line">			svm-&gt;vmcb-&gt;control.insn_bytes : NULL,</span><br><span class="line">			svm-&gt;vmcb-&gt;control.insn_len);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>refer to its usage:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[SVM_EXIT_EXCP_BASE + PF_VECTOR] &#x3D; pf_interception</span><br></pre></td></tr></table></figure>

<p>SVM_EXIT_EXCP_BASE is related to AMD CPU virtualization, PF_VECTOR means page_frame vector, used by page fault.</p>
<p>more details about the PF_VECTOR in <code>arch/x86/kvm/svm.c</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">if (npt_enabled) &#123;</span><br><span class="line">	&#x2F;* Setup VMCB for Nested Paging *&#x2F;</span><br><span class="line">	control-&gt;nested_ctl |&#x3D; SVM_NESTED_CTL_NP_ENABLE;</span><br><span class="line">	clr_intercept(svm, INTERCEPT_INVLPG);</span><br><span class="line">	clr_exception_intercept(svm, PF_VECTOR);</span><br><span class="line">	clr_cr_intercept(svm, INTERCEPT_CR3_READ);</span><br><span class="line">	clr_cr_intercept(svm, INTERCEPT_CR3_WRITE);</span><br><span class="line">	save-&gt;g_pat &#x3D; svm-&gt;vcpu.arch.pat;</span><br><span class="line">	save-&gt;cr3 &#x3D; 0;</span><br><span class="line">	save-&gt;cr4 &#x3D; 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>if AMD CPU’s npt not enabled, PF_VECTOR will be used to intercept page fault.</p>
<p>So just quickly go through guest virtual address translation.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line"> * Fetch a guest pte for a guest virtual address</span><br><span class="line"> *&#x2F;</span><br><span class="line">static int FNAME(walk_addr_generic)(struct guest_walker *walker,</span><br><span class="line">				    struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,</span><br><span class="line">				    gva_t addr, u32 access)</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">error:</span><br><span class="line">	errcode |&#x3D; write_fault | user_fault;</span><br><span class="line">	if (fetch_fault &amp;&amp; (mmu-&gt;nx ||</span><br><span class="line">			    kvm_read_cr4_bits(vcpu, X86_CR4_SMEP)))</span><br><span class="line">		errcode |&#x3D; PFERR_FETCH_MASK;</span><br><span class="line"></span><br><span class="line">	walker-&gt;fault.vector &#x3D; PF_VECTOR;</span><br><span class="line">	walker-&gt;fault.error_code_valid &#x3D; true;</span><br><span class="line">	walker-&gt;fault.error_code &#x3D; errcode;</span><br></pre></td></tr></table></figure>

<p>error is defined to raise PF_VECTOR when failed to find any PTE(Page table entry) </p>
<p>Than let’s find the next method <code>handle_exit()</code> from svm.c</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">static int handle_exit(struct kvm_vcpu *vcpu)</span><br></pre></td></tr></table></figure>

<p>following shows more details</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">struct vcpu_svm *svm &#x3D; to_svm(vcpu);</span><br><span class="line">struct kvm_run *kvm_run &#x3D; vcpu-&gt;run;</span><br><span class="line">u32 exit_code &#x3D; svm-&gt;vmcb-&gt;control.exit_code;</span><br></pre></td></tr></table></figure>

<p>the vcpu structure will be changed to vcpu_svm and than get the exit_code from it.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">trace_kvm_exit(exit_code, vcpu, KVM_ISA_SVM);</span><br><span class="line"></span><br><span class="line">if (!is_cr_intercept(svm, INTERCEPT_CR0_WRITE))</span><br><span class="line">	vcpu-&gt;arch.cr0 &#x3D; svm-&gt;vmcb-&gt;save.cr0;</span><br><span class="line">if (npt_enabled)</span><br><span class="line">	vcpu-&gt;arch.cr3 &#x3D; svm-&gt;vmcb-&gt;save.cr3;</span><br><span class="line"></span><br><span class="line">if (unlikely(svm-&gt;nested.exit_required)) &#123;</span><br><span class="line">	nested_svm_vmexit(svm);</span><br><span class="line">	svm-&gt;nested.exit_required &#x3D; false;</span><br><span class="line"></span><br><span class="line">	return 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>than the exit_code will be traced.</p>
<p>CR0 has various control flags that modify the basic operation of the processor. See more: <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Control_register#CR0">https://en.wikipedia.org/wiki/Control_register#CR0</a></p>
<p>if npt_enabled(CPU enable npt) vcpu will use vmcb saved cr3</p>
<p>vmcb: Intel VT-x name it as vmcs(virtual machine control structure), AMD name it as vmcb(virtual machine control block)</p>
<p>vmcb_control_area and vmcb_save_area combined as virtual machine control block. </p>
<p>note: need more research</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">if (is_guest_mode(vcpu)) &#123;</span><br><span class="line">	int vmexit;</span><br><span class="line"></span><br><span class="line">	trace_kvm_nested_vmexit(svm-&gt;vmcb-&gt;save.rip, exit_code,</span><br><span class="line">				svm-&gt;vmcb-&gt;control.exit_info_1,</span><br><span class="line">				svm-&gt;vmcb-&gt;control.exit_info_2,</span><br><span class="line">				svm-&gt;vmcb-&gt;control.exit_int_info,</span><br><span class="line">				svm-&gt;vmcb-&gt;control.exit_int_info_err,</span><br><span class="line">				KVM_ISA_SVM);</span><br><span class="line"></span><br><span class="line">	vmexit &#x3D; nested_svm_exit_special(svm);</span><br><span class="line"></span><br><span class="line">	if (vmexit &#x3D;&#x3D; NESTED_EXIT_CONTINUE)</span><br><span class="line">		vmexit &#x3D; nested_svm_exit_handled(svm);</span><br><span class="line"></span><br><span class="line">	if (vmexit &#x3D;&#x3D; NESTED_EXIT_DONE)</span><br><span class="line">		return 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>if vm is nested exit, handle nested exit next step, interrupts will be queued and if vm exit due to SVM_EXIT_ERR exit this thread.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">svm_complete_interrupts(svm);</span><br><span class="line"></span><br><span class="line">if (svm-&gt;vmcb-&gt;control.exit_code &#x3D;&#x3D; SVM_EXIT_ERR) &#123;</span><br><span class="line">	kvm_run-&gt;exit_reason &#x3D; KVM_EXIT_FAIL_ENTRY;</span><br><span class="line">	kvm_run-&gt;fail_entry.hardware_entry_failure_reason</span><br><span class="line">		&#x3D; svm-&gt;vmcb-&gt;control.exit_code;</span><br><span class="line">	pr_err(&quot;KVM: FAILED VMRUN WITH VMCB:\n&quot;);</span><br><span class="line">	dump_vmcb(vcpu);</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>last, check if the error code is external interrupt and not kernel handable error</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">if (is_external_interrupt(svm-&gt;vmcb-&gt;control.exit_int_info) &amp;&amp;</span><br><span class="line">    exit_code !&#x3D; SVM_EXIT_EXCP_BASE + PF_VECTOR &amp;&amp;</span><br><span class="line">    exit_code !&#x3D; SVM_EXIT_NPF &amp;&amp; exit_code !&#x3D; SVM_EXIT_TASK_SWITCH &amp;&amp;</span><br><span class="line">    exit_code !&#x3D; SVM_EXIT_INTR &amp;&amp; exit_code !&#x3D; SVM_EXIT_NMI)</span><br><span class="line">	printk(KERN_ERR &quot;%s: unexpected exit_int_info 0x%x &quot;</span><br><span class="line">	       &quot;exit_code 0x%x\n&quot;,</span><br><span class="line">	       __func__, svm-&gt;vmcb-&gt;control.exit_int_info,</span><br><span class="line">	       exit_code);</span><br><span class="line"></span><br><span class="line">if (exit_code &gt;&#x3D; ARRAY_SIZE(svm_exit_handlers)</span><br><span class="line">    || !svm_exit_handlers[exit_code]) &#123;</span><br><span class="line">	WARN_ONCE(1, &quot;svm: unexpected exit reason 0x%x\n&quot;, exit_code);</span><br><span class="line">	kvm_queue_exception(vcpu, UD_VECTOR);</span><br><span class="line">	return 1;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">return svm_exit_handlers[exit_code](svm);</span><br></pre></td></tr></table></figure>

<p>finally invoke svm exit handler</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">return svm_exit_handlers[exit_code](svm);</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2022/07/15/Go-through-KVM-code-due-to-a-tdp-page-fault/" data-id="cld1mhe9x0003yuwb8g646w4b" data-title="Go through KVM code due to a tdp_page_fault" class="article-share-link">Share</a>
      
      
        <a href="/2022/07/15/Go-through-KVM-code-due-to-a-tdp-page-fault/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2022/07/15/Go-through-KVM-code-due-to-a-tdp-page-fault/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kvm/" rel="tag">kvm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virt/" rel="tag">virt</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-python-elementtree-notes" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/07/01/python-elementtree-notes/" class="article-date">
  <time class="dt-published" datetime="2022-07-01T07:13:28.000Z" itemprop="datePublished">2022-07-01</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/languages/">languages</a>►<a class="article-category-link" href="/categories/languages/python/">python</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/07/01/python-elementtree-notes/">Python ElementTree notes</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Develop with libvrt python API, xml parse and operation is frequently required. ElementTree (stantard python library) is introduced in <a target="_blank" rel="noopener" href="https://realpython.com/python-xml-parser/">python-xml-parse</a> come into used for the sake of simplify xml configuration lifecycle handling.</p>
<p>This blog will go throught <a target="_blank" rel="noopener" href="https://docs.python.org/2/library/xml.etree.elementtree.html#module-xml.etree.ElementTree">xml.etree.ElementTree</a> combine with typical situations which is use as learning notes.</p>
<p>First, start with some basic concepts </p>
<blockquote>
</blockquote>
<p>The Element type is a flexible container object, designed to store hierarchical data structures in memory. The type can be described as a cross between a list and a dictionary.</p>
<blockquote>
</blockquote>
<p>Each element has a number of properties associated with it:</p>
<blockquote>
</blockquote>
<ul>
<li>a tag which is a string identifying what kind of data this element represents (the element type, in other words).<blockquote>
</blockquote>
</li>
<li>a number of attributes, stored in a Python dictionary.<blockquote>
</blockquote>
</li>
<li>a text string.<blockquote>
</blockquote>
</li>
<li>an optional tail string.<blockquote>
</blockquote>
</li>
<li>a number of child elements, stored in a Python sequence</li>
</ul>
<p>use following XML as sample data:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;data&gt;</span><br><span class="line">    &lt;country name&#x3D;&quot;Liechtenstein&quot;&gt;</span><br><span class="line">        &lt;rank&gt;1&lt;&#x2F;rank&gt;</span><br><span class="line">        &lt;year&gt;2008&lt;&#x2F;year&gt;</span><br><span class="line">        &lt;gdppc&gt;141100&lt;&#x2F;gdppc&gt;</span><br><span class="line">        &lt;neighbor name&#x3D;&quot;Austria&quot; direction&#x3D;&quot;E&quot;&#x2F;&gt;</span><br><span class="line">        &lt;neighbor name&#x3D;&quot;Switzerland&quot; direction&#x3D;&quot;W&quot;&#x2F;&gt;</span><br><span class="line">    &lt;&#x2F;country&gt;</span><br><span class="line">    &lt;country name&#x3D;&quot;Singapore&quot;&gt;</span><br><span class="line">        &lt;rank&gt;4&lt;&#x2F;rank&gt;</span><br><span class="line">        &lt;year&gt;2011&lt;&#x2F;year&gt;</span><br><span class="line">        &lt;gdppc&gt;59900&lt;&#x2F;gdppc&gt;</span><br><span class="line">        &lt;neighbor name&#x3D;&quot;Malaysia&quot; direction&#x3D;&quot;N&quot;&#x2F;&gt;</span><br><span class="line">    &lt;&#x2F;country&gt;</span><br><span class="line">    &lt;country name&#x3D;&quot;Panama&quot;&gt;</span><br><span class="line">        &lt;rank&gt;68&lt;&#x2F;rank&gt;</span><br><span class="line">        &lt;year&gt;2011&lt;&#x2F;year&gt;</span><br><span class="line">        &lt;gdppc&gt;13600&lt;&#x2F;gdppc&gt;</span><br><span class="line">        &lt;neighbor name&#x3D;&quot;Costa Rica&quot; direction&#x3D;&quot;W&quot;&#x2F;&gt;</span><br><span class="line">        &lt;neighbor name&#x3D;&quot;Colombia&quot; direction&#x3D;&quot;E&quot;&#x2F;&gt;</span><br><span class="line">    &lt;&#x2F;country&gt;</span><br><span class="line">&lt;&#x2F;data&gt;</span><br></pre></td></tr></table></figure>

<p>load xml from file:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Python 2.7.5 (default, Aug  4 2017, 00:39:18)</span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux2</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">&gt;&gt;&gt; import xml.etree.ElementTree as ET</span><br><span class="line">&gt;&gt;&gt; tree &#x3D; ET.parse(&#39;test_data.xml&#39;)</span><br><span class="line">&gt;&gt;&gt; root &#x3D; tree.getroot()</span><br><span class="line">&gt;&gt;&gt; root</span><br><span class="line">&lt;Element &#39;data&#39; at 0x7f58bd8232d0&gt;</span><br></pre></td></tr></table></figure>

<p>or load xml from string:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; test_data_str &#x3D; &#39;&#39;&#39;&lt;?xml version&#x3D;&quot;1.0&quot;?&gt;</span><br><span class="line">... &lt;data&gt;</span><br><span class="line">...     &lt;country name&#x3D;&quot;Liechtenstein&quot;&gt;</span><br><span class="line">...         &lt;rank&gt;1&lt;&#x2F;rank&gt;</span><br><span class="line">...         &lt;year&gt;2008&lt;&#x2F;year&gt;</span><br><span class="line">...         &lt;gdppc&gt;141100&lt;&#x2F;gdppc&gt;</span><br><span class="line">...         &lt;neighbor name&#x3D;&quot;Austria&quot; direction&#x3D;&quot;E&quot;&#x2F;&gt;</span><br><span class="line">...         &lt;neighbor name&#x3D;&quot;Switzerland&quot; direction&#x3D;&quot;W&quot;&#x2F;&gt;</span><br><span class="line">...     &lt;&#x2F;country&gt;</span><br><span class="line">...     &lt;country name&#x3D;&quot;Singapore&quot;&gt;</span><br><span class="line">...         &lt;rank&gt;4&lt;&#x2F;rank&gt;</span><br><span class="line">...         &lt;year&gt;2011&lt;&#x2F;year&gt;</span><br><span class="line">...         &lt;gdppc&gt;59900&lt;&#x2F;gdppc&gt;</span><br><span class="line">...         &lt;neighbor name&#x3D;&quot;Malaysia&quot; direction&#x3D;&quot;N&quot;&#x2F;&gt;</span><br><span class="line">...     &lt;&#x2F;country&gt;</span><br><span class="line">...     &lt;country name&#x3D;&quot;Panama&quot;&gt;</span><br><span class="line">...         &lt;rank&gt;68&lt;&#x2F;rank&gt;</span><br><span class="line">...         &lt;year&gt;2011&lt;&#x2F;year&gt;</span><br><span class="line">...         &lt;gdppc&gt;13600&lt;&#x2F;gdppc&gt;</span><br><span class="line">...         &lt;neighbor name&#x3D;&quot;Costa Rica&quot; direction&#x3D;&quot;W&quot;&#x2F;&gt;</span><br><span class="line">...         &lt;neighbor name&#x3D;&quot;Colombia&quot; direction&#x3D;&quot;E&quot;&#x2F;&gt;</span><br><span class="line">...     &lt;&#x2F;country&gt;</span><br><span class="line">... &lt;&#x2F;data&gt;&#39;&#39;&#39;</span><br><span class="line">&gt;&gt;&gt; ET.fromstring(test_data_str)</span><br><span class="line">&lt;Element &#39;data&#39; at 0x7f58bd823a10&gt;</span><br><span class="line">&gt;&gt;&gt; root &#x3D; ET.fromstring(test_data_str)</span><br><span class="line">&gt;&gt;&gt; root</span><br><span class="line">&lt;Element &#39;data&#39; at 0x7f58bd823f10&gt;</span><br></pre></td></tr></table></figure>

<p>As an element, use dir to check whats inside element we just loaded:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; dir(root)</span><br><span class="line">[&#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dict__&#39;, &#39;__doc__&#39;, &#39;__format__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__len__&#39;, &#39;__module__&#39;, &#39;__new__&#39;, &#39;__nonzero__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;, &#39;__weakref__&#39;, &#39;_children&#39;, &#39;append&#39;, &#39;attrib&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;extend&#39;, &#39;find&#39;, &#39;findall&#39;, &#39;findtext&#39;, &#39;get&#39;, &#39;getchildren&#39;, &#39;getiterator&#39;, &#39;insert&#39;, &#39;items&#39;, &#39;iter&#39;, &#39;iterfind&#39;, &#39;itertext&#39;, &#39;keys&#39;, &#39;makeelement&#39;, &#39;remove&#39;, &#39;set&#39;, &#39;tag&#39;, &#39;tail&#39;, &#39;text&#39;]</span><br></pre></td></tr></table></figure>

<p>as we see, operations is listed and think about some typical user case.</p>
<h2 id="find-attributes"><a href="#find-attributes" class="headerlink" title="find attributes"></a>find attributes</h2><p>Before finding attributes check what attributes the xml has.</p>
<p>For root node, only <data></data> has a tag but no attribute is set. So use tag and attrib to check this before find attributes.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; root.tag</span><br><span class="line">&#39;data&#39;</span><br><span class="line">&gt;&gt;&gt; root.attrib</span><br><span class="line">&#123;&#125;</span><br></pre></td></tr></table></figure>

<p>return value is what we expected. Get deeper, a country tag with name attribute is used. </p>
<p>iterate can be used to get child tag of root.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; for child in root:</span><br><span class="line">...     child</span><br><span class="line">...</span><br><span class="line">&lt;Element &#39;country&#39; at 0x7f58bd823f50&gt;</span><br><span class="line">&lt;Element &#39;country&#39; at 0x7f58bd825110&gt;</span><br><span class="line">&lt;Element &#39;country&#39; at 0x7f58bd825250&gt;</span><br></pre></td></tr></table></figure>

<p>or use index to find tag element directly:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; root[0]</span><br><span class="line">&lt;Element &#39;country&#39; at 0x7f58bd823f50&gt;</span><br></pre></td></tr></table></figure>

<p>for more duplicate case, those method became hard to use, so use iter or findall:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; for neighbor in root.iter(&#39;neighbor&#39;):</span><br><span class="line">...     print neighbor.attrib</span><br><span class="line">...</span><br><span class="line">&#123;&#39;direction&#39;: &#39;E&#39;, &#39;name&#39;: &#39;Austria&#39;&#125;</span><br><span class="line">&#123;&#39;direction&#39;: &#39;W&#39;, &#39;name&#39;: &#39;Switzerland&#39;&#125;</span><br><span class="line">&#123;&#39;direction&#39;: &#39;N&#39;, &#39;name&#39;: &#39;Malaysia&#39;&#125;</span><br><span class="line">&#123;&#39;direction&#39;: &#39;W&#39;, &#39;name&#39;: &#39;Costa Rica&#39;&#125;</span><br><span class="line">&#123;&#39;direction&#39;: &#39;E&#39;, &#39;name&#39;: &#39;Colombia&#39;&#125;</span><br></pre></td></tr></table></figure>

<p>all tags match neighbor is listed. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; for country in root.findall(&#39;country&#39;):</span><br><span class="line">...     rank &#x3D; country.find(&#39;rank&#39;).text</span><br><span class="line">...     name &#x3D; country.get(&#39;name&#39;)</span><br><span class="line">...     print name, rank</span><br><span class="line">...</span><br><span class="line">Liechtenstein 1</span><br><span class="line">Singapore 4</span><br><span class="line">Panama 68</span><br></pre></td></tr></table></figure>

<p>use find all, all tag with name country is found and its rank text and attribute name is listed.</p>
<p>change the parameters for test, change findall target, test if tag not matched what will happend:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; for country in root.findall(&#39;test&#39;):</span><br><span class="line">...     print country</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>when use find instead of findall</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; for tag in root.find(&#39;country&#39;):</span><br><span class="line">...     print tag</span><br><span class="line">...</span><br><span class="line">&lt;Element &#39;rank&#39; at 0x7f58bd823f90&gt;</span><br><span class="line">&lt;Element &#39;year&#39; at 0x7f58bd823fd0&gt;</span><br><span class="line">&lt;Element &#39;gdppc&#39; at 0x7f58bd825050&gt;</span><br><span class="line">&lt;Element &#39;neighbor&#39; at 0x7f58bd825090&gt;</span><br><span class="line">&lt;Element &#39;neighbor&#39; at 0x7f58bd8250d0&gt;</span><br></pre></td></tr></table></figure>

<p>only first matched result is returned.</p>
<p>if find for a unexists tag None will be returned.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; print root.find(&#39;test&#39;)</span><br><span class="line">None</span><br></pre></td></tr></table></figure>

<p>so in most cases, find and findall seems meet all the require for finding a specific tag.</p>
<h2 id="use-tag"><a href="#use-tag" class="headerlink" title="use tag"></a>use tag</h2><p>get attribute of tag:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; root.find(&#39;country&#39;).get(&#39;name&#39;)</span><br><span class="line">&#39;Liechtenstein&#39;</span><br></pre></td></tr></table></figure>

<p>get text inside tag:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; root.find(&#39;country&#39;).text</span><br><span class="line">&#39;\n        &#39;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; root.find(&#39;country&#39;).find(&#39;year&#39;).text</span><br><span class="line">&#39;2008&#39;</span><br></pre></td></tr></table></figure>

<p>list all children</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; root.getchildren()</span><br><span class="line">[&lt;Element &#39;country&#39; at 0x7f58bd825bd0&gt;, &lt;Element &#39;country&#39; at 0x7f58bd823f10&gt;, &lt;Element &#39;country&#39; at 0x7f58bd8239d0&gt;</span><br></pre></td></tr></table></figure>

<h2 id="insert-tag"><a href="#insert-tag" class="headerlink" title="insert tag"></a>insert tag</h2><p>create new element from string:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; new_element_str&#x3D;&#39;&#39;&#39;    &lt;country name&#x3D;&quot;China&quot;&gt;</span><br><span class="line">...         &lt;rank&gt;2&lt;&#x2F;rank&gt;</span><br><span class="line">...         &lt;year&gt;2022&lt;&#x2F;year&gt;</span><br><span class="line">...         &lt;neighbor name&#x3D;&quot;Japan&quot; direction&#x3D;&quot;E&quot;&#x2F;&gt;</span><br><span class="line">...     &lt;&#x2F;country&gt;&#39;&#39;&#39;</span><br><span class="line">(reverse-i-search)&#96;lo&#39;: &#123;&#39;name&#39;: &#39;Colombia&#39;, &#39;direction&#39;: &#39;E&#39;&#125;</span><br><span class="line">KeyboardInterrupt</span><br><span class="line">&gt;&gt;&gt; &#123;&#39;name&#39;: &#39;Colombia&#39;, &#39;direction&#39;: &#39;E&#39;&#125;</span><br><span class="line">&#123;&#39;direction&#39;: &#39;E&#39;, &#39;name&#39;: &#39;Colombia&#39;&#125;</span><br><span class="line">&gt;&gt;&gt; new &#x3D; ET.fromstring(new_element_str)</span><br></pre></td></tr></table></figure>

<p>check origin element tree:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; ET.tostring(root)</span><br><span class="line">&#39;&lt;data&gt;\n    &lt;country name&#x3D;&quot;Liechtenstein&quot;&gt;\n        &lt;rank&gt;1&lt;&#x2F;rank&gt;\n        &lt;year&gt;2008&lt;&#x2F;year&gt;\n        &lt;gdppc&gt;141100&lt;&#x2F;gdppc&gt;\n        &lt;neighbor direction&#x3D;&quot;E&quot; name&#x3D;&quot;Austria&quot; &#x2F;&gt;\n        &lt;neighbor direction&#x3D;&quot;W&quot; name&#x3D;&quot;Switzerland&quot; &#x2F;&gt;\n    &lt;&#x2F;country&gt;\n    &lt;country name&#x3D;&quot;Singapore&quot;&gt;\n        &lt;rank&gt;4&lt;&#x2F;rank&gt;\n        &lt;year&gt;2011&lt;&#x2F;year&gt;\n        &lt;gdppc&gt;59900&lt;&#x2F;gdppc&gt;\n        &lt;neighbor direction&#x3D;&quot;N&quot; name&#x3D;&quot;Malaysia&quot; &#x2F;&gt;\n    &lt;&#x2F;country&gt;\n    &lt;country name&#x3D;&quot;Panama&quot;&gt;\n        &lt;rank&gt;68&lt;&#x2F;rank&gt;\n        &lt;year&gt;2011&lt;&#x2F;year&gt;\n        &lt;gdppc&gt;13600&lt;&#x2F;gdppc&gt;\n        &lt;neighbor direction&#x3D;&quot;W&quot; name&#x3D;&quot;Costa Rica&quot; &#x2F;&gt;\n        &lt;neighbor direction&#x3D;&quot;E&quot; name&#x3D;&quot;Colombia&quot; &#x2F;&gt;\n    &lt;&#x2F;country&gt;\n&lt;&#x2F;data&gt;&#39;</span><br></pre></td></tr></table></figure>

<p>insert new element:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; ET.tostring(root)</span><br><span class="line">&#39;&lt;data&gt;\n    &lt;country name&#x3D;&quot;China&quot;&gt;\n        &lt;rank&gt;2&lt;&#x2F;rank&gt;\n        &lt;year&gt;2022&lt;&#x2F;year&gt;\n        &lt;neighbor direction&#x3D;&quot;E&quot; name&#x3D;&quot;Japan&quot; &#x2F;&gt;\n    &lt;&#x2F;country&gt;&lt;country name&#x3D;&quot;Liechtenstein&quot;&gt;\n        &lt;rank&gt;1&lt;&#x2F;rank&gt;\n        &lt;year&gt;2008&lt;&#x2F;year&gt;\n        &lt;gdppc&gt;141100&lt;&#x2F;gdppc&gt;\n        &lt;neighbor direction&#x3D;&quot;E&quot; name&#x3D;&quot;Austria&quot; &#x2F;&gt;\n        &lt;neighbor direction&#x3D;&quot;W&quot; name&#x3D;&quot;Switzerland&quot; &#x2F;&gt;\n    &lt;&#x2F;country&gt;\n    &lt;country name&#x3D;&quot;Singapore&quot;&gt;\n        &lt;rank&gt;4&lt;&#x2F;rank&gt;\n        &lt;year&gt;2011&lt;&#x2F;year&gt;\n        &lt;gdppc&gt;59900&lt;&#x2F;gdppc&gt;\n        &lt;neighbor direction&#x3D;&quot;N&quot; name&#x3D;&quot;Malaysia&quot; &#x2F;&gt;\n    &lt;&#x2F;country&gt;\n    &lt;country name&#x3D;&quot;Panama&quot;&gt;\n        &lt;rank&gt;68&lt;&#x2F;rank&gt;\n        &lt;year&gt;2011&lt;&#x2F;year&gt;\n        &lt;gdppc&gt;13600&lt;&#x2F;gdppc&gt;\n        &lt;neighbor direction&#x3D;&quot;W&quot; name&#x3D;&quot;Costa Rica&quot; &#x2F;&gt;\n        &lt;neighbor direction&#x3D;&quot;E&quot; name&#x3D;&quot;Colombia&quot; &#x2F;&gt;\n    &lt;&#x2F;country&gt;\n&lt;&#x2F;data&gt;</span><br></pre></td></tr></table></figure>

<p>confirm new element is added:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; for country in root.findall(&#39;country&#39;):</span><br><span class="line">...     country.get(&#39;name&#39;)</span><br><span class="line">...</span><br><span class="line">&#39;China&#39;</span><br><span class="line">&#39;Liechtenstein&#39;</span><br><span class="line">&#39;Singapore&#39;</span><br><span class="line">&#39;Panama&#39;</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2022/07/01/python-elementtree-notes/" data-id="cld1mheb4001dyuwb2id44d0w" data-title="Python ElementTree notes" class="article-share-link">Share</a>
      
      
        <a href="/2022/07/01/python-elementtree-notes/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2022/07/01/python-elementtree-notes/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ElementTree/" rel="tag">ElementTree</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-kvm-introduction-00" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/06/30/kvm-introduction-00/" class="article-date">
  <time class="dt-published" datetime="2022-06-30T15:47:07.000Z" itemprop="datePublished">2022-06-30</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/virtualization/">virtualization</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/06/30/kvm-introduction-00/">KVM introduction 00</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>See notation of <code>virt/kvm/kvm_main.c</code> in linux kernel</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line"> * Kernel-based Virtual Machine driver for Linux</span><br><span class="line"> *</span><br><span class="line"> * This module enables machines with Intel VT-x extensions to run virtual</span><br><span class="line"> * machines without emulation or binary translation.</span><br><span class="line"> *</span><br><span class="line"> * Copyright (C) 2006 Qumranet, Inc.</span><br><span class="line"> * Copyright 2010 Red Hat, Inc. and&#x2F;or its affiliates.</span><br><span class="line"> *</span><br><span class="line"> * Authors:</span><br><span class="line"> *   Avi Kivity   &lt;avi@qumranet.com&gt;</span><br><span class="line"> *   Yaniv Kamay  &lt;yaniv@qumranet.com&gt;</span><br><span class="line"> *</span><br><span class="line"> * This work is licensed under the terms of the GNU GPL, version 2.  See</span><br><span class="line"> * the COPYING file in the top-level directory.</span><br><span class="line"> *</span><br><span class="line"> *</span><br></pre></td></tr></table></figure>

<p>I got some questions</p>
<ul>
<li>what means kernel-based</li>
<li>what is VT-x</li>
<li>emulation? binary traslation?</li>
<li>who is Avi Kivity</li>
<li>is there any user-mode hypervisor?</li>
</ul>
<h2 id="Kernel-based"><a href="#Kernel-based" class="headerlink" title="Kernel-based"></a>Kernel-based</h2><blockquote>
</blockquote>
<p>Kernel-based Virtual Machine (KVM) is a virtualization module in the Linux kernel that allows the kernel to function as a hypervisor. It was merged into the mainline Linux kernel in version 2.6.20, which was released on February 5, 2007. <sup>[1]</sup></p>
<p>its available under <code>linux/virt</code></p>
<h2 id="VT-x"><a href="#VT-x" class="headerlink" title="VT-x"></a>VT-x</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">This module enables machines with Intel VT-x extensions to run virtual</span><br><span class="line">machines without emulation or binary translation.</span><br></pre></td></tr></table></figure>

<p>According to the code notation, Intel VT-x extensions is metioned.</p>
<blockquote>
</blockquote>
<p>Previously codenamed “Vanderpool”, VT-x represents Intel’s technology for virtualization on the x86 platform. On November 13, 2005, Intel released two models of Pentium 4 (Model 662 and 672) as the first Intel processors to support VT-x. The CPU flag for VT-x capability is “vmx”; in Linux, this can be checked via /proc/cpuinfo, or in macOS via sysctl machdep.cpu.features.<sup>[2]</sup></p>
<p>for example, on centos 7.6</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@test ~]# cat &#x2F;proc&#x2F;cpuinfo | grep vmx | head -1</span><br><span class="line">flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology eagerfpu pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 arat</span><br></pre></td></tr></table></figure>

<p>or on Intel CPU MacBook Pro (2020)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ sysctl machdep.cpu.features | grep -i vmx</span><br><span class="line">machdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 PCLMULQDQ DTES64 MON DSCPL VMX EST TM2 SSSE3 FMA CX16 TPR PDCM SSE4.1 SSE4.2 x2APIC MOVBE POPCNT AES PCID XSAVE OSXSAVE SEGLIM64 TSCTMR AVX1.0 RDRAND F16C</span><br></pre></td></tr></table></figure>

<p>vmx is available</p>
<blockquote>
</blockquote>
<p>“VMX” stands for Virtual Machine Extensions, which adds 13 new instructions: VMPTRLD, VMPTRST, VMCLEAR, VMREAD, VMWRITE, VMCALL, VMLAUNCH, VMRESUME, VMXOFF, VMXON, INVEPT, INVVPID, and VMFUNC.[21] These instructions permit entering and exiting a virtual execution mode where the guest OS perceives itself as running with full privilege (ring 0), but the host OS remains protected.<sup>[2]</sup></p>
<p>note: virtual execution mode is a important concept</p>
<p>refer to paper <strong>kvm: the Linux Virtual Machine Monitor</strong> KVM is designed to add a guest mode, joining the existing <em>kernel mode</em> and <em>user mode</em> </p>
<img src="/2022/06/30/kvm-introduction-00/cpu_executino_loop.png" class="">

<p>In <em>guest-mode</em> CPU instruction executed natively but when I/O requests or signal(typically, network packets received or timeout), exit <em>guest-mode</em> is required and kvm than redirect those I/O or signal handling to <em>user-mode</em> process to emulation device and execute actual I/O. After I/O handling finished, KVM will enter guest mode to execute its CPU instructions again.</p>
<p>For <em>kernel-mode</em> handling exit and enter is basic task. And <em>user-mode</em> process calls kernel to enter <em>guest-mode</em> until it interrupt.</p>
<h2 id="Emulation-amp-Binary-translation"><a href="#Emulation-amp-Binary-translation" class="headerlink" title="Emulation &amp; Binary translation"></a>Emulation &amp; Binary translation</h2><blockquote>
</blockquote>
<p>In computing, binary translation is a form of binary recompilation where sequences of instructions are translated from a source instruction set to the target instruction set. In some cases such as instruction set simulation, the target instruction set may be the same as the source instruction set, providing testing and debugging features such as instruction trace, conditional breakpoints and hot spot detection.</p>
<blockquote>
</blockquote>
<p>The two main types are static and dynamic binary translation. Translation can be done in hardware (for example, by circuits in a CPU) or in software (e.g. run-time engines, static recompiler, emulators).<sup>[3]</sup></p>
<p>Emulators mostly used to run softwares or applications on current OS where those softwares or applications are not support. For example, <a target="_blank" rel="noopener" href="https://github.com/OpenEmu/OpenEmu">https://github.com/OpenEmu/OpenEmu</a> a multiple video game system. This is advantage of emulators.</p>
<p>Disadvantage is that binary translation sometimes require instructino scan, if its used for CPU instruction translations, it spends more time than native instruction. More details in <a target="_blank" rel="noopener" href="https://people.redhat.com/pbonzini/qemu-test-doc/_build/html/topics/Translator-Internals.html">Translator-Internals</a> will be talked in next blogs.</p>
<h2 id="Avi-Kivity"><a href="#Avi-Kivity" class="headerlink" title="Avi Kivity"></a>Avi Kivity</h2><p>Mad C++ developer, proud grandfather of KVM. Now working on @ScyllaDB, an open source drop-in replacement for Cassandra that’s 10X faster. Hiring (remotes too).</p>
<p>from <a target="_blank" rel="noopener" href="https://twitter.com/avikivity">https://twitter.com/avikivity</a></p>
<blockquote>
</blockquote>
<p>Avi Kivity began the development of KVM in mid-2006 at Qumranet, a technology startup company that was acquired by Red Hat in 2008. KVM surfaced in October, 2006 and was merged into the Linux kernel mainline in kernel version 2.6.20, which was released on 5 February 2007. </p>
<blockquote>
</blockquote>
<p>KVM is maintained by Paolo Bonzini. <sup>[1]</sup></p>
<h2 id="Virtualization"><a href="#Virtualization" class="headerlink" title="Virtualization"></a>Virtualization</h2><img src="/2022/06/30/kvm-introduction-00/hardware_assisted_virtualization.png" class="">

<p>For hardware assisted virtualiztion, VMM running below ring0 Guest OS, user application can directly execute user requests, and sensitive OS call trap to VMM without binary translation or Paravirtualization so overhead is decreased.</p>
<p>But for older full virtualization design</p>
<img src="/2022/06/30/kvm-introduction-00/full_virtualization.png" class="">

<p>Guest OS runs on Ring1 and VMM runs on Ring0, without hardware assist, OS requests trap to VMM and after binary translation the instruction finally executed.</p>
<h2 id="KVM-Details"><a href="#KVM-Details" class="headerlink" title="KVM Details"></a>KVM Details</h2><h3 id="Memory-map"><a href="#Memory-map" class="headerlink" title="Memory map"></a>Memory map</h3><p>From perspective of Linux guest OS. Physical memory is already prepared and virtual memory is allocated depend on the physical memory. When guest OS require a virtual address(GVA), Guest OS need to translate is to guest physical address(GPA), this obey the prinsiple of Linux, and tlb, page cache will be involved. And no difference with a Linux Guest running on a real server.</p>
<p>From perspective of host, start a Guest need to allocate a memory space as GPA space. So every GPA has a mapped host virtual address(HVA) and also a host physical address(HPA)</p>
<p>So typically, if a guest need to access a virtual memory address</p>
<p>GVA -&gt; GPA -&gt; HVA -&gt; GPA</p>
<p>at least three times of translation is needed.</p>
<p>Nowadays, CPU offer EPT(Intel) or NPT(AMD) to accelerate GPA -&gt; HVA translation. We will refer that in after blogs.</p>
<h3 id="vMMU"><a href="#vMMU" class="headerlink" title="vMMU"></a>vMMU</h3><p>MMU consists of </p>
<ul>
<li>A radix tree ,the page table, encoding the virtual- to-physical translation. This tree is provided by system software on physical memory, but is rooted in a hardware register (the cr3 register)</li>
<li>A mechanism to notify system software of missing translations (page faults)</li>
<li>An on-chip cache(the translation lookaside buffer, or tlb) that accelerates lookups of the page table</li>
<li>Instructions for switching the translation root inorder to provide independent address spaces</li>
<li>Instructions for managing the tlb</li>
</ul>
<p>As referred in <strong>Memory map</strong> GPA -&gt; HVA should be offered by KVM.</p>
<p>If no hardware assist, use shadow table to maintain the map between GPA and HVA, the good point of shadow table is that runtime address translation overhead is decrease but the major problem is how to synchronize guest page table with shadow page table, when guest writes page table, the shadow page table need to be changed together, so virtual MMU need offer hooks to implement this.</p>
<p>Another question is context switch. Shadow page tables based on the fact that guest should sync its tlb with shadow page tables so that tlb management instruction will be trapped. But the most common tlb management instruction in context-switch is invalidates the entire tlb. So the shadow page tables need to be synced again. Causes bad performance when vm runs multi processes.</p>
<p>vMMU is implement in order to improve guest performance which caches all page tables during context switch. This means context swtich could find its cache from vMMU directly, invdalidates tlb has no influence on context-switch.</p>
<ul>
<li>[1] <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine">https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine</a></li>
<li>[2] <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/X86_virtualization#Intel_virtualization_(VT-x)">https://en.wikipedia.org/wiki/X86_virtualization#Intel_virtualization_(VT-x)</a></li>
<li>[3] <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Binary_translation">https://en.wikipedia.org/wiki/Binary_translation</a></li>
<li>[4] <a target="_blank" rel="noopener" href="https://www.kernel.org/doc/ols/2007/ols2007v1-pages-225-230.pdf">https://www.kernel.org/doc/ols/2007/ols2007v1-pages-225-230.pdf</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2022/06/30/kvm-introduction-00/" data-id="cld1mheai000nyuwb2orxc0lk" data-title="KVM introduction 00" class="article-share-link">Share</a>
      
      
        <a href="/2022/06/30/kvm-introduction-00/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2022/06/30/kvm-introduction-00/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kernel/" rel="tag">kernel</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kvm/" rel="tag">kvm</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Achieving-network-wirespeed-in-an-open-standard-manner-introducing-vDPA" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/06/09/Achieving-network-wirespeed-in-an-open-standard-manner-introducing-vDPA/" class="article-date">
  <time class="dt-published" datetime="2022-06-09T14:22:56.000Z" itemprop="datePublished">2022-06-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/virtualization/">virtualization</a>►<a class="article-category-link" href="/categories/virtualization/translation/">translation</a>►<a class="article-category-link" href="/categories/virtualization/translation/virtio-networking/">virtio-networking</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/06/09/Achieving-network-wirespeed-in-an-open-standard-manner-introducing-vDPA/">Achieving network wirespeed in an open standard manner: introducing vDPA</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>之前的文章里，我们讨论了现存的virtio-networking架构，包括基于内核的（vhost-net/virtio-net）以及基于用户态DPDK的（vhost-user/vhost-pmd），现在我们需要转移我们的注意力到一个目标是让virtio-networking架构给VM提供有线连接速度的架构</p>
<p>本文将会涵盖构成这个架构的数据面以及控制面组件。我们将会介绍SR-IOV技术，以及这个技术如何提升网络性能。还会介绍virtio的硬件方案以及vDPA（virtual data path acceleration）带来的巨大好处。最后通过比较这些virtio-networking架构来做一个总结。</p>
<p>本文主要是为了那些有兴趣想了解不同virtio-networking架构的（包括vDPA），但不那么深入细节的人。当然后面也会提供一个技术细节的分享以及一个实践教程。</p>
<h2 id="Data-plane-and-control-plane-for-direct-access-to-NIC"><a href="#Data-plane-and-control-plane-for-direct-access-to-NIC" class="headerlink" title="Data plane and control plane for direct access to NIC"></a>Data plane and control plane for direct access to NIC</h2><p>在之前的vhost-net/virtio-net和vhost-user/vhost-pmd架构里，网卡都是接入在OVS kernel或者OVS-DPDK里的，而virtio的后端接口则是从OVS的另外一个port出去的</p>
<p>为了提升网络性能，直接把网络连到guest里，和之前的virtio架构类似，我们拆分了网卡的控制面和数据面：</p>
<ol>
<li>控制面。提供网卡和guest之前的配置修改和特性协商功能，用来建立和销毁数据面通道</li>
<li>数据面。用来在guest和网卡之间传输数据包。当直接把网卡连接到guest的时候，实际上是要求网卡要支持virtio ring layout的</li>
</ol>
<p>这个架构如下图所示：</p>
<img src="/2022/06/09/Achieving-network-wirespeed-in-an-open-standard-manner-introducing-vDPA/2019-10-02-vdpa-figure1.jpeg" class="">

<p>笔记：</p>
<ul>
<li>如果需要知道KVM，libvirt以及Qemu进程的额外信息，可以看前面的文章</li>
<li>数据面直接从网卡到guest，实际上是通过guest提供一个网卡可访问的共享内存实现的，并且并不经过host kernel。这个意味着网卡和guest都需要使用完全一致的ring layout否则就需要做地址翻译，地址翻译意味着性能损耗</li>
<li>控制面的实现则可能设计host kernel或者qemu进程，这个取决于具体的实现</li>
</ul>
<h2 id="SR-IOV-for-isolating-VM-traffic"><a href="#SR-IOV-for-isolating-VM-traffic" class="headerlink" title="SR-IOV for isolating VM traffic"></a>SR-IOV for isolating VM traffic</h2><p>在vhost-net/virtio-net和vhost-user/virto-pmd架构里，我们是用了软件交换机（OVS）可以让一个网卡对接到物理端口上，然后分发数据包到不同的虚拟机的端口上。</p>
<p>把网卡挂在虚拟机上最简单的方法就是硬件透传，也就是直接把一个网卡提供给guest kernel的驱动。</p>
<p>问题是我们需要在服务器上有一个单独的通过PIC暴露的物理网卡，接下来的问题就是我们如何在物理网卡上创建“虚拟端口”？</p>
<p>SR-IOV（Single root I/O virtualization)是一种PCI设备规范，允许共享一个物理设备给多个虚拟机。换言之，这个功能允许不同的虚拟环境里的虚拟机共享一个网卡。这意味着我们能够拥有一个类似把一个物理网卡拆分为多个以太网接口的功能，帮我们解决了上面提到的“虚拟端口”的创建问题。</p>
<p>SR-IOV有两个主要功能</p>
<ol>
<li>Physical Functions，即PCI设备的完整功能，包括发现，管理和配置功能。每个网卡都有一个对应的PF能提供整个网卡设备的配置</li>
<li>Virtual Functions，是单个PCI功能，可以控制设备的一部分，并且是PF的子集。同一个网卡上能有多个VF</li>
</ol>
<p>我们需要在网卡上配置VF，PF，VF相当于是虚拟接口，PF相当于是网络接口，举个例子，我们有一个10GB网卡有一个外部接口和8个VF。那么这个外部端口的速度以及双工是取决于PF的配置而频率限制则是VF的设置</p>
<p>hypervisor提供了映射virtual function到虚拟机的功能，每个VF都可以被映射到一个VM（一个VM可以同时有多个VF）</p>
<p>然后来看看SR-IOV是如何映射到guest kernel，用户态DPDK或者是直接到host kernel的吧</p>
<img src="/2022/06/09/Achieving-network-wirespeed-in-an-open-standard-manner-introducing-vDPA/2019-10-02-vdpa-figure2.jpeg" class="">

<ol>
<li>OVS和SR-IOV： 我们使用SR-IOV给OVS提供多个物理面端口，比如配置多个单独的mac地址，虽然我们只有一个物理网卡，但可以通过VF实现。并且给每分配一段内核内存到特定到VF（每个VF都有）</li>
<li>OVS DPDK和SR-IOV：跳过物理机内核，通过SR-IOV直接从物理网卡到OVS-DPDK。映射host用户态内存给网卡的VF</li>
<li>SR-IOV + guests：映射guest内存到网卡，跳过所有物理机环节。注意：使用设备透传，ring layout在物理网卡和guest之间是共享的，因此特定网卡才能被使用，因为这个逻辑一定是网卡厂商提供的。</li>
</ol>
<p>注意：当然还有不是很常见的第四个方案，就是透传设备给guest里的用户态DPDK应用。</p>
<h2 id="SR-IOV-for-mapping-NIC-to-guest"><a href="#SR-IOV-for-mapping-NIC-to-guest" class="headerlink" title="SR-IOV for mapping NIC to guest"></a>SR-IOV for mapping NIC to guest</h2><p>重点说一下SR-IOV到guest的情况，这里存在一个问题就是在直接映射内存到网卡的场景下，如何更高效的发包收包。</p>
<p>我们有下面两个方法解决这个问题：</p>
<ol>
<li>使用guest kerel驱动：这个方法就是使用网卡厂商提供的kernel驱动，即直接映射IO内存，这样的话硬件设备就能够直接访问guest kernel的内存了</li>
<li>在guest里使用DPDK-pmd驱动：这个方法，就是使用网卡厂商提供的DPDK pmd驱动，运行在guest的用户态，能够直接映射IO内存，因此硬件设备也能够直接访问用户态的特定进程</li>
</ol>
<p>这一段我们重点看看DPDP pmd驱动的方案，整合起来就是下面这个图：</p>
<img src="/2022/06/09/Achieving-network-wirespeed-in-an-open-standard-manner-introducing-vDPA/2019-10-02-vdpa-figure3.jpeg" class="">

<p>笔记：</p>
<ul>
<li>数据面是和厂商挂钩的直接访问VF</li>
<li>对SRIOV，网卡厂商的驱动需要在host和guest都装</li>
<li>host内核驱动以及guest的pmd驱动并不直接互相访问，PF/VF的驱动是通过其他接口配置的（比如libvirt等）</li>
<li>厂商提供的VF-pmd需要负责网卡VF的配置而PF驱动则负责在host内核上管理好物理网卡设备</li>
</ul>
<p>总结一下这个方案，我们可以通过SR-IOV + DPDK PMC的方式给Guest提供一个很好的网络性能，不过这个方法还是挺麻烦的。因为这个方法是和厂商绑定的，因此需要在guest和host跑一样的驱动，并且特定网卡。这意味着网卡硬件升级，虚拟机应用驱动也需要升级。如果网卡被替换成了另外一个厂商的网卡，那么guest也需要装一个新的pmd。同时迁移虚拟机则会要求host上配置完全一致。也就是说网卡需要版本一致，物理位置需要一致，并且厂家还要提供迁移支持。</p>
<p>因此我们要处理的问题就是如何使用标准接口实现SR-IOV的性能提升，最好是只需要标准驱动，把这个驱动问题从整个架构中解耦出来。</p>
<p>下面的两个方案就是来解决这个问题的</p>
<h2 id="Virtio-full-HW-offloading"><a href="#Virtio-full-HW-offloading" class="headerlink" title="Virtio full HW offloading"></a>Virtio full HW offloading</h2><p>第一个方案是virtio的硬件替代方案，把virtio的控制面和数据面都转移到硬件上，也就是说网卡（当然还是通过VF来提供虚拟接口），支持virtio控制面的标准，包括发现，特性协商，以及建立/销毁数据面，等等。这个设备也支持virtio rang layout，因此一旦内存在网卡和guest之间被映射了，他们就能够直接通信了。</p>
<p>这个方案里，guest能够直接和网卡使用PCI通讯吗所以没有必要使用额外的驱动。然而需要网卡厂商提供支持virtio标准的网卡，包括控制面的软件实现，一般来说都是操作系统实现的，这个情况就是需要网卡自己实现。</p>
<p>下面是硬件架构的图：</p>
<img src="/2022/06/09/Achieving-network-wirespeed-in-an-open-standard-manner-introducing-vDPA/2019-10-02-vdpa-figure4.jpeg" class="">

<p>笔记：</p>
<ul>
<li>实际上控制面需要的操作是非常负责的主要是和IOMMU以及vIOMMU，下篇文章里会说</li>
<li>实际上在host kernel，qemu进程还有guest kernel都涉及这个流程，图里面简化了</li>
<li>当然也可以吧virtio数据面控制面放到kernel里而不是用户态（和SRIOV的场景一致），也就是直接使用virtio-net驱动来和网卡通讯（而不是使用virtio-pmd）</li>
</ul>
<h2 id="vDPA-standard-data-plane"><a href="#vDPA-standard-data-plane" class="headerlink" title="vDPA - standard data plane"></a>vDPA - standard data plane</h2><p>Virtual data path acceleration (vDPA) 是一个通过virtio ring layout和放置一个SRIOV在guest，来标准化网卡SRIOV数据面将这个网络性能改善和厂家实现解耦的方案，通过增加一个通用的控制面以及阮家架构来支持vDPA。提供一个抽象层，在SRIOV之上，并且给未来的可拓展IOV打好基础。</p>
<p>和virtio的硬件方案类似，数据面直接建立在网卡和guest之间，都使用virtio ring layout。然而每个网卡厂家可能就会提供各自的驱动了，然后一个通用的vDPA驱动就被添加到了kernel里面来完成常见网卡驱动或控制面之间的virtio控制面翻译工作。</p>
<p>vDPA是一个灵活性更高的方案，相比硬件方案来说，网卡厂家支持virtio ring layout的成本更小了，并且也能够达到性能提升的目的。</p>
<p>示例图如下：</p>
<img src="/2022/06/09/Achieving-network-wirespeed-in-an-open-standard-manner-introducing-vDPA/2019-10-02-vdpa-figure5.jpeg" class="">

<p>笔记：</p>
<ul>
<li>实际上在host kernel，qemu进程还有guest kernel都涉及这个流程，图里面简化了</li>
<li>类似SRIOV和virtio全硬件方案，数据面控制面都是在guest内核里而不是用户态（优劣和之前提到的一样）</li>
</ul>
<p>vDPA有潜力成为一个权威的给虚拟机提供以太网接口的方案：</p>
<ol>
<li>开源的标准：任何人可用，并且贡献标准，而不被特定的厂商限制</li>
<li>优异的性能：接近SRIOV，中间没有翻译成本</li>
<li>可以支持未来的硬件平台拓展技术</li>
<li>独立于特定厂商的标准驱动，意味着只需要配置一次驱动，而不用太关心网卡和版本</li>
<li>传输保护，guest直接使用单个接口。从host角度容易发现，并可以做好切换</li>
<li>在线迁移，提供不同网卡不同版本的在线迁移</li>
<li>提供一个标准的容器加速接口</li>
<li>裸机，提供标准的网卡驱动mvirtio网卡驱动可以被作为一个裸机驱动，当时用vDPA软件架构的时候，驱动这个驱动来适配不同网卡硬件</li>
</ol>
<h2 id="Comparing-virtio-architectures"><a href="#Comparing-virtio-architectures" class="headerlink" title="Comparing virtio architectures"></a>Comparing virtio architectures</h2><p>总结一下之前的系列里我们学到的内容，包括四种提供给vm以太网络的架构，vhost-net/virito-net, vhost-user/virito-pmd, virtio full HW offloading 和 vDPA。</p>
<p>然后来比较一下他们的优劣：</p>
<img src="/2022/06/09/Achieving-network-wirespeed-in-an-open-standard-manner-introducing-vDPA/virtio-arch-compare.png" class="">

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇文章我们包含了四种提供以太网接口的virtio-networking架构概览，包括速度慢的（virtio-net）到比较快的（vhost-user）还有最快的（virtio硬件方案和vDPA）</p>
<p>我们强调vDPA和SR-IOV相对其他技术来说的优势，也提供了四种技术的对比，接下来会更加深入的使用virtio硬件方案以及vDPA。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2022/06/09/Achieving-network-wirespeed-in-an-open-standard-manner-introducing-vDPA/" data-id="cld1mhe9t0001yuwbfbjaec59" data-title="Achieving network wirespeed in an open standard manner: introducing vDPA" class="article-share-link">Share</a>
      
      
        <a href="/2022/06/09/Achieving-network-wirespeed-in-an-open-standard-manner-introducing-vDPA/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2022/06/09/Achieving-network-wirespeed-in-an-open-standard-manner-introducing-vDPA/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/architecture/" rel="tag">architecture</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/qemu/" rel="tag">qemu</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vDPA/" rel="tag">vDPA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vhost-net/" rel="tag">vhost-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-net/" rel="tag">virtio-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-networking/" rel="tag">virtio-networking</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-linux-memory-management-1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/06/04/linux-memory-management-1/" class="article-date">
  <time class="dt-published" datetime="2022-06-04T15:22:23.000Z" itemprop="datePublished">2022-06-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/linux/">linux</a>►<a class="article-category-link" href="/categories/linux/memory-management/">memory management</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/06/04/linux-memory-management-1/">Linux memory management(1)</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="CPU-aceess-memory"><a href="#CPU-aceess-memory" class="headerlink" title="CPU aceess memory"></a>CPU aceess memory</h2><p>CPU core -&gt; MMU(TLBs, Table Walk Unit) -&gt; Caches -&gt; Memory(Translation tables)</p>
<p>CPU VA -&gt; MMU find PTE(Pysical table entry) -&gt; TLB -&gt; L1 cache -&gt; L2 cache -&gt; L3 cache</p>
<p>note: pretend a architecture with TLB between CPU and L1 cache.</p>
<p>TLB is a some cache form VA-to-PA translaction and formed by PTE blocks.</p>
<p>if TLB miss, CPU find PA from L1 and so on until PA is find and then put the PTE into TLB.</p>
<h3 id="what-is-TLB"><a href="#what-is-TLB" class="headerlink" title="what is TLB?"></a>what is TLB?</h3><p>TLB definition from wiki: A translation lookaside buffer (TLB) is a memory cache that is used to reduce the time taken to access a user memory location. It is a part of the chip’s memory-management unit (MMU). The TLB stores the recent translations of virtual memory to physical memory and can be called an address-translation cache. A TLB may reside between the CPU and the CPU cache, between CPU cache and the main memory or between the different levels of the multi-level cache. The majority of desktop, laptop, and server processors include one or more TLBs in the memory-management hardware, and it is nearly always present in any processor that utilizes paged or segmented virtual memory.</p>
<p>note: </p>
<ol>
<li><p>TLB stores recent translations that means not all address translation entry is stored in TLB, take care about cache miss.</p>
</li>
<li><p>TLB may reside between the CPU and the CPU cache, between the CPU cache and primary storage memory, or between levels of a multi-level cache. </p>
</li>
<li><p>virtual addressing met cache miss or physical addressing, CPU always uses TLB to find and store it into cache.</p>
</li>
<li><p>cache strategy LRU or FIFO</p>
</li>
<li><p>The CPU has to access main memory for an instruction-cache miss, data-cache miss, or TLB miss, but compare to others the third case TLB miss is too expensive.</p>
</li>
<li><p>freqently TLB misses occur degrading performance, because each newly cached page displacing one that will soon be used again. Where the TLB acting as a cache for the memory management unit (MMU) which translates virtual addresses to physical addresses is too small for the working set of pages. TLB thrashing can occur even if instruction cache or data cache thrashing are not occurring, because these are cached in different sizes. Instructions and data are cached in small blocks (cache lines), not entire pages, but address lookup is done at the page level. Thus even if the code and data working sets fit into cache, if the working sets are fragmented across many pages, the virtual address working set may not fit into TLB, causing TLB thrashing.</p>
</li>
</ol>
<h3 id="TLB-miss-handling"><a href="#TLB-miss-handling" class="headerlink" title="TLB-miss handling"></a>TLB-miss handling</h3><p>Two schemes for handling TLB misses are commonly found in modern architectures:</p>
<ul>
<li>With hardware TLB management, the CPU automatically walks the page tables . On x86 for example, use CR3 register to walks page tables if entry exists, bring back to TLB and TLB tries and access will hit. Or raise a page fault exception which need to be handled by operation system and load correct physical address to TLB(page swap in/out). CPU change do not cause loss of compatibility for the programs.</li>
<li>With software-managed TLBs, a TLB miss generates a TLB miss exception, and operating system code is responsible for walking the page tables and performing the translation in software. The operating system then loads the translation into the TLB and restarts the program from the instruction that caused the TLB miss. As with hardware TLB management, if the OS finds no valid translation in the page tables, a page fault has occurred, and the OS must handle it accordingly. Instruction sets of CPUs that have software-managed TLBs have instructions that allow loading entries into any slot in the TLB. The format of the TLB entry is defined as a part of the instruction set architecture (ISA).</li>
</ul>
<p>note:</p>
<ol>
<li>hardware TLB management TLB handling the lifecycle of TLB entries. </li>
<li>hardware TLB management throws page fault that OS must handling and OS should bring the missing table entry of physical address into TLB cache. And than the program resume.</li>
<li>hardware TLB management maintain TLB enties is invisible to software. </li>
<li>hardware TLB management can change from CPU to CPU, but without causing compatibility for the programs. In other words, CPU should obey the rules of TLB management so there is always any page fault exception require OS to handle</li>
<li>software TLB management throws TLB miss exception and OS owns the responsibility to walk page tables and translation in software. Then OS loads TLB table and restart programs (attention! not resume but restart).</li>
<li>compare hardware and software TLB management, according to 2 CPU finds TLB and throw page fault exception when hardware, but in sofware situation, the CPU’s instruction sets should have instruction to load TLB to anywhere and TLB entry can be used directly by CPU instruction</li>
</ol>
<p>In most cases, hardware TLB management is used. But according to wiki, some of the architectures using software TLB management.</p>
<h2 id="Typical-TLB"><a href="#Typical-TLB" class="headerlink" title="Typical TLB"></a>Typical TLB</h2><p>These are typical performance levels of a TLB:</p>
<ul>
<li>Size: 12 bits – 4,096 entries</li>
<li>Hit time: 0.5 – 1 clock cycle</li>
<li>Miss penalty: 10 – 100 clock cycles</li>
<li>Miss rate: 0.01 – 1% (20–40% for sparse/graph applications)</li>
</ul>
<p>The average effective memory cycle rate is defined as <code>m + (1-p)h + pm</code> cycles, where <code>m</code> is the number of cycles required for a memory read, <code>p</code> is the miss rate, and <code>h</code> is the hit time in cycles. If a TLB hit takes 1 clock cycle, a miss takes 30 clock cycles, a memory read takes 30 clock cycles, and the miss rate is 1%, the effective memory cycle rate is an average of 30 + 0.99 * 1 + 0.01 * 30 (31.29 clock cycles per memory access)</p>
<p>note: research more of TLB performance</p>
<p>use perf test TLB miss</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">perf stat -e dTLB-loads,dTLB-load-misses,iTLB-loads,iTLB-load-misses -p $PID</span><br></pre></td></tr></table></figure>

<p>if a high TLB missing rate exists in your OS, try to use huge page to decrease the table entries in TLB which will cut down the miss rate. But some application is not siutable for huge page and more details need to be change before use this solution.</p>
<h2 id="Address-space-switch"><a href="#Address-space-switch" class="headerlink" title="Address-space switch"></a>Address-space switch</h2><p>After process context switches, some TLB entries’ virtual address to physical address mapping is invalid. In order to clean thoes invalid entires, some strategies is required. </p>
<ol>
<li>flush all entries after process context change</li>
<li>mark the entries with its process so the process context change do not matter</li>
<li>some architecture use a sinlge address space operating system, all process use the same virtual-to-pysical mapping</li>
<li>some CPU have a process register and hardware uses TLB entries only the current process ID matches</li>
</ol>
<p>note:<br>flushing TLB is an important security mechanism for memory isolation. Memory isolation is especially critical during switches between the privileged operating system kernel process and the user processes – as was highlighted by the Meltdown security vulnerability[2]. Mitigation strategies such as kernel page-table isolation (KPTI) rely heavily on performance-impacting TLB flushes and benefit greatly from hardware-enabled selective TLB entry management such as PCID.</p>
<h2 id="Virtualization-and-x86-TLB"><a href="#Virtualization-and-x86-TLB" class="headerlink" title="Virtualization and x86 TLB"></a>Virtualization and x86 TLB</h2><p>With the advent of virtualization for server consolidation, a lot of effort has gone into making the x86 architecture easier to virtualize and to ensure better performance of virtual machines on x86 hardware</p>
<p>EPT required.</p>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><ol>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Translation_lookaside_buffer">https://en.wikipedia.org/wiki/Translation_lookaside_buffer</a></li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Meltdown_(security_vulnerability)">https://en.wikipedia.org/wiki/Meltdown_(security_vulnerability)</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2022/06/04/linux-memory-management-1/" data-id="cld1mheam000syuwb5pb3co5q" data-title="Linux memory management(1)" class="article-share-link">Share</a>
      
      
        <a href="/2022/06/04/linux-memory-management-1/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2022/06/04/linux-memory-management-1/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TLB/" rel="tag">TLB</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/reading-notes/" rel="tag">reading notes</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Hands-on-vhost-user-A-warm-welcome-to-DPDK" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/04/14/Hands-on-vhost-user-A-warm-welcome-to-DPDK/" class="article-date">
  <time class="dt-published" datetime="2022-04-14T07:33:37.000Z" itemprop="datePublished">2022-04-14</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/virtualization/">virtualization</a>►<a class="article-category-link" href="/categories/virtualization/translation/">translation</a>►<a class="article-category-link" href="/categories/virtualization/translation/virtio-networking/">virtio-networking</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/04/14/Hands-on-vhost-user-A-warm-welcome-to-DPDK/">Hands on vhost-user: A warm welcome to DPDK</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>这片文章里我们将会配置一个环境，然后在虚拟机里运行一个基于DPDK应用。我们将介绍所有用来在host系统上配置一个虚拟交换机需要的步骤，并通过这个虚拟交换机连接到虚拟机的应用。正文包括描述如何创建，安装和运行虚拟机，以及安装里面的应用。你将会学习到如何创建并设置一个简单的通过guest内的应用发送并接收网络数据包到host的虚拟交换机。基于这些设置，你将会学习到如何如何调整设置来获得最优的吞吐性能。</p>
<h1 id="Setting-up"><a href="#Setting-up" class="headerlink" title="Setting up"></a>Setting up</h1><p>对于乐意使用DPDK但不希望配置和安装相关软件的，我们提供了一个ansible playbooks在github的repo里，自动化了所有步骤，我们就基于这个配置开始吧。</p>
<h1 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements:"></a>Requirements:</h1><ul>
<li>一台运行了Linux发行版的电脑。本文使用Centos 7，不过不同的Linux发行版之间命令的差别也不会特别大，特别是Red Hat Enterprise Linux 7</li>
<li>一个有sudo权限的用户</li>
<li>home目录下有大于25GB的空闲空间</li>
<li>至少8GB的RAM</li>
</ul>
<p>首先我们先安装我们需要的包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install qemu-kvm libvirt-daemon-qemu libvirt-daemon-kvm libvirt virt-install libguestfs-tools-c kernel-tools dpdk dpdk-tools</span><br></pre></td></tr></table></figure>

<h1 id="Creating-a-VM"><a href="#Creating-a-VM" class="headerlink" title="Creating a VM"></a>Creating a VM</h1><p>首先从下面的网站下载一个最新的Centos-Cloud-Base镜像</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo wget -O &#x2F;var&#x2F;lib&#x2F;libvirt&#x2F;images&#x2F;CentOS-7-x86_64-GenericCloud.qcow2 http:&#x2F;&#x2F;cloud.centos.org&#x2F;centos&#x2F;7&#x2F;images&#x2F;CentOS-7-x86_64-GenericCloud.qcow2</span><br></pre></td></tr></table></figure>

<p>这个下载的是一个预安装的Centos7，用来在OpenStack环境运行的。因为我们不使用OpenStack，所以我们需要清理一下这个虚拟机。不过首先我们需要做一个镜像的copy。以此保证我们之后能重复使用这个镜像。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo qemu-img create -f qcow2 -b  &#x2F;var&#x2F;lib&#x2F;libvirt&#x2F;images&#x2F;CentOS-7-x86_64-GenericCloud.qcow2  &#x2F;var&#x2F;lib&#x2F;libvirt&#x2F;images&#x2F;vhuser-test1.qcow2 20G</span><br></pre></td></tr></table></figure>

<p>通过下面的配置我们可以允许非特权用户使用libvirt命令（推荐）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export LIBVIRT_DEFAULT_URI&#x3D;&quot;qemu:&#x2F;&#x2F;&#x2F;system&quot;</span><br></pre></td></tr></table></figure>

<p>然后使用清理命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo virt-sysprep --root-password password:changeme --uninstall cloud-init --selinux-relabel -a &#x2F;var&#x2F;lib&#x2F;libvirt&#x2F;images&#x2F;vhuser-test1.qcow2 --network --install &quot;dpdk,dpdk-tools,pciutils&quot;</span><br></pre></td></tr></table></figure>

<p>这个命令回挂载文件系统并自动应用一些基础配置，然后这个镜像就可以用来启动虚拟机了</p>
<p>我们需要一个网络来连接我们的虚拟机，Libvirt处理网络的方式类似管理虚拟机，你可以通过XML文件定义网络并且通过命令行控制它的启动和停止。</p>
<p>举个例子，我们将使用一个叫做’default’的网络libvirt自带的方便网络。用下面的命令定义’default’网络，启动并检查网络运行状态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@10-0-117-158 ~]# virsh net-define &#x2F;usr&#x2F;share&#x2F;libvirt&#x2F;networks&#x2F;default.xml</span><br><span class="line">Network default defined from &#x2F;usr&#x2F;share&#x2F;libvirt&#x2F;networks&#x2F;default.xml</span><br><span class="line"></span><br><span class="line">[root@10-0-117-158 ~]# virsh net-start default</span><br><span class="line">Network default started</span><br><span class="line"></span><br><span class="line">[root@10-0-117-158 ~]# virsh net-list</span><br><span class="line"> Name      State    Autostart   Persistent</span><br><span class="line">--------------------------------------------</span><br><span class="line"> default   active   no          yes</span><br></pre></td></tr></table></figure>

<p>最后我们使用virt-install来创建虚拟机。这个命令行工具包含了一系列常用的操作系统配置定义。然后我们可以基于这个基本定义做一些改动：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">virt-install --import  --name vhuser-test1 --ram&#x3D;4096 --vcpus&#x3D;3 \</span><br><span class="line">--nographics --accelerate \</span><br><span class="line">       --network network:default,model&#x3D;virtio --mac 02:ca:fe:fa:ce:aa \</span><br><span class="line">      --debug --wait 0 --console pty \</span><br><span class="line">      --disk &#x2F;var&#x2F;lib&#x2F;libvirt&#x2F;images&#x2F;vhuser-test1.qcow2,bus&#x3D;virtio --os-variant centos7.0</span><br></pre></td></tr></table></figure>

<p>这些参数分别制定了。vCPUs的数量，RAM的大小，磁盘的路径，以及虚拟机要连接的网络。</p>
<p>出了通过我们指定的这些参数定义VM之外，virt-install会把虚拟机同时创建出来，所以我们应该可以看到：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@10-0-117-158 ~]# virsh list</span><br><span class="line"> Id   Name           State</span><br><span class="line">------------------------------</span><br><span class="line"> 7    vhuser-test1   running</span><br></pre></td></tr></table></figure>

<p>很好，虚拟机已经运行了。接下来我们先把虚拟机停下来然后做一些额外的配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh shutdown vhuser-test1</span><br></pre></td></tr></table></figure>

<h1 id="Preparing-the-host"><a href="#Preparing-the-host" class="headerlink" title="Preparing the host"></a>Preparing the host</h1><p>DPDK对内存缓存的分配和管理做了优化。在Linux上这个需要使用hugepage的支持，所以必须要在kernel上打开。使用的page大小通常需要大于4K，以此通过使用更少的page数量，以及更少的TLB来提升性能。在翻译虚拟地址到物理地址的时候会产生这些查询。为了在启动时分配hugepage，我们需要在bootloader配置里加上kernel参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo grubby --args&#x3D;&quot;default_hugepagesz&#x3D;1G hugepagesz&#x3D;1G hugepages&#x3D;6 iommu&#x3D;pt intel_iommu&#x3D;on&quot; --update-kernel &#x2F;boot&#x2F;vmlinuz-3.10.0-957.27.2.el7.x86_64</span><br></pre></td></tr></table></figure>

<p>当然我们来解释一下这些参数做了什么：</p>
<p><code>default_hugepagesz=1G</code> 默认创建出来的hugepages默认是1GB</p>
<p><code>hugepagesz=1G</code> 启动过程中创建出来的hugepage大小也是1GB</p>
<p><code>hugepages=6</code> 最开始启动的时候创建6个大小为1GB的hugepage，这个在重启之后可以在/proc/meminfo里看到</p>
<p>注意，补充说明hugepages的设置增加了两个IOMMU相关的参数 <code>iommu=pt intel_iommu=on</code> 这个会初始化Intel VT-d以及IOMMU Pass-Through模式，在Linux用户态处理IO的时候需要用到他们。因此我们修改了kernel参数，现在刚好可以做一下重启。</p>
<p>等到重启完成之后，我们可以通过命令行查看对应的参数已经生效了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@10-0-117-158 ~]# cat &#x2F;proc&#x2F;cmdline</span><br><span class="line">BOOT_IMAGE&#x3D;&#x2F;vmlinuz-3.10.0-957.27.2.el7.x86_64 root&#x3D;&#x2F;dev&#x2F;mapper&#x2F;zstack-root ro noibrs noibpb nopti nospectre_v2 nospectre_v1 l1tf&#x3D;off nospec_store_bypass_disable no_stf_barrier mds&#x3D;off mitigations&#x3D;off crashkernel&#x3D;auto rd.lvm.lv&#x3D;zstack&#x2F;root rd.lvm.lv&#x3D;zstack&#x2F;swap rhgb quiet LANG&#x3D;en_US.UTF-8 default_hugepagesz&#x3D;1G hugepagesz&#x3D;1G hugepages&#x3D;6 iommu&#x3D;pt intel_iommu&#x3D;on</span><br></pre></td></tr></table></figure>

<h1 id="Prepare-the-guest"><a href="#Prepare-the-guest" class="headerlink" title="Prepare the guest"></a>Prepare the guest</h1><p>virt-install命令通过libvirt创建并启动了一个虚拟机。为了将基于DPDK的vswitch TestPMD连接到QEMU，我们需要增加如下的定义到XML的device部分：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh edit vhuser-test1</span><br></pre></td></tr></table></figure>

<p>在<code>&lt;device&gt;</code>部分增加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;interface type&#x3D;&#39;vhostuser&#39;&gt;</span><br><span class="line">  &lt;mac address&#x3D;&#39;56:48:4f:53:54:01&#39;&#x2F;&gt;</span><br><span class="line">  &lt;source type&#x3D;&#39;unix&#39; path&#x3D;&#39;&#x2F;tmp&#x2F;vhost-user1&#39; mode&#x3D;&#39;client&#39;&#x2F;&gt;</span><br><span class="line">  &lt;model type&#x3D;&#39;virtio&#39;&#x2F;&gt;</span><br><span class="line">  &lt;driver name&#x3D;&#39;vhost&#39; rx_queue_size&#x3D;&#39;256&#39; &#x2F;&gt;</span><br><span class="line">&lt;&#x2F;interface&gt;</span><br><span class="line">&lt;interface type&#x3D;&#39;vhostuser&#39;&gt;</span><br><span class="line">  &lt;mac address&#x3D;&#39;56:48:4f:53:54:02&#39;&#x2F;&gt;</span><br><span class="line">  &lt;source type&#x3D;&#39;unix&#39; path&#x3D;&#39;&#x2F;tmp&#x2F;vhost-user2&#39; mode&#x3D;&#39;client&#39;&#x2F;&gt;</span><br><span class="line">  &lt;model type&#x3D;&#39;virtio&#39;&#x2F;&gt;</span><br><span class="line">  &lt;driver name&#x3D;&#39;vhost&#39; rx_queue_size&#x3D;&#39;256&#39; &#x2F;&gt;</span><br><span class="line">&lt;&#x2F;interface&gt;</span><br></pre></td></tr></table></figure>

<p>另一个和vhost-net不同的guest配置就是hugepages。因此我们需要给guest增加如下定义：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;memoryBacking&gt;</span><br><span class="line">  &lt;hugepages&gt;</span><br><span class="line">    &lt;page size&#x3D;&#39;1048576&#39; unit&#x3D;&#39;KiB&#39; nodeset&#x3D;&#39;0&#39;&#x2F;&gt;</span><br><span class="line">  &lt;&#x2F;hugepages&gt;</span><br><span class="line">  &lt;locked&#x2F;&gt;</span><br><span class="line">&lt;&#x2F;memoryBacking&gt;</span><br><span class="line"> &lt;numatune&gt;</span><br><span class="line">  &lt;memory mode&#x3D;&#39;strict&#39; nodeset&#x3D;&#39;0&#39;&#x2F;&gt;</span><br><span class="line">&lt;&#x2F;numatune&gt;</span><br></pre></td></tr></table></figure>

<p>这样就有内存了，然后再修改guest里的配置，这是非常重要的配置，没有的话就没办法收发数据包了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> &lt;cpu mode&#x3D;&#39;host-passthrough&#39; check&#x3D;&#39;none&#39;&gt;</span><br><span class="line">  &lt;topology sockets&#x3D;&#39;1&#39; cores&#x3D;&#39;3&#39; threads&#x3D;&#39;1&#39;&#x2F;&gt;</span><br><span class="line">  &lt;numa&gt;</span><br><span class="line">    &lt;cell id&#x3D;&#39;0&#39; cpus&#x3D;&#39;0-2&#39; memory&#x3D;&#39;3145728&#39; unit&#x3D;&#39;KiB&#39; memAccess&#x3D;&#39;shared&#39;&#x2F;&gt;</span><br><span class="line">  &lt;&#x2F;numa&gt;</span><br><span class="line">&lt;&#x2F;cpu&gt;</span><br></pre></td></tr></table></figure>

<p>然后我们需要启动我们的guest。因为我们配置了让虚拟机连接到vhost-user的UNIX sockets，因此我们需要确保guest启动的时候这些sockets时可用的。这是通过启动testpmd实现的，这个操作会创建我们需要的sockets。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo testpmd -l 0,2,3,4,5 --socket-mem&#x3D;1024 -n 4 \</span><br><span class="line">    --vdev &#39;net_vhost0,iface&#x3D;&#x2F;tmp&#x2F;vhost-user1&#39; \</span><br><span class="line">    --vdev &#39;net_vhost1,iface&#x3D;&#x2F;tmp&#x2F;vhost-user2&#39; -- \</span><br><span class="line">    --portmask&#x3D;f -i --rxq&#x3D;1 --txq&#x3D;1 \</span><br><span class="line">    --nb-cores&#x3D;4 --forward-mode&#x3D;io</span><br></pre></td></tr></table></figure>

<p>最后，这个实验需要连接到vhost-user unix sockets，因此启动QEMU的时候需要用root。所以在<code>/etc/libvirt/qemu.conf</code> 中设置 <code>user=root</code>。 这是因为我们呢的特殊验证场景需要这样配置，生产环境通常不建议这样配置。实际上读者需要在本文演示结束之后把 <code>user=root</code> 这个配置去掉。</p>
<p>现在我们可以通过命令启动虚拟机了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh start vhuser-test1.</span><br></pre></td></tr></table></figure>

<p>通过root登陆之后，我们要做的第一件事就是绑定virtio设备到vfio-pci驱动。为了能够完成这个操作，我们需要加载一些内核模块</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# modprobe  vfio enable_unsafe_noiommu_mode&#x3D;1</span><br><span class="line">[   90.462919] VFIO - User Level meta-driver version: 0.3</span><br><span class="line">[root@localhost ~]# cat &#x2F;sys&#x2F;module&#x2F;vfio&#x2F;parameters&#x2F;enable_unsafe_noiommu_mode</span><br><span class="line">Y</span><br><span class="line">[root@localhost ~]# modprobe vfio-pci</span><br></pre></td></tr></table></figure>

<p>然后找出virtio-net设备的PCI地址：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# dpdk-devbind --status net</span><br><span class="line"></span><br><span class="line">Network devices using kernel driver</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">0000:00:02.0 &#39;Virtio network device 1000&#39; if&#x3D;eth0 drv&#x3D;virtio-pci unused&#x3D;virtio_pci,vfio-pci *Active*</span><br><span class="line">0000:00:08.0 &#39;Virtio network device 1000&#39; if&#x3D;eth1 drv&#x3D;virtio-pci unused&#x3D;virtio_pci,vfio-pci</span><br><span class="line">0000:00:09.0 &#39;Virtio network device 1000&#39; if&#x3D;eth2 drv&#x3D;virtio-pci unused&#x3D;virtio_pci,vfio-pci</span><br><span class="line"></span><br><span class="line">No &#39;Crypto&#39; devices detected</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">No &#39;Eventdev&#39; devices detected</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">No &#39;Mempool&#39; devices detected</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">No &#39;Compress&#39; devices detected</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br></pre></td></tr></table></figure>

<p>在dpdk-devbind的输出中找到virtio-devices的部分，并且没有被标记Active状态的。我们可以用这些设备来进行实验。注意：地址可能是不一样的。当我们首次启动这些设备的时候将会自动绑定到virtio-pci驱动，因为我们需要和非kernel的驱动一起使用，首先就是要将这些设备和virtio-pci设备解绑，然后再绑定到vfio-pci驱动</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# dpdk-devbind -b vfio-pci 0000:00:08.0 0000:00:09.0</span><br><span class="line">[  360.862724] iommu: Adding device 0000:00:08.0 to group 0</span><br><span class="line">[  360.871147] vfio-pci 0000:00:08.0: Adding kernel taint for vfio-noiommu group on device</span><br><span class="line">[  360.951240] iommu: Adding device 0000:00:09.0 to group 1</span><br><span class="line">[  360.960126] vfio-pci 0000:00:09.0: Adding kernel taint for vfio-noiommu group on device</span><br></pre></td></tr></table></figure>

<h1 id="Generating-traffic"><a href="#Generating-traffic" class="headerlink" title="Generating traffic"></a>Generating traffic</h1><p>我们已经安装并配置好所有东西了，接下来就是运行网络负载了。首先在host上我们需要启动testpmd实例作为虚拟交换机。然后设置它转发所有在net_vhost0收到的数据包到net_vhost1。testpmd需要在虚拟机启动之前启动，因为它会尝试连接到由QEMU创建的属于vhost-user设备初始化出来的unix sockets。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">testpmd -l 0,2,3,4,5 --socket-mem&#x3D;1024 -n 4 \</span><br><span class="line">    --vdev &#39;net_vhost0,iface&#x3D;&#x2F;tmp&#x2F;vhost-user1&#39; \</span><br><span class="line">    --vdev &#39;net_vhost1,iface&#x3D;&#x2F;tmp&#x2F;vhost-user2&#39; -- \</span><br><span class="line">    --portmask&#x3D;f -i --rxq&#x3D;1 --txq&#x3D;1 \</span><br><span class="line">    --nb-cores&#x3D;4 --forward-mode&#x3D;io</span><br></pre></td></tr></table></figure>

<p>然后我们来启动之前准备好的虚拟机：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh start vhuser-test1</span><br></pre></td></tr></table></figure>

<p>注意这时候我们能够在testpmd看到vhost-user收到的数据包了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Port 1: link state change event</span><br><span class="line">VHOST_CONFIG: vring base idx:0 file:0</span><br><span class="line">VHOST_CONFIG: read message VHOST_USER_GET_VRING_BASE</span><br><span class="line">VHOST_CONFIG: vring base idx:1 file:0</span><br><span class="line">VHOST_CONFIG: read message VHOST_USER_GET_VRING_BASE</span><br><span class="line"></span><br><span class="line">Port 0: link state change event</span><br><span class="line">VHOST_CONFIG: vring base idx:0 file:0</span><br><span class="line">VHOST_CONFIG: read message VHOST_USER_GET_VRING_BASE</span><br><span class="line">VHOST_CONFIG: vring base idx:1 file:0</span><br></pre></td></tr></table></figure>

<p>当guest启动之后我们就能够启动testpmd了。testpmd会初始化端口以及DPDK实现的virtio-net驱动。另外还有virtio特性的协商以及其他一些通用功能的协商也都在这一步发生了。</p>
<p>在我们启动testpmd之前，需要确认vfio内核模块已经加载并绑定了virtio-net设备到vfio-pci驱动：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dpdk-devbind -b vfio-pci 0000:00:08.0 0000:00:09.0</span><br></pre></td></tr></table></figure>

<p>然后可以启动testpmd：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">testpmd -l 0,1,2 --socket-mem 1024 -n 4 \</span><br><span class="line">    --proc-type auto --file-prefix pg -- \</span><br><span class="line">    --portmask&#x3D;3 --forward-mode&#x3D;macswap --port-topology&#x3D;chained \</span><br><span class="line">    --disable-rss -i --rxq&#x3D;1 --txq&#x3D;1 \</span><br><span class="line">    --rxd&#x3D;256 --txd&#x3D;256 --nb-cores&#x3D;2 --auto-start</span><br></pre></td></tr></table></figure>

<p>现在我们可以检查testpmd处理了多少数据包了，我们可以通过输入命令 <code>show port stats all</code> 来看对应（RX/TX）方向的信息，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">testpmd&gt; show port stats all</span><br><span class="line"></span><br><span class="line">  ######################## NIC statistics for port 0  ########################</span><br><span class="line">  RX-packets: 75525952   RX-missed: 0          RX-bytes:  4833660928</span><br><span class="line">  RX-errors: 0</span><br><span class="line">  RX-nombuf:  0         </span><br><span class="line">  TX-packets: 75525984   TX-errors: 0          TX-bytes:  4833662976</span><br><span class="line"></span><br><span class="line">  Throughput (since last show)</span><br><span class="line">  Rx-pps:      4684120</span><br><span class="line">  Tx-pps:      4684120</span><br><span class="line">  #########################################################################</span><br><span class="line"></span><br><span class="line">  ######################## NIC statistics for port 1  ########################</span><br><span class="line">  RX-packets: 75525984   RX-missed: 0          RX-bytes:  4833662976</span><br><span class="line">  RX-errors: 0</span><br><span class="line">  RX-nombuf:  0         </span><br><span class="line">  TX-packets: 75526016   TX-errors: 0          TX-bytes:  4833665024</span><br><span class="line"></span><br><span class="line">  Throughput (since last show)</span><br><span class="line">  Rx-pps:      4681229</span><br><span class="line">  Tx-pps:      4681229</span><br><span class="line"></span><br><span class="line">  #########################################################################</span><br></pre></td></tr></table></figure>

<p>testpmd有不同的转发模式，这个例子里面我们用的是macswap，此模式会交换目标和源头的mac地址。另外的转发模式，比如’io’则不会处理包，所以会给出更高深职很不现实的数据。另外一个转发模式就是’noisy’，可以模拟调整包的缓存/内存的查找。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2022/04/14/Hands-on-vhost-user-A-warm-welcome-to-DPDK/" data-id="cld1mhea00006yuwb46p57oqf" data-title="Hands on vhost-user: A warm welcome to DPDK" class="article-share-link">Share</a>
      
      
        <a href="/2022/04/14/Hands-on-vhost-user-A-warm-welcome-to-DPDK/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2022/04/14/Hands-on-vhost-user-A-warm-welcome-to-DPDK/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DPDK/" rel="tag">DPDK</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/qemu/" rel="tag">qemu</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vhost-net/" rel="tag">vhost-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-net/" rel="tag">virtio-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-networking/" rel="tag">virtio-networking</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-A-journey-to-the-vhost-users-realm" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/02/09/A-journey-to-the-vhost-users-realm/" class="article-date">
  <time class="dt-published" datetime="2022-02-09T07:30:26.000Z" itemprop="datePublished">2022-02-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/virtualization/">virtualization</a>►<a class="article-category-link" href="/categories/virtualization/translation/">translation</a>►<a class="article-category-link" href="/categories/virtualization/translation/virtio-networking/">virtio-networking</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/02/09/A-journey-to-the-vhost-users-realm/">A journey to the vhost-users realm</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>这篇文章是基于上一篇 <a href="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/" title="HOWTO">HOWTO</a> 的一篇深入介绍以使用DPDK达成高性能用户态网络功能的vhost-user/virtio-pmd架构。本文主要是供对这个架构的更多实质性实现有兴趣的研发/架构师，并将会提供一个便于理解的时间博客来探索这些概念。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>之前的deep dive文章里，我们展示了将网络处理逻辑从qemu移动到kernel driver并通过vhost-net协议后带来的好处。这篇文章里，我们将会更深一步展示一下如何通过使用DPDK把host和guest的数据面从kernel移动到用户态之后获取更好的网络性能。为了实现这个我们需要看一下这个新的vhost-user库的实现。</p>
<p>在这篇文章的结尾，你应该对vhost-user/virtio-pmd架构有一个更深的理解，同时能够了解它系统重大性能提升背后的原因。</p>
<h2 id="DPDK-and-its-benefits"><a href="#DPDK-and-its-benefits" class="headerlink" title="DPDK and its benefits"></a>DPDK and its benefits</h2><p>你可能已经听过DPDK了。这个用户态快速法宝处理库是很多网络功能虚拟化应用的核心（NFV，Network Function Virtualization），通过这个库他们可以实现一个完整的用户态应用，而跳过内核的网络协议栈.</p>
<p>DPDK是一组用户态的库，能够使一个用户创建一个优化过的高性能包处理应用。它带来了很多优势，让他在开发人员之中特别受欢迎。下面是列举一部分优势：</p>
<ul>
<li>Processor affinity  DPDK可以把每一个不同的线程pin到一个特定的逻辑核心上来满足并行最大化</li>
<li>Huge pages DPDK有数层内存管理（比如Mempool library或者是Mbuf libraty）。然而实际上所有的内存都mmap在hugetlbfs里面分配的。使用2MB或者是甚至1GB的页，DPDK减少了cache missi以及TLB的查询</li>
<li>Lockless ring buffers DPDK包处理是基于Ring library的，它提供了一个高效无锁的ring queueu支持爆发式的入队出队操作。</li>
<li>Poll Mode Driver 为了避免中断开销，DPDK提供了一个Poll Mode Driver（PMD）抽象</li>
<li>VFIO支持 VFIO（Virtual Function I/O）提供了一个用户态驱动开发框架，允许用户态应用直接通过I/O空间映射到应用内存的方式直接访问硬件设备。</li>
</ul>
<p>附带在这些特性之后的还有另外两个DPDK支持的技术，提供给我们一套极大提升网络应用性能的工具：</p>
<ul>
<li>Vhost-user库 用户态库，实现了vhost协议</li>
<li>Virtio-PMD 基于DPDK的PMD抽象构建，virtio-pmd驱动实现了virtio标准，同时允许通过一个标准和有效的方式使用虚拟硬件。</li>
</ul>
<h2 id="DPDK-and-OVS-A-perfect-combination"><a href="#DPDK-and-OVS-A-perfect-combination" class="headerlink" title="DPDK and OVS: A perfect combination"></a>DPDK and OVS: A perfect combination</h2><p>一个关于DPDK提升性能的好例子就是Open VSwitch。这是一个功能丰富，多层的，分布式虚拟路由器，被广泛作为虚拟化环境以及SDN应用的主要网络层应用。</p>
<p>经典的OVS被划分为一个高效的基于内核的数据路径（fastpath）组合上一个flow table和一个比较慢的用户态数据路径（slowpath）处理不匹配任何flow（fastpath中已有的）的包。通过集成OVS和DPDK，fastpath也移动到了用户态，减少了kernel切换到用户态的交互，提升了最大性能。结果上对比原生的OVS，OVS+DPDK会有～10x的性能提升。</p>
<p>所以我们如何结合OVS-DPDK的这些特性和性能到一个virtio架构中去呢？接下来将一个一个介绍相关的组件。</p>
<h2 id="Vhost-user-library-in-DPDK"><a href="#Vhost-user-library-in-DPDK" class="headerlink" title="Vhost-user library in DPDK"></a>Vhost-user library in DPDK</h2><p>vhost协议是一组消息和机制，被设计用于将virtio数据处理路径从qemu卸载出来（the primary，主要是要卸载包处理）到一个外部元素（the handler，配置virtio rings以及实际的包处理）。最相关的机制是：</p>
<ul>
<li>一组消息允许primary发送virtqueue内存的布局，并且配置到handler</li>
<li>一堆eventft类的文件描述符，允许guest不通过primary直接发送和接受handler的消息：Available Buffer Notification (从guest发送到handler同志有buffers可以被处理了）和 the Used Buffer Notification (从handler发送到guest说明buffers的处理解释了）</li>
</ul>
<p>之前的virtio-networking文章我们描述了一个具体的vhost协议的实现（vhost-net内核模块）以及如何允许qemu把网络处理卸载到host-kernel上。然后我们介绍的vhost-user库，这个库是基于DPDK构建的，是一个vhost协议的用户态实现，允许qemu把virtio设备的包处理卸载到任意DPDK应用（比如Open vSwitch）。</p>
<p>vhost-user库和vhost-net kernel模块主要的不同是通信channel。vhost-net kernel驱动实现了是通过ioctls，而vhost-user库则是通过定义消息结构然后通过unix socket发送的。</p>
<p>DPDK应用可以被配置到提供unix socket（server模式）并且qemu可以连接上去（client模式）。然而，反过来也是可以的，这样就可以允许在不重启vm的情况下重启DPDK了。</p>
<p>在这个socket上，所有请求都被primary（这里是QEMU）初始化，其中一些请求要求返回的，比如GET_FEATURES请求或者其他设置了REPLY_ACK的请求。</p>
<p>在这个场景下，vhost-net kernel模块和vhost-user的库允许primary通过以下重要步骤配置数据面卸载</p>
<ol>
<li>Feature negotiation（特性协商）：virtio特性和vhost-user-specific特性通过类似的方式协商，首先primary会获取handler的特性bitmask然后设置一个它支持的子集</li>
<li>Memory region configuration（内存域配置）：master设置内存域的分布，然后handler就可以mmap()这些内存了</li>
<li>Vring configuration：primary会设置virtqueue的数量，并且设置他们在memory region内的地址。注意：vhost-user支持multiqueue所以设置更多的queue能用来改善性能。</li>
<li>Kick and Call file descriptors sending：通常irqfd和ioeventfd机制生效。详细内容可以回顾【】。更多关于virtio queue的机制可以看virtio数据面的文章</li>
</ol>
<p>总结一下这些机制，DPDK应用能够通过和guest共享内存处理包并且直接发送和接受guest的通知而不经过qemu。</p>
<p>最后一个把所有东西整合到一起的就是QEMU的virtio device模型，它有两个主要任务：</p>
<ul>
<li>模拟virtio设备，在guest里面展示的i一个特定的PCI端口，并且可以被guest查询和完整配置。同时他会把ioeventfd映射到模拟设备的内存映射I/O空间，同时把irqfd映射到Global System Interrupt（GSI）。结果就是，guest对这些东西时没有感知的，通知和中断都会被转发到vhost-user库而没有qemu参与。</li>
<li>替换实际的virtio数据路径实现，这个设备作为vhost-user协议的master来卸载处理逻辑到vhost-user库的DPDK进程里</li>
<li>处理来自virtqueue的请求，并翻译成vhost-user的请求转发到slave。</li>
</ul>
<p>下面的图展示了vhost-user-library作为DPDK应用的一部分的和QEMU交互以及guest使用virtio-device-model和virtio-pci设备：</p>
<img src="/2022/02/09/A-journey-to-the-vhost-users-realm/2019-09-24-virtio-networking-fig1.png" class="">

<p>对这个图有一些需要提的点</p>
<ul>
<li>virtio memory region是guest初始化的</li>
<li>能正确相应的virtio驱动通过定义在virtio规范的标准配置PCI BARs和virtio device接口交互是正常的</li>
<li>virtio-device-model（qemu内）使用vhost-user协议配置vhost-user库，同时配置irqfd和ioeventfd的文件描述符</li>
<li>virtio memory region是guest分配并映射到vhost-user库的（可以是DPDK应用）</li>
<li>结果是DPDK应用能够直接读写包到guest内存，并通过ioeventfd和irqfd机制直接通知guest</li>
</ul>
<h2 id="Userland-Networking-in-the-guest"><a href="#Userland-Networking-in-the-guest" class="headerlink" title="Userland Networking in the guest"></a>Userland Networking in the guest</h2><p>我们已经介绍了DPDK vhost-user实现，允许我们从host内核（vhost-net）卸载数据路径处理逻辑到专门的DPDK用户态应用（比如open vSwitch）因此动态改进了网络的性能。现在，我们将继续了解如何对guest做一样的改动来运行一个高性能网络应用（比如NFV服务）到guest的用户态来替换virtio-net内核模块。</p>
<p>为了能够直接在设备上运行用户态网络应用，我们需要三个组件：</p>
<ol>
<li>VFIO：VFIO是一个用户态驱动开发框架，允许用户态应用直接和设备交互（跳过kernel）</li>
<li>Virtio-pmd驱动：是一个DPDK驱动，基于Poll Mode Driver抽象构建，实现了virtio协议</li>
<li>IOMMU驱动：IOMMU驱动管理了虚拟IOMMU（I/O Memory Management Unit），一个可以对DMA-capable设备执行地址映射的模拟设备</li>
</ol>
<p>接下来我们一个一个的描述他们的细节吧。</p>
<h2 id="VFIO"><a href="#VFIO" class="headerlink" title="VFIO"></a>VFIO</h2><p>VFIO是Virtual Function I/O的缩写。然而，Alex Williamson，vfio-pci kernel驱动的maintainer建议叫它 “Versatile Framework for userspace I/O”，这是一个更加准确的名字。VFIO是一个构建用户态驱动的基础框架，它提供了：</p>
<ul>
<li>映射设备配置和I/O memory regions到用户内存</li>
<li>DMA和驱动的重新映射以及基于IOMMU groups的隔离。后面我们会深入介绍IOMMU以及它是如何工作的，目前假设它允许创建映射到物理内存的虚拟I/O内存空间（类似普通MMU映射到non-IO虚拟内存），所以当一个设备想要DMA到虚拟I/O地址，IOMMU将重新映射该地址并可能应用隔离和其他安全策略</li>
<li>Eventfd和irqfd 基础信号机制支持用户态的信号和中断</li>
</ul>
<p>引用内核文档：“如果你想在VFIO之前写一个驱动，你要么必须经历完整的开发周期才能成为合适的上游驱动，要么在代码树之外维护，要么使用没有IOMMU保护概念的UIO框架，限制中断支持并需要root权限才能访问诸如PCI配置空间之类的东西。”</p>
<p>VFIO暴露了用户友好的API，用于创建character设备（在 /dev/vfio/）支持ioctl通过device descriptor来描述设备、I/O regions 和 他们的读/写/mmap offsets，同时提供机制来描述和注册中断通知。</p>
<h2 id="Virtio-pmd"><a href="#Virtio-pmd" class="headerlink" title="Virtio-pmd"></a>Virtio-pmd</h2><p>DPDK提供了一个叫做Poll Mode Driver (PMD)的驱动抽象。这个东西作用在设备驱动和用户应用之间。提供了一系列高灵活性的东西给用户应用，同时保证了拓展性。比如给新设备实现驱动的能力。</p>
<p>它提供的一些更有用的特性如下：</p>
<ul>
<li>一组API允许特定的驱动实现驱动标准包括接收和发送方法。</li>
<li>每个端口和每个队列硬件的卸载支持静态和动态配置</li>
<li>一个用于数据的可拓展API可用，允许驱动定义自己的驱动标准的数据，同时应用可以调查回溯这些数据</li>
</ul>
<p>virtio Poll Mode Driver（virtio-pmd）是众多使用PMD API的一个驱动之一，为使用DPDK编写的应用程序提供对virtio设备的快速无锁访问，使用virtio的virtqueues提供数据包接收和传输的基本功能。</p>
<p>除了这些PMD的特性之外，virtio-pmd驱动的实现还支持：</p>
<ul>
<li>接收时每个数据包可灵活合并缓冲区和发送时每个数据包的分散缓冲区</li>
<li>组播和混杂模式</li>
<li>MAC/vlan过滤</li>
</ul>
<p>结果就是一个高性能的用户态virtio驱动允许DPDK应用完整的使用virtio标准接口。</p>
<h2 id="Introducing-the-IOMMU"><a href="#Introducing-the-IOMMU" class="headerlink" title="Introducing the IOMMU"></a>Introducing the IOMMU</h2><p>IOMMU可以说基本上等于I/O空间（设备通过DMA直接访问内存）的MMU。它位于主存和设备之间，创建一个virtual I/O空间给每个设备，提供一个机制，动态映射虚拟内存到无力设备。因此当一个驱动配置了设备的DMA（比如一个网卡），并且配置了虚拟地址，当设备尝试访问虚拟地址的时候，这些虚拟地址就被IOMMU重新映射了。</p>
<p>它提供了很多优势比如：</p>
<ul>
<li>可以分配大段相邻虚拟内存，而不需要相邻物理内存</li>
<li>一些设备不支持足够长的访问物理内存的地址，IOMMU解决了这个问题</li>
<li>保护内存，避免DMA攻击通过恶意构造错误的设备并执行访问了并不是分配给设备的内存空间。设备只能看到虚拟地址并且运行操作系统独占的IOMMU映射。</li>
<li>一些架构里支持中断重映射，允许中断隔离和迁移</li>
</ul>
<img src="/2022/02/09/A-journey-to-the-vhost-users-realm/2019-09-24-virtio-networking-fig2.png" class="">

<p>通常情况下，所有东西都是有代价的，IOMMU的缺点是：</p>
<ul>
<li>因为要做page translation，所以性能下降</li>
<li>如果增加page translation表会消耗物理内存</li>
</ul>
<p>vIOMMU - IOMMU for the guest</p>
<p>当然，如果有一个物理IOMMU（比如intel VT-d和AMD-VI）qemu里也是会存在虚拟IOMMU的。QEMU的vIOMMU有以下特征：</p>
<ul>
<li>它翻译guest I/O虚拟地址（IOVA）到guest物理地址（GPA），并且能够通过QEMU的内存管理系统翻译成QEMU的host虚拟地址（HVA）</li>
<li>设备隔离执行</li>
<li>实现了I/O TLB API，因此映射可以在qemu外部查询</li>
</ul>
<p>因此为了获取一个虚拟设备和虚拟IOMMU协作：</p>
<ol>
<li>使用一个可用API在vIOMMU创建一个必要的IOVA映射，目前API是：<br> a. 内核驱动的内核DMA API<br> b. 用户态驱动的VFIO </li>
<li>用虚拟I/O地址配置设备的DMA</li>
</ol>
<h2 id="vIOMMU-and-DPDK-integration"><a href="#vIOMMU-and-DPDK-integration" class="headerlink" title="vIOMMU and DPDK integration"></a>vIOMMU and DPDK integration</h2><p>当一个QEMU模拟的设备尝试DMA到guest的virtio I/O空间，会使用到vIOMMU TLB来查询对应的页的映射并且执行一个安全的DMA访问。问题是如果实际的DMA被卸载到外部进程比如一个使用vhost-user库的DPDK应用？</p>
<p>当vhost-user库尝试直接访问这些共享内存，它需要把所有地址（I/O虚拟地址）到它自己的地址。因此这个需要让QEMU的vIOMMU来提供Device TLB API。Vhost-user库（或者说是vhost-kernel驱动）使用PCIe’s Address Translation Services标准消息集合来向QEMU请求一个页地址的翻译，通过一个次级的通信channel（另一个unix socket）这个channel会在配置IOMMU的时候创建。</p>
<p>总的来说，这里一共有三次地址翻译需要被处理：</p>
<ol>
<li>Qemu的vIOMMU翻译I/O虚拟地址到Guest物理地址</li>
<li>Qemu的内存管理翻译Guest物理地址到Host虚拟地址（在qemu进程的地址空间内的Host虚拟地址）</li>
<li>Vhost-user库翻译Qemu的Host虚拟地址到Vhost-user的Host虚拟地址。通常情况下，当vhost-user库映射qemu内存地址的时候，很简单就是把QEMU的Host虚拟地址翻译到mmap返回的地址即可</li>
</ol>
<p>显然，这些地址翻译存在潜在的性能影响，特别是使用的是动态映射。然而，静态的大页分配（也是DPDK实际上做的）可以最小化这些性能损失。</p>
<p>下面的图优化了之前的vhost-user架构，来包含IOMMU的组件：</p>
<img src="/2022/02/09/A-journey-to-the-vhost-users-realm/2019-09-24-virtio-networking-fig3.png" class="">

<p>关于这个相当复杂的图表要提几点：</p>
<ul>
<li>Guest物理内存空间是从Guest视角当作物理内存空间的，但显然这是QEMU进程的虚拟地址。当virtqueue memory region被分配的时候，实际上是在Guest的物理内存空间里</li>
<li>当I/O虚拟地址分配给包含virtqueue的内存范围，和Guest物理地址相关联的一个条目就会被增加到vIOMMU的TLB table里</li>
<li>另一方面，qemu的内存管理系统是能够知道guest的物理内存空间的和它自己的内存空间是在一起的。因此qemu的内存管理系统是可以翻译guest物理地址到QEMU（host）虚拟地址的</li>
<li>当vhost-user库尝试去访问未被翻译过的IOVA的时候，它会通过secondary unix socket发送一个IOTLB miss的消息</li>
<li>IOTLB API收到这个请求后就会开始查找这个地址，首先是把IOVA翻译成GPA然后GPA翻译成HVA。然后发送这个翻译好的结果到master的unix socket也就是vhost-user库。</li>
<li>最后，vhost-user库需要做最后一次翻译，因为qemu的内存被映射到了自己的内存空间里，所以需要把QEMU的HVA翻译到自己的HVA来访问共享内存</li>
</ul>
<h2 id="Putting-everything-together"><a href="#Putting-everything-together" class="headerlink" title="Putting everything together"></a>Putting everything together</h2><p>这个文章涵盖了大量的组件，包括DPDK，virtio-pmd，VFIO，IOMMU等等</p>
<p>下面的图展示了把这些组件整合之后实现的vhost-user/virtio-pmd架构：</p>


<p>对这个图标要提一点：</p>
<ul>
<li>把这个图和上一个图相比，增加了通过硬件IOMMU，VFIO以及特定vendor的PMD驱动，连接OVS-DPDK应用到物理网卡的组件。现在就不会有疑惑了，因为访问硬件的方式和guest是一样的。</li>
</ul>
<h2 id="An-example-flow"><a href="#An-example-flow" class="headerlink" title="An example flow"></a>An example flow</h2>

<h2 id="Control-Plane"><a href="#Control-Plane" class="headerlink" title="Control Plane"></a>Control Plane</h2><p>这是设置控制面必要的步骤</p>
<ol>
<li>当host上的DPDK应用（OVS）启动，会创建一个socket（server模式）和qemu处理virtio相关的协商逻辑</li>
<li>当qemu启动，它会连接到main socket，然后如果检测到VHOST_USER_PROTOCOL_F_SLAVE_REQ如果vhost-user提供了这个特性，qemu就会创建一个second socket并且把这个socket发送到vhost-user来连接并发送IOTLB同步消息</li>
<li>当QEMU &lt;-&gt; vhost-library的协商结束，两个sockets在他们之间是共享的。一个是virtio配置，另外一个是iotlb消息交换用的</li>
<li>guest启动然后vfio驱动就和PCI设备绑定了。这个驱动能够提供访问iommu groups（iommu group主要取决于硬件拓扑）</li>
<li>当DPDK在guest里启动的时候有以下步骤<br> a. 初始化PCI-vfio设备，同时映射PCI配置空间到用户内存<br> b. 分配virtqueue<br> c. 使用vfio，DMA映射virtqueue内存空间，这样通过IOMMU内核驱动dma映射到vIOMMU设备<br> d. 然后，virto特性协商就开始了。本场景里，使用的virtqueue的地址是IOVA（在I/O虚拟内存空间）。映射eventfd和irqfd也完成了，因此中断和通知就被直接路由在guest和vhost-user库之间，而没有QEMU参与<br> e. 最后，DPDK应用分配一个大片的连续内存作为网络buffer。这部分映射也通过VFIO和IOMMU驱动添加到vIOMMU</li>
</ol>
<p>到此，配置完成并且数据面（virtqueues和notification机制）已经可以使用了。</p>
<h2 id="Data-Plane"><a href="#Data-Plane" class="headerlink" title="Data Plane"></a>Data Plane</h2><p>为了发送数据包，会有以下步骤：</p>
<ol>
<li>guest里的DPDK应用命令virtio-pmd发包。首先会写buffers然后把一致性描述符增加到可用描述符ring里</li>
<li>vhost-user PMD在host端会polling这个virtioqueue，所以他立刻会检测到新的描述符可用并开始处理</li>
<li>对每一个描述符，vhost-user PMD都会map他们的buffer（这一步就是地址翻译，翻译IOVA到HVA）。很少情况下会有buffer内存在一个没被映射到vhost-user IOTLB的页，如果出现miss的情况，会发送一个请求给QEMU，而实际上DPDK应用在guest里会分配静态大页，保证IOTLB请求会被最小限度的发送到QEMU。</li>
<li>这个vhost-user PMD会把这个buffers拷贝到mbufs（也就是DPDK应用使用的message buffers）</li>
<li>这些描述符会被添加到使用过的描述符ring。这些会立刻被guest里的DPDK应用发现，因为guest里的DPDK应用会不断polling virtqueue</li>
<li>然后这些mbufs就被host的DPDK应用消费了</li>
</ol>
<h2 id="Summary-and-conclusions"><a href="#Summary-and-conclusions" class="headerlink" title="Summary and conclusions"></a>Summary and conclusions</h2><p>DPDK是一个很有前途的技术，因为他提供工了极大提升用户态性能的能力。不仅这个技术本身，和OVS结合，能够满足灵活高效的现代虚拟环境的要求，也在NFV部署中扮演着一个重要的角色。</p>
<p>为了充分利用这个技术，数据中心交换数据路径以及在guest中启用NFV应用，有必要在host和guest之间安全的创建一个有效的数据路径。这也就是virtio-net技术起到的作用。</p>
<p>vhost-user提供了一个可靠并且安全的机制来卸载网络处理逻辑到基于DPDK的应用里。它和vIOMMU集成，并提供隔离和内存保护，同时将libvirt从处理数据包的繁重工作下解放出来。</p>
<p>在guest里，符合virtio标准DPDK（virtio-pmd）利用有效的内存管理和高性能的DPDK Poll Mode driver使得在guest里创建也能创建快速数据路径。</p>
<p>如果你想要学习更多关于virtio技术，vhost-user以及DPDK请不要错过下一篇文章。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2022/02/09/A-journey-to-the-vhost-users-realm/" data-id="cld1mhec8005wyuwbfprq2cun" data-title="A journey to the vhost-users realm" class="article-share-link">Share</a>
      
      
        <a href="/2022/02/09/A-journey-to-the-vhost-users-realm/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2022/02/09/A-journey-to-the-vhost-users-realm/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DPDK/" rel="tag">DPDK</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/architecture/" rel="tag">architecture</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/qemu/" rel="tag">qemu</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vhost-net/" rel="tag">vhost-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-net/" rel="tag">virtio-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-networking/" rel="tag">virtio-networking</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-How-vhost-user-came-into-being-Virtio-networking-and-DPDK" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/" class="article-date">
  <time class="dt-published" datetime="2022-02-09T07:25:37.000Z" itemprop="datePublished">2022-02-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/virtualization/">virtualization</a>►<a class="article-category-link" href="/categories/virtualization/translation/">translation</a>►<a class="article-category-link" href="/categories/virtualization/translation/virtio-networking/">virtio-networking</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/">How vhost-user came into being Virtio-networking and DPDK</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在这篇文章里我们将会通过一个宏观的视角介绍一个基于DPDK（Data Plane Development Kit）在host和guest之间的解决方案。这篇文章将会附带一篇面向架构师/研发人员的详细介绍以及一篇提供实际操作帮助的文章。</p>
<p>之前的文章里面包括解决方案，技术介绍以及实践的文章，引导读者了解virtio-networking的生态，包括了基本的组件，kvm，qemu，libvirt，以及vhost protocol和vhost-net/virtio-net架构。这个架构是给予host kernel的vhost-net（后端）和guest kernel的virtio-net（前端）组成的。</p>
<p>vhost-net/virtio-net架构提供了一个这些年来被广泛部署使用的生产解决方案。一部分是因为这个方案对用户开发应用并在虚拟机里运行因为它是用的是标准的Linux sockets来连接到网络的（通过host）。另一方面这个解决方案并不是那么完美，里面还是包含了一些性能开销的，这个问题在后面会被详细解释。</p>
<p>为了讲清楚性能问题，我们将会介绍vhost-user/virtio-pmd架构。为了了解细节，我们会先回顾一下DPDK，如何将OVS连接到DPDK以及virtio是如何适配到这个架构的前端和后端里去的。</p>
<p>在这篇文章的最后，你会对vhost-user/virtio-pmd架构以及这个架构与vhost-net/virtio-net的不同有牢固的认识。</p>
<h2 id="DPDK-overview"><a href="#DPDK-overview" class="headerlink" title="DPDK overview"></a>DPDK overview</h2><p>DPDK目标是提供一个简单和完善的用于数据面应用快速包处理的架构。它实现了一个数据包处理的运行时完成模型，意思是说，所有资源都需要在运行数据面应用之前分配好。这些专门的资源只会被专门的逻辑处理核心处理。</p>
<p>这个设计和Linux kernel通过调度器+中断在进程间上下文切换的机制不同，DPDK架构中设备是被一个定时的polling访问的。这个设计去除了上先问切换以及进程中断带来的开销，保证CPU核心100%都在做包处理。</p>
<p>在实践中，DPDK提供了一系列poll模式的驱动（PMDs）是的包传输能够直接在用户态和物理接口之间进行，完全跳过了kernel网络栈。这个方法提供了一个重要的通过排除中断处理以及kernel网络协议栈提升kernel转发性能的方法。</p>
<p>DPDK是一系列库。因此为了使用它们，你需要一个link到这些库并且调用相关api的应用。</p>
<p>下面的图标展示了之前的virtio构件和一个DPDK应用使用PMD驱动来访问物理网卡（跳过了kernel）：</p>
<img src="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/2019-09-20-virtio-and-dpdk-fig1.jpeg" class="">

<h2 id="OVS-DPDK-overview"><a href="#OVS-DPDK-overview" class="headerlink" title="OVS-DPDK overview"></a>OVS-DPDK overview</h2><p>在之前的文章中介绍过，open vSwtich通常在内核空间的数据路径做包转发，这意味着OVS kernel模块包含了一个简单的记录收到的转发包的flow table。然后一小部分的包我们可以称为异常包（比如第一个打开Openflow flow的包）并不匹配任何内核空间中已经存在的条目，而是发送到用户态的OVS守护进程（ovs-vswitchd）来处理。守护进程会分析这个包然后更新OVS的kernell里的flow table然后后面的发送到这个flow的包就能够直接通过OVS的内核态转发表直接发走了。</p>
<p>这个方法排除了大部分流量的用户态内核态的上下文切换，然而我们仍然被linux的网络协议栈限制，因为它并不适合高频率包的用户场景。</p>
<p>如果我们把OVS和DPDK集成在一起，把前面提到的PMD驱动当作杠杆然后移动OVS内核模块转发表到用户态。</p>
<p>下面的图展示了OVS-DPDK应用，所有的OVS组件都在用户态运行，并通过PMD驱动和物理网卡通信：</p>
<img src="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/2019-09-20-virtio-and-dpdk-fig2.jpeg" class="">

<p>这里要提一下，虽然我们只看到DPDK应用运行在host的用户空间，在guest里运行带PMD驱动的DPDK应用也是可以的。下一节我们将会详细解释这个。</p>
<h2 id="The-vhot-user-virtio-pmd-architecture"><a href="#The-vhot-user-virtio-pmd-architecture" class="headerlink" title="The vhot-user/virtio-pmd architecture"></a>The vhot-user/virtio-pmd architecture</h2><p>在vhost-user/virtio-pmd架构，virtio会在host的用户态guest的用户态使用DPDK：</p>
<ol>
<li>vhost-user（后端） 运行在host用户空间，作为OVS-DPDK的用户态应用。之前提到的DPDK是一个库而vhost-user模块是附带在这些库里面的API。OVS-DPDK是确切的链接到这个库并调用API的应用。任意一个创建在host上的guest VM都会有一个对应的vhost-user被创建出来用来和guest的virtio前端通信。</li>
<li>virtio-pmd（前端）运行在guest用户态，是一个poll模式驱动，消费专门的cores并且执行不会中断的polling。一个运行在用户态的应用消费virtio-pmd也需要连接到DPDK库</li>
</ol>
<p>这个图展示了他们是如何一起运作的：</p>
<img src="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/2019-09-20-virtio-and-dpdk-fig3.jpeg" class="">

<p>如果把这个架构和基于内核的vhost-net/virtio-net架构做对比，vhost-net被vhost-user取代了，而virtio-net则被virtio-pmd取代。</p>
<p>通过启用host用户态通过共享内存跳过kernel直接访问物理网卡然后通过virtio-pmd在guest的用户态也跳过kernel，整体的性能能够提升2-4倍</p>
<p>然而这个方法对用户能力有更多的要求，在vhost-net/virtio-net架构中，数据面通讯是直接通过guest OS视角的：简单的安装virtio驱动到guest kernel然后guest用户态应用自动获得了一个标准的Linux网络接口。</p>
<p>相反vhost-user/virtio-pmd架构，guest的用户态应用为了优化数据面被要求使用virtio-pmd驱动（DPDK库提供）。这不是一个很简单的任务，并且要求专业的DPDK的配置和使用知识。</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>这篇文章我们介绍了vhost-user/virtio-pmd架构，通过提升了一部分使用成本改善了virtio接口的性能因为我们现在需要把应用link并使用DPDK。</p>
<p>这里有一系列用户场景比如虚拟网络功能（VNFs, virtial network functions)性能是一个大缺陷而virtio DPDK的架构能够帮助实现对应的性能指标。然而开发应用是需要专业知识的，并需要对DPDK API的理解以及不同的优化。</p>
<p>下一篇文章我们会提供一个深入的vhost-net/virtio-pmd的内部架构以及不同控制面数据面组件的介绍。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/" data-id="cld1mhea10007yuwbcpc5alsm" data-title="How vhost-user came into being Virtio-networking and DPDK" class="article-share-link">Share</a>
      
      
        <a href="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DPDK/" rel="tag">DPDK</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/qemu/" rel="tag">qemu</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vhost-net/" rel="tag">vhost-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-net/" rel="tag">virtio-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-networking/" rel="tag">virtio-networking</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-hands-on-vhost-net-do-or-do-not-there-is-no-try" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/12/28/hands-on-vhost-net-do-or-do-not-there-is-no-try/" class="article-date">
  <time class="dt-published" datetime="2021-12-28T13:55:31.000Z" itemprop="datePublished">2021-12-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/virtualization/">virtualization</a>►<a class="article-category-link" href="/categories/virtualization/translation/">translation</a>►<a class="article-category-link" href="/categories/virtualization/translation/virtio-networking/">virtio-networking</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/12/28/hands-on-vhost-net-do-or-do-not-there-is-no-try/">Hands on vhost-net: Do. Or do not. There is no try</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Vhost-net利用标准的virtio网络接口悄悄地成为了基于qemu-kvm的虚拟化环境的默认流量卸载机制。这个机制允许通过内核模块执行网络处理，解放了qemu进程，改进了网络性能。</p>
<p>在之前的文章里介绍了网络架构的组成：<a href="#">Post not found: introduction-virtio-networking-and-vhost-net [Introduction to virtio-networking and vhost-net]</a>  同时提供了一个更加详细的解释：<a href="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/" title="[Deep dive into Virtio-networking and vhost-net]">[Deep dive into Virtio-networking and vhost-net]</a>  。这篇文章里我们将会提供一个详细的步骤实践性的设置一个架构。大建好之后我们能够检查主要的组件是如何工作的。</p>
<p>这篇文章主要面向研发人员，hackers和其他任何有兴趣学习真实的网络卸载是如何做到的。</p>
<p>在读完这篇文章之后（希望能够在你的PC上重建这个环境），你将会对虚拟化使用到的工具更加熟悉（比如virsh）。你将了解如何建立一个vhost-net环境并且了解到如何检查一个运行的云主机同时测试他的网络性能。</p>
<p>对那些需要快速建立环境并直接逆向工程的人，这里有特别的准备！<a target="_blank" rel="noopener" href="https://github.com/redhat-virtio-net/virtio-hands-on">https://github.com/redhat-virtio-net/virtio-hands-on</a> 能够自动化部署环境的ansible脚本。</p>
<h2 id="Setting-things-up"><a href="#Setting-things-up" class="headerlink" title="Setting things up"></a>Setting things up</h2><p>因为懒得准备原文中相同的环境，这里我用ZStack常用环境来做替代</p>
<h3 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h3><p>一个安装了CentOS Linux release 7.6.1810 (Core)的环境<br>使用root用户（或者有sudo权限的用户）<br>home目录下有25G以上的空闲空间<br>至少8GB的RAM<br>首先安装一堆依赖，建议通过ZStack iso安装可以选择Host模式，如果是专家模式需要通过，如下命令安装</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum --disablerepo&#x3D;* --enablerepo&#x3D;zstack-mn,qemu-kvm-ev-mn install libguestfs-tools qemu-kvm libvirt kernel-tools iperf3 -y</span><br></pre></td></tr></table></figure>

<p>另外根据OS版本下载一个rpm包 <a target="_blank" rel="noopener" href="https://pkgs.org/download/netperf">https://pkgs.org/download/netperf</a></p>
<p>对应的Centos 7的包是 <a target="_blank" rel="noopener" href="https://centos.pkgs.org/7/lux/netperf-2.7.0-1.el7.lux.x86_64.rpm.html">https://centos.pkgs.org/7/lux/netperf-2.7.0-1.el7.lux.x86_64.rpm.html</a></p>
<p>安装virt-install</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum --disablerepo&#x3D;* --enablerepo&#x3D;ali* install virt-install -y</span><br></pre></td></tr></table></figure>

<p>接下来确保当前用户被加入了libvirt的用户组，做一下修改</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -a -G libvirt $(whoami)</span><br></pre></td></tr></table></figure>

<p>然后重新登录，并重启libvirt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart libvirtd</span><br></pre></td></tr></table></figure>

<h3 id="Creating-VM"><a href="#Creating-VM" class="headerlink" title="Creating VM"></a>Creating VM</h3><p>首先下载一个镜像，可以在内部 <a target="_blank" rel="noopener" href="http://192.168.200.100/mirror/diskimages/">http://192.168.200.100/mirror/diskimages/</a> 找一个，这里直接用的一个c76的镜像</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;archive.fedoraproject.org&#x2F;pub&#x2F;archive&#x2F;fedora&#x2F;linux&#x2F;releases&#x2F;30&#x2F;Cloud&#x2F;x86_64&#x2F;images&#x2F;Fedora-Cloud-Base-30-1.2.x86_64.qcow2</span><br></pre></td></tr></table></figure>

<p>这是一个封装好的镜像，我们把他作为一个copy，保证以后可以继续使用，执行如下命令，创建并查一下一下image的信息是否和预期一致</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# qemu-img create -f qcow2 -b Fedora-Cloud-Base-30-1.2.x86_64.qcow2 virtio-test1.qcow2 20G</span><br><span class="line">Formatting &#39;virtio-test1.qcow2&#39;, fmt&#x3D;qcow2 size&#x3D;21474836480 backing_file&#x3D;centos76.qcow2 cluster_size&#x3D;65536 lazy_refcounts&#x3D;off refcount_bits&#x3D;16</span><br><span class="line">[root@host ~]# qemu-img info virtio-test1.qcow2</span><br><span class="line">image: virtio-test1.qcow2</span><br><span class="line">file format: qcow2</span><br><span class="line">virtual size: 20 GiB (21474836480 bytes)</span><br><span class="line">disk size: 196 KiB</span><br><span class="line">cluster_size: 65536</span><br><span class="line">backing file: centos76.qcow2</span><br><span class="line">Format specific information:</span><br><span class="line">    compat: 1.1</span><br><span class="line">    lazy refcounts: false</span><br><span class="line">    refcount bits: 16</span><br><span class="line">    corrupt: false</span><br></pre></td></tr></table></figure>

<p>然后清理一下这个操作系统：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo virt-sysprep --root-password password:password123 --uninstall cloud-init --selinux-relabel -a virtio-test1.qcow2</span><br></pre></td></tr></table></figure>

<p>这个命令会挂载文件系统，并自动做一些基础设置，让这个镜像准备好启动</p>
<p>我们需要把网络连接到虚拟机网络。Libvirt对网络的配置就和管理虚拟机一样，你可以通过xml文件定义一个网络，通过命令行控制他的启动和停止。</p>
<p>举个例子，我们使用一个libvirt提供的叫做‘default’的网络的便利设置用如下的命令定义<code>default</code>网络启动并检测他已经运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# virsh net-define &#x2F;usr&#x2F;share&#x2F;libvirt&#x2F;networks&#x2F;default.xml</span><br><span class="line">Network default defined from &#x2F;usr&#x2F;share&#x2F;libvirt&#x2F;networks&#x2F;default.xml</span><br><span class="line">[root@host ~]# virsh net-start default</span><br><span class="line">Network default started</span><br><span class="line">[root@host ~]# virsh net-list</span><br><span class="line"> Name      State    Autostart   Persistent</span><br><span class="line">--------------------------------------------</span><br><span class="line"> default   active   no          yes</span><br></pre></td></tr></table></figure>

<p>最后我们能够使用virt-install创建虚拟机。这是一个命令行工具创建一堆操作系统需要的定义，并给出一个基础的我们可以自定义的配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virt-install --import --name virtio-test1 --ram&#x3D;4096 --vcpus&#x3D;2 --nographics --accelerate --network network:default,model&#x3D;virtio --mac 02:ca:fe:fa:ce:01       --debug --wait 0 --console pty --disk &#x2F;root&#x2F;virtio-test1.qcow2,bus&#x3D;virtio</span><br></pre></td></tr></table></figure>

<p>这些命令用的选项特定了vCPUs的数量，还有我们虚拟机的RAM大小并且指定磁盘的路径和云主机要连接的网络。</p>
<p>除开虚拟机通过我们这些选项定义之外，virt-install命令也能够启动虚拟机，所以我们需要列出来：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# virsh list</span><br><span class="line"> Id   Name           State</span><br><span class="line">------------------------------</span><br><span class="line"> 9    virtio-test1   running</span><br></pre></td></tr></table></figure>

<p>我们的虚拟机在运行了</p>
<p>提醒一下，virsh是一个libvirt的命令行接口，你可以这样启动一个虚拟机：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh start virtio-test1</span><br></pre></td></tr></table></figure>

<p>进入虚拟机的console：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh console virtio-test1</span><br></pre></td></tr></table></figure>

<p>停止一个运行的虚拟机：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh shutdown virtio-test1</span><br></pre></td></tr></table></figure>

<p>删除运行的虚拟机（不要做这个除非你想在创建一遍）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">virsh undefine virtio-test1</span><br><span class="line">Inspecting the guest</span><br></pre></td></tr></table></figure>

<p>就像已经提到的，virt-install命令能够自动使用libvirt创建和启动云主机。每个虚拟机创建都是通过xml文件描述需要模拟的硬件设置并提交给libvirt。我们通过dump配置的内容可以看一下相关的文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;devices&gt;</span><br><span class="line">...</span><br><span class="line">    &lt;interface type&#x3D;&#39;network&#39;&gt;</span><br><span class="line">      &lt;mac address&#x3D;&#39;02:ca:fe:fa:ce:01&#39;&#x2F;&gt;</span><br><span class="line">      &lt;source network&#x3D;&#39;default&#39; bridge&#x3D;&#39;virbr0&#39;&#x2F;&gt;</span><br><span class="line">      &lt;target dev&#x3D;&#39;vnet0&#39;&#x2F;&gt;</span><br><span class="line">      &lt;model type&#x3D;&#39;virtio&#39;&#x2F;&gt;</span><br><span class="line">      &lt;alias name&#x3D;&#39;net0&#39;&#x2F;&gt;</span><br><span class="line">      &lt;address type&#x3D;&#39;pci&#39; domain&#x3D;&#39;0x0000&#39; bus&#x3D;&#39;0x00&#39; slot&#x3D;&#39;0x02&#39; function&#x3D;&#39;0x0&#39;&#x2F;&gt;</span><br><span class="line">    &lt;&#x2F;interface&gt;</span><br><span class="line">...</span><br><span class="line">&lt;&#x2F;devices&gt;</span><br></pre></td></tr></table></figure>

<p>我们能够看到一个virtio设备被创建好了，并连接到网络，这堆配置里有virbr0。这个设备有PCI，bus和slot</p>
<p>然后通过console命令进入console</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh console virtio-test1</span><br></pre></td></tr></table></figure>

<p>进入guset安装一些测试依赖：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dnf install pciutils iperf3</span><br></pre></td></tr></table></figure>

<p>然后在虚拟机里看一下，实际上虚拟PCI总线上挂了个网络设备</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# lspci -s 0000:00:02.0 -v</span><br><span class="line">00:02.0 Ethernet controller: Red Hat, Inc. Virtio network device</span><br><span class="line">	Subsystem: Red Hat, Inc. Device 0001</span><br><span class="line">	Physical Slot: 2</span><br><span class="line">	Flags: bus master, fast devsel, latency 0, IRQ 11</span><br><span class="line">	I&#x2F;O ports at c040 [size&#x3D;32]</span><br><span class="line">	Memory at febc0000 (32-bit, non-prefetchable) [size&#x3D;4K]</span><br><span class="line">	Memory at febf4000 (64-bit, prefetchable) [size&#x3D;16K]</span><br><span class="line">	Expansion ROM at feb80000 [disabled] [size&#x3D;256K]</span><br><span class="line">	Capabilities: [98] MSI-X: Enable+ Count&#x3D;3 Masked-</span><br><span class="line">	Capabilities: [84] Vendor Specific Information: VirtIO: &lt;unknown&gt;</span><br><span class="line">	Capabilities: [70] Vendor Specific Information: VirtIO: Notify</span><br><span class="line">	Capabilities: [60] Vendor Specific Information: VirtIO: DeviceCfg</span><br><span class="line">	Capabilities: [50] Vendor Specific Information: VirtIO: ISR</span><br><span class="line">	Capabilities: [40] Vendor Specific Information: VirtIO: CommonCfg</span><br><span class="line">	Kernel driver in use: virtio-pci</span><br></pre></td></tr></table></figure>

<p>注意：lspci后面跟的地址是根据xml里面的source，bus，slot，function拼接起来的。</p>
<p>除了典型的PCI设备信息之外（比如内存阈和功能之外，驱动还实现了基于PCI的通用virtio功能吗，并创建了一个由virtio_net驱动的网络设备，比如我们可以深入的看一下这个设备</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# readlink &#x2F;sys&#x2F;devices&#x2F;pci0000\:00&#x2F;0000\:00\:02.0&#x2F;virtio0&#x2F;driver</span><br><span class="line">..&#x2F;..&#x2F;..&#x2F;..&#x2F;bus&#x2F;virtio&#x2F;drivers&#x2F;virtio_net</span><br></pre></td></tr></table></figure>

<p>通过命令行readlink，可以看到这个pci设备使用的是virtio_net驱动。</p>
<p>是这个virtio_net驱动控制了操作系统使用的网络接口的创建：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link&#x2F;loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link&#x2F;ether 02:ca:fe:fa:ce:01 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">Inspecting the host</span><br></pre></td></tr></table></figure>
<p>我们已经看过guset了，让我们再看看host。注意我们通过‘network’类型配置网络接口的默认行为是使用vhost-net</p>
<p>首先我们看一下vhost-net是否加载</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# lsmod | grep vhost</span><br><span class="line">vhost_net              22507  1</span><br><span class="line">tun                    31881  4 vhost_net</span><br><span class="line">vhost                  48422  1 vhost_net</span><br><span class="line">macvtap                22796  1 vhost_net</span><br></pre></td></tr></table></figure>

<p>我们能够检查到QEMU使用了tun，kvm和vhost-net设备，同时通过检查/proc文件系统也能发现这些文件描述符被分给qemu处理了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# ls -lh &#x2F;proc&#x2F;40888&#x2F;fd | grep &#39;&#x2F;dev&#39;</span><br><span class="line">lrwx------ 1 root root 64 Dec 27 22:55 0 -&gt; &#x2F;dev&#x2F;null</span><br><span class="line">lrwx------ 1 root root 64 Dec 27 22:55 10 -&gt; &#x2F;dev&#x2F;ptmx</span><br><span class="line">lrwx------ 1 root root 64 Dec 27 22:55 13 -&gt; &#x2F;dev&#x2F;kvm</span><br><span class="line">lr-x------ 1 root root 64 Dec 27 22:55 3 -&gt; &#x2F;dev&#x2F;urandom</span><br><span class="line">lrwx------ 1 root root 64 Dec 27 22:55 35 -&gt; &#x2F;dev&#x2F;net&#x2F;tun</span><br><span class="line">lrwx------ 1 root root 64 Dec 27 22:55 37 -&gt; &#x2F;dev&#x2F;vhost-net</span><br></pre></td></tr></table></figure>

<p>这意味着qemu进程，不仅打开了kvm设备执行虚拟化操作，同时也创建了一个tun/tap设备和一打开了一个vhost-net设备。当然我们也能够看到一个辅助qemu的vhost内核线程也被创建出来了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# ps -ef | grep &#39;\[vhost&#39;</span><br><span class="line">root     40056 21741  0 09:53 pts&#x2F;0    00:00:00 grep --color&#x3D;auto \[vhost</span><br><span class="line">root     40894     2  0 Dec27 ?        00:00:03 [vhost-40888]</span><br><span class="line">[root@host ~]# pgrep qemu</span><br><span class="line">40888</span><br></pre></td></tr></table></figure>

<p>这个vhost内核线程的名字就是vhost-$qemu_pid</p>
<p>最后我们可以看到qemu进程创建的tun接口（上面通过/proc找到的）通过bridge把host和guest连在一起了。注意，虽然tap设备被挂在了qemu进程上，实际上进行tap设备读写的是vhost内核线程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# ip -d tuntap</span><br><span class="line">virbr0-nic: tap UNKNOWN_FLAGS:800</span><br><span class="line">	Attached to processes:</span><br><span class="line">vnet0: tap vnet_hdr</span><br><span class="line">	Attached to processes: qemu-kvm(40888)</span><br></pre></td></tr></table></figure>

<p>OK，所以vhost已经启动并且运行了，qemu也连接到了vhost上。现在我们可以制造一些流量来看看系统的表现。</p>
<h2 id="Generating-traffic"><a href="#Generating-traffic" class="headerlink" title="Generating traffic"></a>Generating traffic</h2><p>如果你已经完成之前的步骤的话，你已经可以尝试通过ip地址从host发送数据到guest或者反过来。举个例子，通过iperf3测试网络性能，注意这些测试方法不是正确的benchmarks，不同的输入比如软硬件版本，不同的网络协议栈参数，会显著影响测试结果。性能吞吐量，或者是特定的使用量基准不在本文的范围之内。</p>
<p>首先检查guest的ip地址，执行 iperf3 server (或者任意你打算用来测试连通性的工具）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ip addr</span><br><span class="line">...</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</span><br><span class="line">    link&#x2F;ether 02:ca:fe:fa:ce:01 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.122.41&#x2F;24 brd 192.168.122.255 scope global dynamic noprefixroute eth0</span><br><span class="line">       valid_lft 2808sec preferred_lft 2808sec</span><br><span class="line">    inet6 fe80::ca:feff:fefa:ce01&#x2F;64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<p>然后再host上运行iperf3的client：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# iperf3 -c 192.168.122.41</span><br><span class="line">Connecting to host 192.168.122.41, port 5201</span><br><span class="line">[ ID] Interval           Transfer     Bandwidth       Retr</span><br><span class="line">[  4]   0.00-10.00  sec  34.3 GBytes  29.5 Gbits&#x2F;sec    1             sender</span><br><span class="line">[  4]   0.00-10.00  sec  34.3 GBytes  29.5 Gbits&#x2F;sec                  receiver</span><br></pre></td></tr></table></figure>

<p>在iperf3的输出里我们能看到一个29.5 Gbits/sec 的传输速率（主机这个网络的带宽收到很多以来的影响，不要假定会和你的环境一致）。我们可以通过 -l 参数修改包的大小来测试更多数据平面。</p>
<p>如果我们在iperf3测试过程中运行top命令我们能够看到vhost-$pid内核线程使用了100%的core来做包转发，同时QEMU使用了几乎两倍的cores（可以多试几次，刚好中间观察到一个200% 一个100%，注意创建guest的时候指定的qemu的vcpu数量就是2）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">top - 10:07:24 up 7 days, 17:30,  3 users,  load average: 1.49, 0.55, 0.28</span><br><span class="line">Tasks: 612 total,   2 running, 610 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s):  2.9 us,  6.6 sy,  0.0 ni, 90.4 id,  0.0 wa,  0.0 hi,  0.2 si,  0.0 st</span><br><span class="line">KiB Mem : 13174331+total, 10001235+free,  4672644 used, 27058324 buff&#x2F;cache</span><br><span class="line">KiB Swap:  4194300 total,  4194300 free,        0 used. 12307848+avail Mem</span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</span><br><span class="line">40888 root      20   0 6158388   1.2g  11436 S 200.0  0.9   3:38.41 qemu-kvm</span><br><span class="line">40894 root      20   0       0      0      0 R 100.0  0.0   0:58.46 vhost-40888</span><br></pre></td></tr></table></figure>

<p>要测试延迟，我们使用netperf命令启动一个netperf服务，然后测试延迟。（注：需要在guest里先启动一个netserver，如何在guest安装netperf 2021.12.18 fedora安装netperf）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# netperf -l 30 -H 192.168.122.41 -p 16604 -t TCP_RR</span><br><span class="line">MIGRATED TCP REQUEST&#x2F;RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 192.168.122.41 () port 0 AF_INET : first burst 0</span><br><span class="line">Local &#x2F;Remote</span><br><span class="line">Socket Size   Request  Resp.   Elapsed  Trans.</span><br><span class="line">Send   Recv   Size     Size    Time     Rate</span><br><span class="line">bytes  Bytes  bytes    bytes   secs.    per sec</span><br><span class="line"></span><br><span class="line">16384  87380  1        1       30.00    36481.26</span><br><span class="line">16384  131072</span><br></pre></td></tr></table></figure>

<p>计算打开vhost-net host → guest TCP_RR延迟为 1 / 36481.26 = 0.0000274s</p>
<p>就像之前说的，我们进行的不是benchmark或者是吞吐测试。我们只是熟悉一下这个方法。</p>
<h2 id="Extra-Disable-vhost-net"><a href="#Extra-Disable-vhost-net" class="headerlink" title="Extra: Disable vhost-net"></a>Extra: Disable vhost-net</h2><p>就像我们看到的vhost-net是被作为默认行为的，因为带来了性能的提升。然而因为我们是来实践学习的，金庸vhost-net来看一下性能有什么不同，通过这个我们将知道qemu做了多“重”的包处理的操作，以及对性能造成了什么影响。</p>
<p>首先停止vm：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh shutdown virtio-test1</span><br></pre></td></tr></table></figure>

<p>编辑云主机配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh edit virtio-test1</span><br></pre></td></tr></table></figure>

<p>修改网卡配置为，增加了<driver name='qemu'/></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;devices&gt;</span><br><span class="line">...</span><br><span class="line">    &lt;interface type&#x3D;&#39;network&#39;&gt;</span><br><span class="line">      &lt;mac address&#x3D;&#39;02:ca:fe:fa:ce:01&#39;&#x2F;&gt;</span><br><span class="line">      &lt;source network&#x3D;&#39;default&#39;&#x2F;&gt;</span><br><span class="line">      &lt;model type&#x3D;&#39;virtio&#39;&#x2F;&gt;</span><br><span class="line">      &lt;driver name&#x3D;&#39;qemu&#39;&#x2F;&gt;</span><br><span class="line">      &lt;address type&#x3D;&#39;pci&#39; domain&#x3D;&#39;0x0000&#39; bus&#x3D;&#39;0x00&#39; slot&#x3D;&#39;0x02&#39; function&#x3D;&#39;0x0&#39;&#x2F;&gt;</span><br><span class="line">    &lt;&#x2F;interface&gt;</span><br><span class="line">...</span><br><span class="line">&lt;&#x2F;devices&gt;</span><br></pre></td></tr></table></figure>

<p>修改成功后llibvirt会显示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Domain virtio-test1 XML configuration not changed</span><br></pre></td></tr></table></figure>

<p>然后启动云主机：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh start virtio-test1</span><br></pre></td></tr></table></figure>

<p>我们可以检查一下指向/dev/vhost-net的文件描述符没了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# ls -lh &#x2F;proc&#x2F;37518&#x2F;fd | grep &#39;&#x2F;dev&#39;</span><br><span class="line">lrwx------ 1 root root 64 Dec 28 11:22 0 -&gt; &#x2F;dev&#x2F;null</span><br><span class="line">lrwx------ 1 root root 64 Dec 28 11:22 10 -&gt; &#x2F;dev&#x2F;ptmx</span><br><span class="line">lrwx------ 1 root root 64 Dec 28 11:22 13 -&gt; &#x2F;dev&#x2F;kvm</span><br><span class="line">lr-x------ 1 root root 64 Dec 28 11:22 3 -&gt; &#x2F;dev&#x2F;urandom</span><br><span class="line">lrwx------ 1 root root 64 Dec 28 11:22 33 -&gt; &#x2F;dev&#x2F;net&#x2F;tun</span><br><span class="line">Analyzing the performance impact</span><br></pre></td></tr></table></figure>

<p>如果我们在没有vhost-net的环境重复之前的测试，我们能看到vhost-net的线程没有在运行了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# ps -ef | grep &#39;\[vhost&#39;</span><br><span class="line">root      9076 23993  0 11:24 pts&#x2F;0    00:00:00 grep --color&#x3D;auto \[vhost</span><br></pre></td></tr></table></figure>

<p>我们获得的性能iperf3测试结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# iperf3 -c 192.168.122.41</span><br><span class="line">Connecting to host 192.168.122.41, port 5201</span><br><span class="line">[  4] local 192.168.122.1 port 58628 connected to 192.168.122.41 port 5201</span><br><span class="line">[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd</span><br><span class="line">[  4]   0.00-1.00   sec  1.89 GBytes  16.2 Gbits&#x2F;sec    0   2.08 MBytes</span><br><span class="line">[  4]   1.00-2.00   sec  1.78 GBytes  15.3 Gbits&#x2F;sec    0   2.19 MBytes</span><br><span class="line">[  4]   2.00-3.00   sec  1.82 GBytes  15.6 Gbits&#x2F;sec    0   2.37 MBytes</span><br><span class="line">[  4]   3.00-4.00   sec  1.82 GBytes  15.7 Gbits&#x2F;sec    0   2.47 MBytes</span><br><span class="line">[  4]   4.00-5.00   sec  1.73 GBytes  14.8 Gbits&#x2F;sec    0   2.61 MBytes</span><br><span class="line">[  4]   5.00-6.00   sec  1.80 GBytes  15.4 Gbits&#x2F;sec    0   2.64 MBytes</span><br><span class="line">[  4]   6.00-7.00   sec  1.82 GBytes  15.6 Gbits&#x2F;sec    0   2.64 MBytes</span><br><span class="line">[  4]   7.00-8.00   sec  1.81 GBytes  15.6 Gbits&#x2F;sec    0   2.69 MBytes</span><br><span class="line">[  4]   8.00-9.00   sec  2.33 GBytes  20.0 Gbits&#x2F;sec    0   2.81 MBytes</span><br><span class="line">[  4]   9.00-10.00  sec  2.32 GBytes  19.9 Gbits&#x2F;sec    0   2.93 MBytes</span><br><span class="line">- - - - - - - - - - - - - - - - - - - - - - - - -</span><br><span class="line">[ ID] Interval           Transfer     Bandwidth       Retr</span><br><span class="line">[  4]   0.00-10.00  sec  19.1 GBytes  16.4 Gbits&#x2F;sec    0             sender</span><br><span class="line">[  4]   0.00-10.00  sec  19.1 GBytes  16.4 Gbits&#x2F;sec                  receiver</span><br><span class="line"></span><br><span class="line">iperf Done.</span><br></pre></td></tr></table></figure>

<p>可以看到传输速度从上面的29.5 Gbits/sec掉到了16.4 Gbits/sec</p>
<p>在通过top命令检查，我们可以发现qemu对CPU的使用，峰值会变得很高（这个需要多测试一下，是一个浮动的范围，关掉vhost-net之后大概是150% ～ 260%左右，之前开vhost-net的时候最高也就200%）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">top - 11:27:10 up 7 days, 18:50,  3 users,  load average: 0.57, 0.36, 0.29</span><br><span class="line">Tasks: 617 total,   3 running, 614 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s):  3.8 us,  4.0 sy,  0.0 ni, 91.7 id,  0.5 wa,  0.0 hi,  0.1 si,  0.0 st</span><br><span class="line">KiB Mem : 13174331+total, 10047221+free,  3931432 used, 27339668 buff&#x2F;cache</span><br><span class="line">KiB Swap:  4194300 total,  4194300 free,        0 used. 12381579+avail Mem</span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</span><br><span class="line">37518 root      20   0 6605056 514392  11372 R 242.9  0.4   1:02.22 qemu-kvm</span><br></pre></td></tr></table></figure>

<p>如果我们再比较一下两种不同网络架构下的TCP和UDP的延迟，我们可以看到vhost-net对两种形式的性能都有一致的提升</p>
<p>记录一下关闭vhoset-net之后的测试结果</p>
<p>关闭vhost-net的host → guest TCP_RR测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# netperf -l 30 -H 192.168.122.41 -p 16604 -t TCP_RR</span><br><span class="line">MIGRATED TCP REQUEST&#x2F;RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 192.168.122.41 () port 0 AF_INET : first burst 0</span><br><span class="line">Local &#x2F;Remote</span><br><span class="line">Socket Size   Request  Resp.   Elapsed  Trans.</span><br><span class="line">Send   Recv   Size     Size    Time     Rate</span><br><span class="line">bytes  Bytes  bytes    bytes   secs.    per sec</span><br><span class="line"></span><br><span class="line">16384  87380  1        1       30.00    27209.58</span><br><span class="line">计算延迟为 1&#x2F;27209.58 &#x3D; 0.0000367s</span><br></pre></td></tr></table></figure>

<p>关闭vhost-net的host → guest UDP_RR测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# netperf -l 30 -H 192.168.122.41 -p 16604 -t UDP_RR</span><br><span class="line">MIGRATED UDP REQUEST&#x2F;RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 192.168.122.41 () port 0 AF_INET : first burst 0</span><br><span class="line">Local &#x2F;Remote</span><br><span class="line">Socket Size   Request  Resp.   Elapsed  Trans.</span><br><span class="line">Send   Recv   Size     Size    Time     Rate</span><br><span class="line">bytes  Bytes  bytes    bytes   secs.    per sec</span><br><span class="line"></span><br><span class="line">212992 212992 1        1       30.00    27516.04</span><br></pre></td></tr></table></figure>

<p>计算延迟为 1/27516.04=0.0000363s</p>
<p>打开vhost-net的host → guest UDP_RR测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# netperf -l 30 -H 192.168.122.41 -p 16604 -t UDP_RR</span><br><span class="line">MIGRATED UDP REQUEST&#x2F;RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 192.168.122.41 () port 0 AF_INET : first burst 0</span><br><span class="line">Local &#x2F;Remote</span><br><span class="line">Socket Size   Request  Resp.   Elapsed  Trans.</span><br><span class="line">Send   Recv   Size     Size    Time     Rate</span><br><span class="line">bytes  Bytes  bytes    bytes   secs.    per sec</span><br><span class="line"></span><br><span class="line">212992 212992 1        1       30.00    37681.20</span><br></pre></td></tr></table></figure>

<p>计算延迟为 1/37681.20 = 0.0000265s</p>
<p>同样的测试一下guest→host方向的流量延迟，这里不列代码只记录测试结果</p>
<p>打开vhost-net guest→host TCP_RR 1/36030.14 = 0.0000278s</p>
<p>打开vhost-net guest→host UDP_RR 1/37690.97 = 0.0000265s</p>
<p>关闭vhost-net guest→host TCP_RR 1/26697.53 = 0.0000375s</p>
<p>关闭vhost-net guest→host UDP_RR 1/25850.89 = 0.0000387s</p>
<p>结果如下表</p>
<img src="/2021/12/28/hands-on-vhost-net-do-or-do-not-there-is-no-try/lantency.png" class="">

<p>使用strace统计系统调用，关闭vhost-net，测试iperf3的情况</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# strace -c -p 37518 # 进程的pid，统计结束后用ctrl+c结束</span><br><span class="line">strace: Process 37518 attached</span><br><span class="line">^Cstrace: Process 37518 detached</span><br><span class="line">% time     seconds  usecs&#x2F;call     calls    errors syscall</span><br><span class="line">------ ----------- ----------- --------- --------- ----------------</span><br><span class="line"> 41.63    1.137491           8    141188      2775 read</span><br><span class="line"> 28.66    0.783060           6    135331           ioctl</span><br><span class="line"> 27.16    0.742136           6    121757           writev</span><br><span class="line">  2.40    0.065491           8      8380           ppoll</span><br><span class="line">  0.13    0.003653           6       594       275 futex</span><br><span class="line">  0.01    0.000243           5        50           write</span><br><span class="line">  0.00    0.000020          20         1           clone</span><br><span class="line">  0.00    0.000012           3         4           sendmsg</span><br><span class="line">  0.00    0.000008           4         2           rt_sigprocmask</span><br><span class="line">------ ----------- ----------- --------- --------- ----------------</span><br><span class="line">100.00    2.732114                407307      3050 total</span><br></pre></td></tr></table></figure>

<p>打开vhost-net之后的iperf3测试时strace qemu的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# strace -c -p 27346</span><br><span class="line">strace: Process 27346 attached</span><br><span class="line">^Cstrace: Process 27346 detached</span><br><span class="line">% time     seconds  usecs&#x2F;call     calls    errors syscall</span><br><span class="line">------ ----------- ----------- --------- --------- ----------------</span><br><span class="line"> 99.96    6.819794      131150        52           ppoll</span><br><span class="line">  0.02    0.001416          10       136           write</span><br><span class="line">  0.01    0.000862          13        66           futex</span><br><span class="line">  0.00    0.000341           9        39           read</span><br><span class="line">  0.00    0.000083          10         8           sendmsg</span><br><span class="line">  0.00    0.000022          22         1           clone</span><br><span class="line">  0.00    0.000016           8         2           rt_sigprocmask</span><br><span class="line">------ ----------- ----------- --------- --------- ----------------</span><br><span class="line">100.00    6.822534                   304           total</span><br></pre></td></tr></table></figure>

<p>另外一个好方法就是看qmue发送了多少IOCTLs到KVM。因为每次I/O事件都需要qemu处理，qemu需要发送IOCTL给KVM来切换回VMX noroot的guest模式。我们可以通过strace分析qemu在每个syscall上花费的时间。根据上面两次strace的结果可以看到没打开vhost-net的情况，存在很多ioctl的调用，而打开vhost之后基本上只有ppoll。</p>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>在这篇文章里面我们完整了提供了一个创建一个QEMU + vhost-net的虚拟机，检查guest和host来理解这个架构的输入输出。我们也展示了性能是如何变化的。这个系列也到此为止。从 Introduction to virtio-networking and vhost-net 的总览到技术视角深入理解的 Deep dive into Virtio-networking and vhost-net 详细的解释了这些组件，现在展示完了如果配置，希望这些内容呢能够提供足够的资源给IT专家，架构师以及研发人员理解这个技术并开始和他一起工作。</p>
<p>下一个话题将会涉及 Userland networking and DPDK</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2021/12/28/hands-on-vhost-net-do-or-do-not-there-is-no-try/" data-id="cld1mheca005zyuwb5nnk6dee" data-title="Hands on vhost-net: Do. Or do not. There is no try" class="article-share-link">Share</a>
      
      
        <a href="/2021/12/28/hands-on-vhost-net-do-or-do-not-there-is-no-try/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2021/12/28/hands-on-vhost-net-do-or-do-not-there-is-no-try/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/architecture/" rel="tag">architecture</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/qemu/" rel="tag">qemu</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vhost-net/" rel="tag">vhost-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-net/" rel="tag">virtio-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-networking/" rel="tag">virtio-networking</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-deep-dive-virtio-networking-and-vhost-net" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/" class="article-date">
  <time class="dt-published" datetime="2021-12-22T11:17:19.000Z" itemprop="datePublished">2021-12-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/virtualization/">virtualization</a>►<a class="article-category-link" href="/categories/virtualization/translation/">translation</a>►<a class="article-category-link" href="/categories/virtualization/translation/virtio-networking/">virtio-networking</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/">deep dive virtio networking and vhost net</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在这篇文章里我们将会解释在 <a href="#">Post not found: introduction-virtio-networking-and-vhost-net [introduction]</a> 里描述的vhost-net架构，并通过技术视角来弄清楚所有东西是怎么协同工作的。这系列博客里的这部分内容是为了让你们更好的理解virtio-networking领域是如何将虚拟化和网络连接在一起的</p>
<p>本文主要主要是面向对有兴趣理解上一篇blog提到的vhost-net/virtio-net底层原理的架构师和研发人员</p>
<p>我们将从描述不同的virtio规范的标准组件和共享内存在hypervisor里如何组织的，QEMU如何模拟一个virtio网络设备以及一个guest使用根据virtio规范来实现一个虚拟化驱动来管理并和设备通讯的</p>
<p>在展示过QEMU virtio架构，我们将会分析I/O瓶颈和限制，同时我们将会用host的kernel来解决这个问题，同时最后给出一个宏观的vhost-net的架构</p>
<p>最后，我们将会展示如何通过在它所运行的host上使用OVS（一个开源的虚拟化，SDN，分布式交换机）连接虚拟机到外部网络</p>
<p>希望在读完这篇文章后，你能够理解vhost-net/virtio-net架构是如何工作的，同时能够理解这架构中每一个组件的目标和作用以及数据包是如何被发送和接收的</p>
<h2 id="Previous-Concepts"><a href="#Previous-Concepts" class="headerlink" title="Previous Concepts"></a>Previous Concepts</h2><p>在这个部分我们将会简短的介绍一些帮助你完全理解这篇文章需要知道的概念。对于精通这个问题的人是比较基础的内容，但主要是为了提供一个共同的基础</p>
<h3 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h3><p>让我们从最基础的开始。一个物理网卡（Nic，Network Interface Card）是一个真实的硬件组件，允许物理机连接到外部网络。网卡可以承担一些offload，比如代替CPU执行checksum计算，Segmentation Offload（把一个很大片的数据转换成很多切片，切片大小就是以太网MTU的大小）或者是Large Receive Offload（从CPU角度看，就是把很多数据包合成一个数据包）</p>
<p>另外我们还有tun/tap设备，虚拟化的用户态用来交换数据包的点对点网络设备。交换二层（ethernet frames）数据的叫做tap设备，如果交换 (IP packets)三层数据的就是tun设备。</p>
<p>当tun的kernel模块被加载之后会创建一个特殊的/dev/net/tun设备。进程可以创建tap设备，并发打开这个设备发送一些特殊的ioctl命令给这个设备。新创建的tap设备在。dev文件系统里存在一个名字，并且其他的进程可以打开，并发送接收Ethernet frames</p>
<h3 id="IPC，System-programming"><a href="#IPC，System-programming" class="headerlink" title="IPC，System programming"></a>IPC，System programming</h3><p>Unix sockets是一个在同一台物理机器上做进程间通信的有效方法。在这篇文章涉及的范围内，通讯的服务端监听了文件系统上一个路径下的Unix socket，因此一个客户端（client）可以连接到这个路径使用它。这样，进程间就可以交换消息了。注意，unix sockets也可以用来在进程间交换文件描述符。</p>
<p>eventfd是一个轻量级IPC的实现。虽然Unix sockets允许发送和接收各种消息，eventfd只是一个生产者可以修改，消费者可以读取的整型值。这个使得eventfd更适合用于等待通知机制，而不是传输信息的场景。</p>
<p>这两个IPC系统都为通信中的每个进程公开一个文件描述符。 fcntl调用对该文件描述符执行不同的操作，使它们成为非阻塞的（因此，如果没有可读取的内容，则读取操作会立即返回）。 ioctl调用遵循相同的模式，但实现特定于设备的操作，例如发送命令。</p>
<p>共享内存是我们要介绍的最后一个IPC方法。不同于提供一个进程间通讯的channel，共享内存使用进程的内存区域指向相同的内存页面，因此一个进程覆盖了这部分内存的修改也会影响其他进程之后的读操作。</p>
<h3 id="QEMU-and-device-emulation"><a href="#QEMU-and-device-emulation" class="headerlink" title="QEMU and device emulation"></a>QEMU and device emulation</h3><p>QEMU是一个host层的虚拟机模拟器，给guest提供了一系列不同的硬件和设备模型。对host来说，qemu是一个标准Linux可调度的标准进程，有自己的进程内存。在进程里QEMU分配了内存区域来给guest当作物理内存么，同时QEMU还要执行虚拟机的CPU指令</p>
<p>为了在裸机设备上执行I/O操作么，比如存储和网络，CPU必须给物理设备下发特殊的指令并访问特殊的内存区域，比如这个设备被映射到的内存区域</p>
<p>当guest访问这些内存区域的时候，控制面就返回到了执行设备透明模拟的guest的QEMU里</p>
<h3 id="KVM"><a href="#KVM" class="headerlink" title="KVM"></a>KVM</h3><p>Kernel-based Virtual Machine(KVM)是一个Linux内置的开源虚拟化技术。它为虚拟化软件提供硬件辅助，利用内置CPU虚拟化技术减少虚拟化开销（缓存、I/O、内存），提高安全性。</p>
<p>使用KVM，QEMU可以创建一个虚拟机，该虚拟机具有处理器识别的虚拟 CPU (vCPU)，运行native-speed指令。当特殊的比如需要和设备或者特殊内存交互的命令到达KVM的时候，vCPU会停下来，然后通知QEMU暂停的原因，然后hypervisor就会对这个事件作出反应了。</p>
<p>在常规的KVM操作里，hyervisor会打开/dev/kvm这个设备，然后通过ioctl和他通讯调用它创建虚拟机，增加CPU，增加内存（qemu分配，但是虚拟机看来是物理内存），发送CPU中断（外部设备引发的），等等。举个例子，某一个ioctl的命令运行了KVM vCPU，阻塞了QEMU而且vCPU需要等到它找到了需要硬件辅助的命令。那时ioctl就会返回（这个叫做vmexit）同时QEMU也能知道这个exit的原因（比如offending instruction)。</p>
<p>对一些特别的内存区域，KVM有类似的访问方式，把内存区域标记为只读或者完全不映射，通过KVM_EXIT_MMIO造成一个vmexit。</p>
<h3 id="The-virtio-specification"><a href="#The-virtio-specification" class="headerlink" title="The virtio specification"></a>The virtio specification</h3><h4 id="Virtio-specification-devices-and-drivers"><a href="#Virtio-specification-devices-and-drivers" class="headerlink" title="Virtio specification: devices and drivers"></a>Virtio specification: devices and drivers</h4><p>Virtio是一个虚拟机数据I/O的一个开放规范，提供了简单、有效、标准并且可拓展的虚拟设备机制，而不是固定在在每个环境或每个操作系统的机制。它主要基于guest可以和host共享内存以进行I/O来实现。</p>
<p>virtio规范基于两个元素：设备和驱动。在最经典的实现里，hypervisor通过一系列传输方法试将virtio设备暴露给guest。设计上他们在虚拟机内看起来是物理设备。</p>
<p>嘴常见的传输方法就是PCI或者PCIe总线。然而，这些设备在一些预定义好的guest内存地址是可用的（MMIO transport）。这些设备可以完全在没有物理设备或者是物理的兼容性接口的情况下被虚拟出来。</p>
<p>最典型最简单的暴露virtio设备的方法是通过PCI端口因为我们可以借用PCI已经是一个成熟并且在QEMU和Linux驱动自持的很好的协议。实际的PCI硬件通过特殊的物理内存地址范围和/或特殊的处理器指令暴露配置空间（比如，驱动可以通过这些内存范围读或者写设备的寄存器）。在虚拟机世界里，hypervisor能捕获访问这些内存范围并且执行设备模拟，暴露和真实设备相同的内存布局，并提供相同的返回。virtio标准也定义了PCI配置空间的布局，因此实现它是很简单的。</p>
<p>当guest驱动并使用PCI/PCI自动发现机制的时候，virtio设备通过PCI vendor ID和他们的PCI device ID标识自己。guest kernel通过使用这些标记来知道使用哪些驱动来处理这些设备。特别是，linux内核已经包含了virtio驱动程序。</p>
<p>virtio驱动必须能够分配给hypervisor和设备都能读写的内存区域，比如通过共享内存。我们把数据平面作为使用内存区域进行数据通讯的一个部分，同时控制平面主要是来配置他们。我们会在后续的文章里提供一个更深层次的virtio协议的实现细节以及内存布局。</p>
<p>virtio内核驱动共享了一个通用的传输专用接口（virtio-pci），并被用于实际的传输以及设备实现（比如virtio-net，virtio-scsi）</p>
<h4 id="Virtio-specification-virtqueues"><a href="#Virtio-specification-virtqueues" class="headerlink" title="Virtio specification: virtqueues"></a>Virtio specification: virtqueues</h4><p>Virtqueues是virtio设备的批量数据传输机制。每个设备可以有0个或者多个virtqueue <a href="xxxx">link</a>。它由guest分配的host可以访问并且可以读或者写的缓存队列组成。补充一下，virtio标准也定义了双向的通知：</p>
<ul>
<li>Available Buffer Notification：使用驱动来通知buffer就绪并可以被设备处理</li>
<li>Used Buffer Notification：被设备使用来通知已经处理完了一些buffers</li>
</ul>
<p>在PCI的场景，guest通过写一个特殊的内存地址发送available buffer notification，然后设备（这个场景里是QEMU）使用vCPU中断来发送used buffer notification。</p>
<p>virtio规范也允许notifications动态的启用或者停用。这种情况下设备和驱动可以批量的缓存通知或者主动的向virtqueues请求新的缓存。这个方法更适合高流量的场景。</p>
<p>总结一下，virtio驱动接口暴露了一下内容：</p>
<ul>
<li>Device’s feature bits（设备和guest需要协商的部分）</li>
<li>Status bits（状态位）</li>
<li>Configuration space（包含设备特殊信息，比如MAC地址）</li>
<li>Notification system（配置变更，缓存可用，缓存使用）</li>
<li>Zero or more virtqueues</li>
<li>Transport specific interface to the device</li>
</ul>
<h3 id="Networking-with-virtio-qemu-implementation"><a href="#Networking-with-virtio-qemu-implementation" class="headerlink" title="Networking with virtio: qemu implementation"></a>Networking with virtio: qemu implementation</h3><img src="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/2019-09-12-virtio-networking-fig1.png" class="">

<p><em>Figure 1: virtio-net on qemu</em></p>
<p>virtio网络设备是一个虚拟以太网卡，支持TX/RX（发送，接收）多队列。空的缓存被放在N virtqueues里用来接收数据包，往外送的数据包责备放到另外一个N virtqueues里面等待发送。另一个virtqueue被用于数据面之外的驱动和设备的通讯，比如控制高级过滤特性，设置mac地址，或者是一堆活跃的队列。像物理网卡一样，virtio设备支持很多特性比如offloading，能够让真实的host设备来处理。</p>
<p>为了发送数据包，驱动会发送给设备一个缓存包括metadata信息比如在要发送的packet frame上带有的数据包期望的offloading。驱动能够将这个缓存拆分成不同的条目，比如可以把这个metadata的header从packet frame上分离出来。</p>
<p>这些缓存被驱动管理，被映射给设备。因此这个情况下我们可以说这个设备实际上在hypervisor里（结合前面提到hypervisor）因为qemu能够访问所有的guest内存，所以有能力知道缓存的位置并能够对他们进行读写。</p>
<p>下面的流程图表示了virtio-net设备配置和使用virtio-net驱动发送数据包通过PCI和virtio-net设备通讯。在组装好要发送的数据包之后，出发了一个available buffer notification，把控制返回给QEMU然后它就能够把包通过TAP设备送出去</p>
<p>QEMU然后通知guest这些缓存操作执行完成了（读或者写）并且它通过把这些数据放到virtqueue然后发送一个used notification event来触发guest的vCPU中断。</p>
<p>接收数据包的过程和发送类似。唯一不同的是在接收数据包的场合，空缓存会提前被guest分配出来然后提供给设备一个可用缓存保证它能够把即将收到的数据写进去。</p>
<img src="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/2019-09-12-virtio-networking-fig2.png.jpg" class="">

<p><em>Figure 2: Qemu virtio sending buffer flow diagram</em></p>
<h2 id="Vhost-net"><a href="#Vhost-net" class="headerlink" title="Vhost-net"></a>Vhost-net</h2><p>vhost-net是一个内核驱动，实现了vhost协议的处理侧，用来实现一个高效的数据面。比如数据包转发，在这个实现里，qemu和vhost-net内核驱动（handler）使用ioctls来交换vhost消息和一大批叫做irqfd的类似eventfd的文件描述符然后ioeventfs被用来和guest交换通知</p>
<p>当vhost-net驱动被加载的时候，它会在/dev/vhost-net暴露一个字符设备。当qemu启用了vhost-net并启动后，它会打开这个设备并且一些ioctl调用初始化vhost-net实例。这是把vhost-net和hypervisor联系起来必不可少的步骤，准备virtio特性检查，然后给guest提供映射到vhost-net驱动的内存。</p>
<p>在初始化的过程里vhost-net驱动创建了一个内核线程叫做vhost-$pid这个$pid就是hypervisor（也就是qemu进程）的pid。这个线程也被叫做“vhost worker thread”</p>
<p>tag设备仍然被用于VM和host的通讯但是现在这个worker thread处理了这些I/O事件，比如它会不断poll驱动的通知或者是tap的事件并且做数据的转发。</p>
<p>Qemu分配了一个eventfd然后注册了到了vhost和KVM来实现通知的传递。vhost-pid内核线程poll这个eventfd，当guest写某一个特殊地址的时候KVM则会写这个eventfd。这个机制被叫做ioeventfd。用这个方法，对一个特别的guest内存地址简单的读写操作不需要再穿过钢轨的QEMU进程唤醒同时能够被直接路由到vhost worker thread。这也提供了异步的优势，也不再需要vCPU停下来（因此没必要立刻做上下文切换）</p>
<p>另一方面，qemu分配了另外的eventfd并再次注册他们到KVM和vhost来处理vCPU的直接中断注入。这个机制又叫irqfd，他们允许host通过写irqfd来注入vCPU中断到guest。同时也具有异步特性，不需要立刻做上下文切换</p>
<p>注意这些改动对virtio包处理后端对guest来说是完全透明的，guest仍然使用的是标准的virtio接口</p>
<p>下面的图展示了qemu数据路径的offloading到vhost-net内核驱动：</p>
<img src="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/2019-09-12-virtio-networking-fig3.png" class="">

<p><em>Figure 3: vhost-net block diagram</em></p>
<img src="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/2019-09-12-virtio-networking-fig4.png" class="">

<p><em>Figure 4: vhost-net sending buffer diagram flow</em></p>
<h3 id="Communication-with-the-outside-world"><a href="#Communication-with-the-outside-world" class="headerlink" title="Communication with the outside world"></a>Communication with the outside world</h3><p>guest能够和host通过tap设备通讯，然而还有一个遗留的问题就是guest如何和同一个host伤的其他vm或者是其他host上的vm通讯（使用internet通讯）</p>
<p>我们可以内核网络协议栈提供的转发和路由的机制，比如标准的Linux bridges。然而一个更加高级的解决方式就是一个全虚拟化的分发，管理交换机比如Open Virtual Switch</p>
<p>就像在总览篇说的一样，OVS的数据路径在这个场景里是作为内核模块运行的，ovs-switchd是一个用户态的控制管理守护进程然后ovsdb-server是一个转发数据库。</p>
<p>就像图上画的一样，OVS的数据路径在kernel运行然后在物理网卡和虚拟TAP设备间做包的forward：</p>
<img src="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/2019-09-12-virtio-networking-fig5.png" class="">

<p><em>Figure 5: Introduce OVS</em></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章里，我们展示了virtio-net的架构是如何工作的，我们对每一个步骤做了详细的解剖并解释了每个组件的功能。</p>
<p>我们开始解释了默认的qemu IO设备如何通过提供给guest一个开放virtio标准的实现来运作的。我们接下来分析了guest如何和这些设备通过virtio驱动能够发送和接收数据包，发送和接收通知的</p>
<p>然后我们评价了在数据路径中有qemu的情况下需要切换上下文，然后展示了如何在host上使用vhost协议通过vhost-net内核驱动offload这些任务。我们也能够覆盖virtio通知在这个新设计下是如何工作的</p>
<p>最后我们展示了如何将VM连接到外部的非自己所在的host的世界。</p>
<p>在接下来的文章里我们将会提供一个使用之前学习到的解决方案里的不同组件完成关于vhost-net/virtio-net的架构实现。</p>
<p>如果你因为什么原因跳过了那篇文章，我们将在下一篇文章介绍一个新的用户态的使用DPDK的vhost处理协议。我们会列举他的优点，使用DPDK和用户态的观点，我们将建立第二个符合这些概念的架构。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2021/12/22/deep-dive-virtio-networking-and-vhost-net/" data-id="cld1mhea7000byuwb5tqr1j5u" data-title="deep dive virtio networking and vhost net" class="article-share-link">Share</a>
      
      
        <a href="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/architecture/" rel="tag">architecture</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/qemu/" rel="tag">qemu</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vhost-net/" rel="tag">vhost-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-net/" rel="tag">virtio-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-networking/" rel="tag">virtio-networking</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/3/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/5/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/arch-notes/">arch-notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/devops/">devops</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/languages/">languages</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/languages/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/languages/python/">python</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/linux/memory-management/">memory management</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/management/">management</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project-related-works/">project-related-works</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/">virtualization</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/cpu/">cpu</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/edk2-ovmf/">edk2-ovmf</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/kvm/">kvm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/libvirt/">libvirt</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/translation/">translation</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/translation/virtio/">virtio</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/translation/virtio-networking/">virtio-networking</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/v2v/">v2v</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/virtio-balloon/">virtio-balloon</a></li></ul></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/BSOD/" rel="tag">BSOD</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DPDK/" rel="tag">DPDK</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ElementTree/" rel="tag">ElementTree</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TDP/" rel="tag">TDP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TLB/" rel="tag">TLB</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/architecture/" rel="tag">architecture</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/code-reading/" rel="tag">code-reading</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/colo/" rel="tag">colo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cpu/" rel="tag">cpu</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/edk2-ovmf/" rel="tag">edk2-ovmf</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ft/" rel="tag">ft</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/interview/" rel="tag">interview</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kernel/" rel="tag">kernel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kvm/" rel="tag">kvm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/libvirt/" rel="tag">libvirt</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/live-migration/" rel="tag">live-migration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/maven/" rel="tag">maven</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/memory-balloon/" rel="tag">memory balloon</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nessus/" rel="tag">nessus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nexus/" rel="tag">nexus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/others/" rel="tag">others</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/paper-reading/" rel="tag">paper-reading</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/perf/" rel="tag">perf</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/performance/" rel="tag">performance</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/qemu/" rel="tag">qemu</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/reading-notes/" rel="tag">reading notes</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/security/" rel="tag">security</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/software-arch/" rel="tag">software-arch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sysstat/" rel="tag">sysstat</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/system-design/" rel="tag">system-design</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/v2v/" rel="tag">v2v</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vDPA/" rel="tag">vDPA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vhost-net/" rel="tag">vhost-net</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/virt/" rel="tag">virt</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/virt-top/" rel="tag">virt-top</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/virtio/" rel="tag">virtio</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/virtio-balloon/" rel="tag">virtio-balloon</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/virtio-net/" rel="tag">virtio-net</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/virtio-networking/" rel="tag">virtio-networking</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/virtualization/" rel="tag">virtualization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/windows/" rel="tag">windows</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/BSOD/" style="font-size: 10px;">BSOD</a> <a href="/tags/DPDK/" style="font-size: 12.86px;">DPDK</a> <a href="/tags/ElementTree/" style="font-size: 10px;">ElementTree</a> <a href="/tags/TDP/" style="font-size: 11.43px;">TDP</a> <a href="/tags/TLB/" style="font-size: 10px;">TLB</a> <a href="/tags/architecture/" style="font-size: 18.57px;">architecture</a> <a href="/tags/code-reading/" style="font-size: 10px;">code-reading</a> <a href="/tags/colo/" style="font-size: 10px;">colo</a> <a href="/tags/cpu/" style="font-size: 12.86px;">cpu</a> <a href="/tags/edk2-ovmf/" style="font-size: 10px;">edk2-ovmf</a> <a href="/tags/ft/" style="font-size: 11.43px;">ft</a> <a href="/tags/interview/" style="font-size: 10px;">interview</a> <a href="/tags/java/" style="font-size: 12.86px;">java</a> <a href="/tags/kernel/" style="font-size: 14.29px;">kernel</a> <a href="/tags/kvm/" style="font-size: 15.71px;">kvm</a> <a href="/tags/libvirt/" style="font-size: 12.86px;">libvirt</a> <a href="/tags/linux/" style="font-size: 17.14px;">linux</a> <a href="/tags/live-migration/" style="font-size: 10px;">live-migration</a> <a href="/tags/maven/" style="font-size: 10px;">maven</a> <a href="/tags/memory-balloon/" style="font-size: 10px;">memory balloon</a> <a href="/tags/nessus/" style="font-size: 10px;">nessus</a> <a href="/tags/nexus/" style="font-size: 10px;">nexus</a> <a href="/tags/others/" style="font-size: 10px;">others</a> <a href="/tags/paper-reading/" style="font-size: 10px;">paper-reading</a> <a href="/tags/perf/" style="font-size: 10px;">perf</a> <a href="/tags/performance/" style="font-size: 11.43px;">performance</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/qemu/" style="font-size: 20px;">qemu</a> <a href="/tags/reading-notes/" style="font-size: 10px;">reading notes</a> <a href="/tags/security/" style="font-size: 10px;">security</a> <a href="/tags/software-arch/" style="font-size: 12.86px;">software-arch</a> <a href="/tags/sysstat/" style="font-size: 10px;">sysstat</a> <a href="/tags/system-design/" style="font-size: 12.86px;">system-design</a> <a href="/tags/v2v/" style="font-size: 10px;">v2v</a> <a href="/tags/vDPA/" style="font-size: 10px;">vDPA</a> <a href="/tags/vhost-net/" style="font-size: 17.14px;">vhost-net</a> <a href="/tags/virt/" style="font-size: 14.29px;">virt</a> <a href="/tags/virt-top/" style="font-size: 10px;">virt-top</a> <a href="/tags/virtio/" style="font-size: 17.14px;">virtio</a> <a href="/tags/virtio-balloon/" style="font-size: 10px;">virtio-balloon</a> <a href="/tags/virtio-net/" style="font-size: 17.14px;">virtio-net</a> <a href="/tags/virtio-networking/" style="font-size: 17.14px;">virtio-networking</a> <a href="/tags/virtualization/" style="font-size: 10px;">virtualization</a> <a href="/tags/windows/" style="font-size: 10px;">windows</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">May 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">April 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">February 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">November 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/10/">October 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">July 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">June 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/12/06/2023-12-06/">The disk in the guest OS is unmounted during the kernel startup process.</a>
          </li>
        
          <li>
            <a href="/2023/05/26/understanding-cpu-topology-for-improved-performance/">Understanding CPU Topology for Improved Performance</a>
          </li>
        
          <li>
            <a href="/2023/04/27/virtio-memory-balloon/">Understand virtio memory balloon</a>
          </li>
        
          <li>
            <a href="/2023/04/13/qemu-colo-details/">Qemu Colo Details</a>
          </li>
        
          <li>
            <a href="/2023/03/13/KVM%E8%99%9A%E6%8B%9F%E5%8C%96%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/">KVM虚拟化性能分析</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
        <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a></br>
      
      &copy; 2023 Alan Jager<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>

<script>
    var GUEST_INFO = ['nick','mail','link'];
    var guest_info = 'nick,mail,link'.split(',').filter(function(item){
        return GUEST_INFO.indexOf(item) > -1
    });
    var notify = '' == true;
    var verify = 'false' == true;
    new Valine({
        el: '.vcomment',
        notify: notify,
        verify: verify,
        appId: "r30r51B3r5JFqlxR88Jua6So-gzGzoHsz",
        appKey: "wnL9j38siXbLqBHGnWpzmVxv",
        placeholder: "Just go go",
        pageSize:'10',
        avatar:'mm',
        lang:'zh-cn'
    });
</script>

  </div>
</body>
</html>