<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>花の様に</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="花の様に">
<meta property="og:url" content="http://hanayo.cn/page/4/index.html">
<meta property="og:site_name" content="花の様に">
<meta property="og:locale">
<meta property="article:author" content="Alan Jager">
<meta property="article:tag" content="ブログ">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="花の様に" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">花の様に</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://hanayo.cn"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-How-vhost-user-came-into-being-Virtio-networking-and-DPDK" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/" class="article-date">
  <time class="dt-published" datetime="2022-02-09T07:25:37.000Z" itemprop="datePublished">2022-02-09</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/virtualization/">virtualization</a>►<a class="article-category-link" href="/categories/virtualization/translation/">translation</a>►<a class="article-category-link" href="/categories/virtualization/translation/virtio-networking/">virtio-networking</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/">How vhost-user came into being Virtio-networking and DPDK</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在这篇文章里我们将会通过一个宏观的视角介绍一个基于DPDK（Data Plane Development Kit）在host和guest之间的解决方案。这篇文章将会附带一篇面向架构师/研发人员的详细介绍以及一篇提供实际操作帮助的文章。</p>
<p>之前的文章里面包括解决方案，技术介绍以及实践的文章，引导读者了解virtio-networking的生态，包括了基本的组件，kvm，qemu，libvirt，以及vhost protocol和vhost-net/virtio-net架构。这个架构是给予host kernel的vhost-net（后端）和guest kernel的virtio-net（前端）组成的。</p>
<p>vhost-net/virtio-net架构提供了一个这些年来被广泛部署使用的生产解决方案。一部分是因为这个方案对用户开发应用并在虚拟机里运行因为它是用的是标准的Linux sockets来连接到网络的（通过host）。另一方面这个解决方案并不是那么完美，里面还是包含了一些性能开销的，这个问题在后面会被详细解释。</p>
<p>为了讲清楚性能问题，我们将会介绍vhost-user/virtio-pmd架构。为了了解细节，我们会先回顾一下DPDK，如何将OVS连接到DPDK以及virtio是如何适配到这个架构的前端和后端里去的。</p>
<p>在这篇文章的最后，你会对vhost-user/virtio-pmd架构以及这个架构与vhost-net/virtio-net的不同有牢固的认识。</p>
<h2 id="DPDK-overview"><a href="#DPDK-overview" class="headerlink" title="DPDK overview"></a>DPDK overview</h2><p>DPDK目标是提供一个简单和完善的用于数据面应用快速包处理的架构。它实现了一个数据包处理的运行时完成模型，意思是说，所有资源都需要在运行数据面应用之前分配好。这些专门的资源只会被专门的逻辑处理核心处理。</p>
<p>这个设计和Linux kernel通过调度器+中断在进程间上下文切换的机制不同，DPDK架构中设备是被一个定时的polling访问的。这个设计去除了上先问切换以及进程中断带来的开销，保证CPU核心100%都在做包处理。</p>
<p>在实践中，DPDK提供了一系列poll模式的驱动（PMDs）是的包传输能够直接在用户态和物理接口之间进行，完全跳过了kernel网络栈。这个方法提供了一个重要的通过排除中断处理以及kernel网络协议栈提升kernel转发性能的方法。</p>
<p>DPDK是一系列库。因此为了使用它们，你需要一个link到这些库并且调用相关api的应用。</p>
<p>下面的图标展示了之前的virtio构件和一个DPDK应用使用PMD驱动来访问物理网卡（跳过了kernel）：</p>
<img src="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/2019-09-20-virtio-and-dpdk-fig1.jpeg" class="">

<h2 id="OVS-DPDK-overview"><a href="#OVS-DPDK-overview" class="headerlink" title="OVS-DPDK overview"></a>OVS-DPDK overview</h2><p>在之前的文章中介绍过，open vSwtich通常在内核空间的数据路径做包转发，这意味着OVS kernel模块包含了一个简单的记录收到的转发包的flow table。然后一小部分的包我们可以称为异常包（比如第一个打开Openflow flow的包）并不匹配任何内核空间中已经存在的条目，而是发送到用户态的OVS守护进程（ovs-vswitchd）来处理。守护进程会分析这个包然后更新OVS的kernell里的flow table然后后面的发送到这个flow的包就能够直接通过OVS的内核态转发表直接发走了。</p>
<p>这个方法排除了大部分流量的用户态内核态的上下文切换，然而我们仍然被linux的网络协议栈限制，因为它并不适合高频率包的用户场景。</p>
<p>如果我们把OVS和DPDK集成在一起，把前面提到的PMD驱动当作杠杆然后移动OVS内核模块转发表到用户态。</p>
<p>下面的图展示了OVS-DPDK应用，所有的OVS组件都在用户态运行，并通过PMD驱动和物理网卡通信：</p>
<img src="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/2019-09-20-virtio-and-dpdk-fig2.jpeg" class="">

<p>这里要提一下，虽然我们只看到DPDK应用运行在host的用户空间，在guest里运行带PMD驱动的DPDK应用也是可以的。下一节我们将会详细解释这个。</p>
<h2 id="The-vhot-user-virtio-pmd-architecture"><a href="#The-vhot-user-virtio-pmd-architecture" class="headerlink" title="The vhot-user/virtio-pmd architecture"></a>The vhot-user/virtio-pmd architecture</h2><p>在vhost-user/virtio-pmd架构，virtio会在host的用户态guest的用户态使用DPDK：</p>
<ol>
<li>vhost-user（后端） 运行在host用户空间，作为OVS-DPDK的用户态应用。之前提到的DPDK是一个库而vhost-user模块是附带在这些库里面的API。OVS-DPDK是确切的链接到这个库并调用API的应用。任意一个创建在host上的guest VM都会有一个对应的vhost-user被创建出来用来和guest的virtio前端通信。</li>
<li>virtio-pmd（前端）运行在guest用户态，是一个poll模式驱动，消费专门的cores并且执行不会中断的polling。一个运行在用户态的应用消费virtio-pmd也需要连接到DPDK库</li>
</ol>
<p>这个图展示了他们是如何一起运作的：</p>
<img src="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/2019-09-20-virtio-and-dpdk-fig3.jpeg" class="">

<p>如果把这个架构和基于内核的vhost-net/virtio-net架构做对比，vhost-net被vhost-user取代了，而virtio-net则被virtio-pmd取代。</p>
<p>通过启用host用户态通过共享内存跳过kernel直接访问物理网卡然后通过virtio-pmd在guest的用户态也跳过kernel，整体的性能能够提升2-4倍</p>
<p>然而这个方法对用户能力有更多的要求，在vhost-net/virtio-net架构中，数据面通讯是直接通过guest OS视角的：简单的安装virtio驱动到guest kernel然后guest用户态应用自动获得了一个标准的Linux网络接口。</p>
<p>相反vhost-user/virtio-pmd架构，guest的用户态应用为了优化数据面被要求使用virtio-pmd驱动（DPDK库提供）。这不是一个很简单的任务，并且要求专业的DPDK的配置和使用知识。</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>这篇文章我们介绍了vhost-user/virtio-pmd架构，通过提升了一部分使用成本改善了virtio接口的性能因为我们现在需要把应用link并使用DPDK。</p>
<p>这里有一系列用户场景比如虚拟网络功能（VNFs, virtial network functions)性能是一个大缺陷而virtio DPDK的架构能够帮助实现对应的性能指标。然而开发应用是需要专业知识的，并需要对DPDK API的理解以及不同的优化。</p>
<p>下一篇文章我们会提供一个深入的vhost-net/virtio-pmd的内部架构以及不同控制面数据面组件的介绍。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/" data-id="cld1mhea10007yuwbcpc5alsm" data-title="How vhost-user came into being Virtio-networking and DPDK" class="article-share-link">Share</a>
      
      
        <a href="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2022/02/09/How-vhost-user-came-into-being-Virtio-networking-and-DPDK/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DPDK/" rel="tag">DPDK</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/qemu/" rel="tag">qemu</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vhost-net/" rel="tag">vhost-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-net/" rel="tag">virtio-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-networking/" rel="tag">virtio-networking</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-hands-on-vhost-net-do-or-do-not-there-is-no-try" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/12/28/hands-on-vhost-net-do-or-do-not-there-is-no-try/" class="article-date">
  <time class="dt-published" datetime="2021-12-28T13:55:31.000Z" itemprop="datePublished">2021-12-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/virtualization/">virtualization</a>►<a class="article-category-link" href="/categories/virtualization/translation/">translation</a>►<a class="article-category-link" href="/categories/virtualization/translation/virtio-networking/">virtio-networking</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/12/28/hands-on-vhost-net-do-or-do-not-there-is-no-try/">Hands on vhost-net: Do. Or do not. There is no try</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Vhost-net利用标准的virtio网络接口悄悄地成为了基于qemu-kvm的虚拟化环境的默认流量卸载机制。这个机制允许通过内核模块执行网络处理，解放了qemu进程，改进了网络性能。</p>
<p>在之前的文章里介绍了网络架构的组成：<a href="#">Post not found: introduction-virtio-networking-and-vhost-net [Introduction to virtio-networking and vhost-net]</a>  同时提供了一个更加详细的解释：<a href="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/" title="[Deep dive into Virtio-networking and vhost-net]">[Deep dive into Virtio-networking and vhost-net]</a>  。这篇文章里我们将会提供一个详细的步骤实践性的设置一个架构。大建好之后我们能够检查主要的组件是如何工作的。</p>
<p>这篇文章主要面向研发人员，hackers和其他任何有兴趣学习真实的网络卸载是如何做到的。</p>
<p>在读完这篇文章之后（希望能够在你的PC上重建这个环境），你将会对虚拟化使用到的工具更加熟悉（比如virsh）。你将了解如何建立一个vhost-net环境并且了解到如何检查一个运行的云主机同时测试他的网络性能。</p>
<p>对那些需要快速建立环境并直接逆向工程的人，这里有特别的准备！<a target="_blank" rel="noopener" href="https://github.com/redhat-virtio-net/virtio-hands-on">https://github.com/redhat-virtio-net/virtio-hands-on</a> 能够自动化部署环境的ansible脚本。</p>
<h2 id="Setting-things-up"><a href="#Setting-things-up" class="headerlink" title="Setting things up"></a>Setting things up</h2><p>因为懒得准备原文中相同的环境，这里我用ZStack常用环境来做替代</p>
<h3 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h3><p>一个安装了CentOS Linux release 7.6.1810 (Core)的环境<br>使用root用户（或者有sudo权限的用户）<br>home目录下有25G以上的空闲空间<br>至少8GB的RAM<br>首先安装一堆依赖，建议通过ZStack iso安装可以选择Host模式，如果是专家模式需要通过，如下命令安装</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum --disablerepo&#x3D;* --enablerepo&#x3D;zstack-mn,qemu-kvm-ev-mn install libguestfs-tools qemu-kvm libvirt kernel-tools iperf3 -y</span><br></pre></td></tr></table></figure>

<p>另外根据OS版本下载一个rpm包 <a target="_blank" rel="noopener" href="https://pkgs.org/download/netperf">https://pkgs.org/download/netperf</a></p>
<p>对应的Centos 7的包是 <a target="_blank" rel="noopener" href="https://centos.pkgs.org/7/lux/netperf-2.7.0-1.el7.lux.x86_64.rpm.html">https://centos.pkgs.org/7/lux/netperf-2.7.0-1.el7.lux.x86_64.rpm.html</a></p>
<p>安装virt-install</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum --disablerepo&#x3D;* --enablerepo&#x3D;ali* install virt-install -y</span><br></pre></td></tr></table></figure>

<p>接下来确保当前用户被加入了libvirt的用户组，做一下修改</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -a -G libvirt $(whoami)</span><br></pre></td></tr></table></figure>

<p>然后重新登录，并重启libvirt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart libvirtd</span><br></pre></td></tr></table></figure>

<h3 id="Creating-VM"><a href="#Creating-VM" class="headerlink" title="Creating VM"></a>Creating VM</h3><p>首先下载一个镜像，可以在内部 <a target="_blank" rel="noopener" href="http://192.168.200.100/mirror/diskimages/">http://192.168.200.100/mirror/diskimages/</a> 找一个，这里直接用的一个c76的镜像</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;archive.fedoraproject.org&#x2F;pub&#x2F;archive&#x2F;fedora&#x2F;linux&#x2F;releases&#x2F;30&#x2F;Cloud&#x2F;x86_64&#x2F;images&#x2F;Fedora-Cloud-Base-30-1.2.x86_64.qcow2</span><br></pre></td></tr></table></figure>

<p>这是一个封装好的镜像，我们把他作为一个copy，保证以后可以继续使用，执行如下命令，创建并查一下一下image的信息是否和预期一致</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# qemu-img create -f qcow2 -b Fedora-Cloud-Base-30-1.2.x86_64.qcow2 virtio-test1.qcow2 20G</span><br><span class="line">Formatting &#39;virtio-test1.qcow2&#39;, fmt&#x3D;qcow2 size&#x3D;21474836480 backing_file&#x3D;centos76.qcow2 cluster_size&#x3D;65536 lazy_refcounts&#x3D;off refcount_bits&#x3D;16</span><br><span class="line">[root@host ~]# qemu-img info virtio-test1.qcow2</span><br><span class="line">image: virtio-test1.qcow2</span><br><span class="line">file format: qcow2</span><br><span class="line">virtual size: 20 GiB (21474836480 bytes)</span><br><span class="line">disk size: 196 KiB</span><br><span class="line">cluster_size: 65536</span><br><span class="line">backing file: centos76.qcow2</span><br><span class="line">Format specific information:</span><br><span class="line">    compat: 1.1</span><br><span class="line">    lazy refcounts: false</span><br><span class="line">    refcount bits: 16</span><br><span class="line">    corrupt: false</span><br></pre></td></tr></table></figure>

<p>然后清理一下这个操作系统：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo virt-sysprep --root-password password:password123 --uninstall cloud-init --selinux-relabel -a virtio-test1.qcow2</span><br></pre></td></tr></table></figure>

<p>这个命令会挂载文件系统，并自动做一些基础设置，让这个镜像准备好启动</p>
<p>我们需要把网络连接到虚拟机网络。Libvirt对网络的配置就和管理虚拟机一样，你可以通过xml文件定义一个网络，通过命令行控制他的启动和停止。</p>
<p>举个例子，我们使用一个libvirt提供的叫做‘default’的网络的便利设置用如下的命令定义<code>default</code>网络启动并检测他已经运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# virsh net-define &#x2F;usr&#x2F;share&#x2F;libvirt&#x2F;networks&#x2F;default.xml</span><br><span class="line">Network default defined from &#x2F;usr&#x2F;share&#x2F;libvirt&#x2F;networks&#x2F;default.xml</span><br><span class="line">[root@host ~]# virsh net-start default</span><br><span class="line">Network default started</span><br><span class="line">[root@host ~]# virsh net-list</span><br><span class="line"> Name      State    Autostart   Persistent</span><br><span class="line">--------------------------------------------</span><br><span class="line"> default   active   no          yes</span><br></pre></td></tr></table></figure>

<p>最后我们能够使用virt-install创建虚拟机。这是一个命令行工具创建一堆操作系统需要的定义，并给出一个基础的我们可以自定义的配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virt-install --import --name virtio-test1 --ram&#x3D;4096 --vcpus&#x3D;2 --nographics --accelerate --network network:default,model&#x3D;virtio --mac 02:ca:fe:fa:ce:01       --debug --wait 0 --console pty --disk &#x2F;root&#x2F;virtio-test1.qcow2,bus&#x3D;virtio</span><br></pre></td></tr></table></figure>

<p>这些命令用的选项特定了vCPUs的数量，还有我们虚拟机的RAM大小并且指定磁盘的路径和云主机要连接的网络。</p>
<p>除开虚拟机通过我们这些选项定义之外，virt-install命令也能够启动虚拟机，所以我们需要列出来：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# virsh list</span><br><span class="line"> Id   Name           State</span><br><span class="line">------------------------------</span><br><span class="line"> 9    virtio-test1   running</span><br></pre></td></tr></table></figure>

<p>我们的虚拟机在运行了</p>
<p>提醒一下，virsh是一个libvirt的命令行接口，你可以这样启动一个虚拟机：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh start virtio-test1</span><br></pre></td></tr></table></figure>

<p>进入虚拟机的console：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh console virtio-test1</span><br></pre></td></tr></table></figure>

<p>停止一个运行的虚拟机：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh shutdown virtio-test1</span><br></pre></td></tr></table></figure>

<p>删除运行的虚拟机（不要做这个除非你想在创建一遍）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">virsh undefine virtio-test1</span><br><span class="line">Inspecting the guest</span><br></pre></td></tr></table></figure>

<p>就像已经提到的，virt-install命令能够自动使用libvirt创建和启动云主机。每个虚拟机创建都是通过xml文件描述需要模拟的硬件设置并提交给libvirt。我们通过dump配置的内容可以看一下相关的文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;devices&gt;</span><br><span class="line">...</span><br><span class="line">    &lt;interface type&#x3D;&#39;network&#39;&gt;</span><br><span class="line">      &lt;mac address&#x3D;&#39;02:ca:fe:fa:ce:01&#39;&#x2F;&gt;</span><br><span class="line">      &lt;source network&#x3D;&#39;default&#39; bridge&#x3D;&#39;virbr0&#39;&#x2F;&gt;</span><br><span class="line">      &lt;target dev&#x3D;&#39;vnet0&#39;&#x2F;&gt;</span><br><span class="line">      &lt;model type&#x3D;&#39;virtio&#39;&#x2F;&gt;</span><br><span class="line">      &lt;alias name&#x3D;&#39;net0&#39;&#x2F;&gt;</span><br><span class="line">      &lt;address type&#x3D;&#39;pci&#39; domain&#x3D;&#39;0x0000&#39; bus&#x3D;&#39;0x00&#39; slot&#x3D;&#39;0x02&#39; function&#x3D;&#39;0x0&#39;&#x2F;&gt;</span><br><span class="line">    &lt;&#x2F;interface&gt;</span><br><span class="line">...</span><br><span class="line">&lt;&#x2F;devices&gt;</span><br></pre></td></tr></table></figure>

<p>我们能够看到一个virtio设备被创建好了，并连接到网络，这堆配置里有virbr0。这个设备有PCI，bus和slot</p>
<p>然后通过console命令进入console</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh console virtio-test1</span><br></pre></td></tr></table></figure>

<p>进入guset安装一些测试依赖：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dnf install pciutils iperf3</span><br></pre></td></tr></table></figure>

<p>然后在虚拟机里看一下，实际上虚拟PCI总线上挂了个网络设备</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# lspci -s 0000:00:02.0 -v</span><br><span class="line">00:02.0 Ethernet controller: Red Hat, Inc. Virtio network device</span><br><span class="line">	Subsystem: Red Hat, Inc. Device 0001</span><br><span class="line">	Physical Slot: 2</span><br><span class="line">	Flags: bus master, fast devsel, latency 0, IRQ 11</span><br><span class="line">	I&#x2F;O ports at c040 [size&#x3D;32]</span><br><span class="line">	Memory at febc0000 (32-bit, non-prefetchable) [size&#x3D;4K]</span><br><span class="line">	Memory at febf4000 (64-bit, prefetchable) [size&#x3D;16K]</span><br><span class="line">	Expansion ROM at feb80000 [disabled] [size&#x3D;256K]</span><br><span class="line">	Capabilities: [98] MSI-X: Enable+ Count&#x3D;3 Masked-</span><br><span class="line">	Capabilities: [84] Vendor Specific Information: VirtIO: &lt;unknown&gt;</span><br><span class="line">	Capabilities: [70] Vendor Specific Information: VirtIO: Notify</span><br><span class="line">	Capabilities: [60] Vendor Specific Information: VirtIO: DeviceCfg</span><br><span class="line">	Capabilities: [50] Vendor Specific Information: VirtIO: ISR</span><br><span class="line">	Capabilities: [40] Vendor Specific Information: VirtIO: CommonCfg</span><br><span class="line">	Kernel driver in use: virtio-pci</span><br></pre></td></tr></table></figure>

<p>注意：lspci后面跟的地址是根据xml里面的source，bus，slot，function拼接起来的。</p>
<p>除了典型的PCI设备信息之外（比如内存阈和功能之外，驱动还实现了基于PCI的通用virtio功能吗，并创建了一个由virtio_net驱动的网络设备，比如我们可以深入的看一下这个设备</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# readlink &#x2F;sys&#x2F;devices&#x2F;pci0000\:00&#x2F;0000\:00\:02.0&#x2F;virtio0&#x2F;driver</span><br><span class="line">..&#x2F;..&#x2F;..&#x2F;..&#x2F;bus&#x2F;virtio&#x2F;drivers&#x2F;virtio_net</span><br></pre></td></tr></table></figure>

<p>通过命令行readlink，可以看到这个pci设备使用的是virtio_net驱动。</p>
<p>是这个virtio_net驱动控制了操作系统使用的网络接口的创建：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ip link</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000</span><br><span class="line">    link&#x2F;loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link&#x2F;ether 02:ca:fe:fa:ce:01 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">Inspecting the host</span><br></pre></td></tr></table></figure>
<p>我们已经看过guset了，让我们再看看host。注意我们通过‘network’类型配置网络接口的默认行为是使用vhost-net</p>
<p>首先我们看一下vhost-net是否加载</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# lsmod | grep vhost</span><br><span class="line">vhost_net              22507  1</span><br><span class="line">tun                    31881  4 vhost_net</span><br><span class="line">vhost                  48422  1 vhost_net</span><br><span class="line">macvtap                22796  1 vhost_net</span><br></pre></td></tr></table></figure>

<p>我们能够检查到QEMU使用了tun，kvm和vhost-net设备，同时通过检查/proc文件系统也能发现这些文件描述符被分给qemu处理了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# ls -lh &#x2F;proc&#x2F;40888&#x2F;fd | grep &#39;&#x2F;dev&#39;</span><br><span class="line">lrwx------ 1 root root 64 Dec 27 22:55 0 -&gt; &#x2F;dev&#x2F;null</span><br><span class="line">lrwx------ 1 root root 64 Dec 27 22:55 10 -&gt; &#x2F;dev&#x2F;ptmx</span><br><span class="line">lrwx------ 1 root root 64 Dec 27 22:55 13 -&gt; &#x2F;dev&#x2F;kvm</span><br><span class="line">lr-x------ 1 root root 64 Dec 27 22:55 3 -&gt; &#x2F;dev&#x2F;urandom</span><br><span class="line">lrwx------ 1 root root 64 Dec 27 22:55 35 -&gt; &#x2F;dev&#x2F;net&#x2F;tun</span><br><span class="line">lrwx------ 1 root root 64 Dec 27 22:55 37 -&gt; &#x2F;dev&#x2F;vhost-net</span><br></pre></td></tr></table></figure>

<p>这意味着qemu进程，不仅打开了kvm设备执行虚拟化操作，同时也创建了一个tun/tap设备和一打开了一个vhost-net设备。当然我们也能够看到一个辅助qemu的vhost内核线程也被创建出来了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# ps -ef | grep &#39;\[vhost&#39;</span><br><span class="line">root     40056 21741  0 09:53 pts&#x2F;0    00:00:00 grep --color&#x3D;auto \[vhost</span><br><span class="line">root     40894     2  0 Dec27 ?        00:00:03 [vhost-40888]</span><br><span class="line">[root@host ~]# pgrep qemu</span><br><span class="line">40888</span><br></pre></td></tr></table></figure>

<p>这个vhost内核线程的名字就是vhost-$qemu_pid</p>
<p>最后我们可以看到qemu进程创建的tun接口（上面通过/proc找到的）通过bridge把host和guest连在一起了。注意，虽然tap设备被挂在了qemu进程上，实际上进行tap设备读写的是vhost内核线程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# ip -d tuntap</span><br><span class="line">virbr0-nic: tap UNKNOWN_FLAGS:800</span><br><span class="line">	Attached to processes:</span><br><span class="line">vnet0: tap vnet_hdr</span><br><span class="line">	Attached to processes: qemu-kvm(40888)</span><br></pre></td></tr></table></figure>

<p>OK，所以vhost已经启动并且运行了，qemu也连接到了vhost上。现在我们可以制造一些流量来看看系统的表现。</p>
<h2 id="Generating-traffic"><a href="#Generating-traffic" class="headerlink" title="Generating traffic"></a>Generating traffic</h2><p>如果你已经完成之前的步骤的话，你已经可以尝试通过ip地址从host发送数据到guest或者反过来。举个例子，通过iperf3测试网络性能，注意这些测试方法不是正确的benchmarks，不同的输入比如软硬件版本，不同的网络协议栈参数，会显著影响测试结果。性能吞吐量，或者是特定的使用量基准不在本文的范围之内。</p>
<p>首先检查guest的ip地址，执行 iperf3 server (或者任意你打算用来测试连通性的工具）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# ip addr</span><br><span class="line">...</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</span><br><span class="line">    link&#x2F;ether 02:ca:fe:fa:ce:01 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.122.41&#x2F;24 brd 192.168.122.255 scope global dynamic noprefixroute eth0</span><br><span class="line">       valid_lft 2808sec preferred_lft 2808sec</span><br><span class="line">    inet6 fe80::ca:feff:fefa:ce01&#x2F;64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<p>然后再host上运行iperf3的client：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# iperf3 -c 192.168.122.41</span><br><span class="line">Connecting to host 192.168.122.41, port 5201</span><br><span class="line">[ ID] Interval           Transfer     Bandwidth       Retr</span><br><span class="line">[  4]   0.00-10.00  sec  34.3 GBytes  29.5 Gbits&#x2F;sec    1             sender</span><br><span class="line">[  4]   0.00-10.00  sec  34.3 GBytes  29.5 Gbits&#x2F;sec                  receiver</span><br></pre></td></tr></table></figure>

<p>在iperf3的输出里我们能看到一个29.5 Gbits/sec 的传输速率（主机这个网络的带宽收到很多以来的影响，不要假定会和你的环境一致）。我们可以通过 -l 参数修改包的大小来测试更多数据平面。</p>
<p>如果我们在iperf3测试过程中运行top命令我们能够看到vhost-$pid内核线程使用了100%的core来做包转发，同时QEMU使用了几乎两倍的cores（可以多试几次，刚好中间观察到一个200% 一个100%，注意创建guest的时候指定的qemu的vcpu数量就是2）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">top - 10:07:24 up 7 days, 17:30,  3 users,  load average: 1.49, 0.55, 0.28</span><br><span class="line">Tasks: 612 total,   2 running, 610 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s):  2.9 us,  6.6 sy,  0.0 ni, 90.4 id,  0.0 wa,  0.0 hi,  0.2 si,  0.0 st</span><br><span class="line">KiB Mem : 13174331+total, 10001235+free,  4672644 used, 27058324 buff&#x2F;cache</span><br><span class="line">KiB Swap:  4194300 total,  4194300 free,        0 used. 12307848+avail Mem</span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</span><br><span class="line">40888 root      20   0 6158388   1.2g  11436 S 200.0  0.9   3:38.41 qemu-kvm</span><br><span class="line">40894 root      20   0       0      0      0 R 100.0  0.0   0:58.46 vhost-40888</span><br></pre></td></tr></table></figure>

<p>要测试延迟，我们使用netperf命令启动一个netperf服务，然后测试延迟。（注：需要在guest里先启动一个netserver，如何在guest安装netperf 2021.12.18 fedora安装netperf）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# netperf -l 30 -H 192.168.122.41 -p 16604 -t TCP_RR</span><br><span class="line">MIGRATED TCP REQUEST&#x2F;RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 192.168.122.41 () port 0 AF_INET : first burst 0</span><br><span class="line">Local &#x2F;Remote</span><br><span class="line">Socket Size   Request  Resp.   Elapsed  Trans.</span><br><span class="line">Send   Recv   Size     Size    Time     Rate</span><br><span class="line">bytes  Bytes  bytes    bytes   secs.    per sec</span><br><span class="line"></span><br><span class="line">16384  87380  1        1       30.00    36481.26</span><br><span class="line">16384  131072</span><br></pre></td></tr></table></figure>

<p>计算打开vhost-net host → guest TCP_RR延迟为 1 / 36481.26 = 0.0000274s</p>
<p>就像之前说的，我们进行的不是benchmark或者是吞吐测试。我们只是熟悉一下这个方法。</p>
<h2 id="Extra-Disable-vhost-net"><a href="#Extra-Disable-vhost-net" class="headerlink" title="Extra: Disable vhost-net"></a>Extra: Disable vhost-net</h2><p>就像我们看到的vhost-net是被作为默认行为的，因为带来了性能的提升。然而因为我们是来实践学习的，金庸vhost-net来看一下性能有什么不同，通过这个我们将知道qemu做了多“重”的包处理的操作，以及对性能造成了什么影响。</p>
<p>首先停止vm：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh shutdown virtio-test1</span><br></pre></td></tr></table></figure>

<p>编辑云主机配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh edit virtio-test1</span><br></pre></td></tr></table></figure>

<p>修改网卡配置为，增加了<driver name='qemu'/></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;devices&gt;</span><br><span class="line">...</span><br><span class="line">    &lt;interface type&#x3D;&#39;network&#39;&gt;</span><br><span class="line">      &lt;mac address&#x3D;&#39;02:ca:fe:fa:ce:01&#39;&#x2F;&gt;</span><br><span class="line">      &lt;source network&#x3D;&#39;default&#39;&#x2F;&gt;</span><br><span class="line">      &lt;model type&#x3D;&#39;virtio&#39;&#x2F;&gt;</span><br><span class="line">      &lt;driver name&#x3D;&#39;qemu&#39;&#x2F;&gt;</span><br><span class="line">      &lt;address type&#x3D;&#39;pci&#39; domain&#x3D;&#39;0x0000&#39; bus&#x3D;&#39;0x00&#39; slot&#x3D;&#39;0x02&#39; function&#x3D;&#39;0x0&#39;&#x2F;&gt;</span><br><span class="line">    &lt;&#x2F;interface&gt;</span><br><span class="line">...</span><br><span class="line">&lt;&#x2F;devices&gt;</span><br></pre></td></tr></table></figure>

<p>修改成功后llibvirt会显示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Domain virtio-test1 XML configuration not changed</span><br></pre></td></tr></table></figure>

<p>然后启动云主机：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh start virtio-test1</span><br></pre></td></tr></table></figure>

<p>我们可以检查一下指向/dev/vhost-net的文件描述符没了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# ls -lh &#x2F;proc&#x2F;37518&#x2F;fd | grep &#39;&#x2F;dev&#39;</span><br><span class="line">lrwx------ 1 root root 64 Dec 28 11:22 0 -&gt; &#x2F;dev&#x2F;null</span><br><span class="line">lrwx------ 1 root root 64 Dec 28 11:22 10 -&gt; &#x2F;dev&#x2F;ptmx</span><br><span class="line">lrwx------ 1 root root 64 Dec 28 11:22 13 -&gt; &#x2F;dev&#x2F;kvm</span><br><span class="line">lr-x------ 1 root root 64 Dec 28 11:22 3 -&gt; &#x2F;dev&#x2F;urandom</span><br><span class="line">lrwx------ 1 root root 64 Dec 28 11:22 33 -&gt; &#x2F;dev&#x2F;net&#x2F;tun</span><br><span class="line">Analyzing the performance impact</span><br></pre></td></tr></table></figure>

<p>如果我们在没有vhost-net的环境重复之前的测试，我们能看到vhost-net的线程没有在运行了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# ps -ef | grep &#39;\[vhost&#39;</span><br><span class="line">root      9076 23993  0 11:24 pts&#x2F;0    00:00:00 grep --color&#x3D;auto \[vhost</span><br></pre></td></tr></table></figure>

<p>我们获得的性能iperf3测试结果为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# iperf3 -c 192.168.122.41</span><br><span class="line">Connecting to host 192.168.122.41, port 5201</span><br><span class="line">[  4] local 192.168.122.1 port 58628 connected to 192.168.122.41 port 5201</span><br><span class="line">[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd</span><br><span class="line">[  4]   0.00-1.00   sec  1.89 GBytes  16.2 Gbits&#x2F;sec    0   2.08 MBytes</span><br><span class="line">[  4]   1.00-2.00   sec  1.78 GBytes  15.3 Gbits&#x2F;sec    0   2.19 MBytes</span><br><span class="line">[  4]   2.00-3.00   sec  1.82 GBytes  15.6 Gbits&#x2F;sec    0   2.37 MBytes</span><br><span class="line">[  4]   3.00-4.00   sec  1.82 GBytes  15.7 Gbits&#x2F;sec    0   2.47 MBytes</span><br><span class="line">[  4]   4.00-5.00   sec  1.73 GBytes  14.8 Gbits&#x2F;sec    0   2.61 MBytes</span><br><span class="line">[  4]   5.00-6.00   sec  1.80 GBytes  15.4 Gbits&#x2F;sec    0   2.64 MBytes</span><br><span class="line">[  4]   6.00-7.00   sec  1.82 GBytes  15.6 Gbits&#x2F;sec    0   2.64 MBytes</span><br><span class="line">[  4]   7.00-8.00   sec  1.81 GBytes  15.6 Gbits&#x2F;sec    0   2.69 MBytes</span><br><span class="line">[  4]   8.00-9.00   sec  2.33 GBytes  20.0 Gbits&#x2F;sec    0   2.81 MBytes</span><br><span class="line">[  4]   9.00-10.00  sec  2.32 GBytes  19.9 Gbits&#x2F;sec    0   2.93 MBytes</span><br><span class="line">- - - - - - - - - - - - - - - - - - - - - - - - -</span><br><span class="line">[ ID] Interval           Transfer     Bandwidth       Retr</span><br><span class="line">[  4]   0.00-10.00  sec  19.1 GBytes  16.4 Gbits&#x2F;sec    0             sender</span><br><span class="line">[  4]   0.00-10.00  sec  19.1 GBytes  16.4 Gbits&#x2F;sec                  receiver</span><br><span class="line"></span><br><span class="line">iperf Done.</span><br></pre></td></tr></table></figure>

<p>可以看到传输速度从上面的29.5 Gbits/sec掉到了16.4 Gbits/sec</p>
<p>在通过top命令检查，我们可以发现qemu对CPU的使用，峰值会变得很高（这个需要多测试一下，是一个浮动的范围，关掉vhost-net之后大概是150% ～ 260%左右，之前开vhost-net的时候最高也就200%）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">top - 11:27:10 up 7 days, 18:50,  3 users,  load average: 0.57, 0.36, 0.29</span><br><span class="line">Tasks: 617 total,   3 running, 614 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s):  3.8 us,  4.0 sy,  0.0 ni, 91.7 id,  0.5 wa,  0.0 hi,  0.1 si,  0.0 st</span><br><span class="line">KiB Mem : 13174331+total, 10047221+free,  3931432 used, 27339668 buff&#x2F;cache</span><br><span class="line">KiB Swap:  4194300 total,  4194300 free,        0 used. 12381579+avail Mem</span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</span><br><span class="line">37518 root      20   0 6605056 514392  11372 R 242.9  0.4   1:02.22 qemu-kvm</span><br></pre></td></tr></table></figure>

<p>如果我们再比较一下两种不同网络架构下的TCP和UDP的延迟，我们可以看到vhost-net对两种形式的性能都有一致的提升</p>
<p>记录一下关闭vhoset-net之后的测试结果</p>
<p>关闭vhost-net的host → guest TCP_RR测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# netperf -l 30 -H 192.168.122.41 -p 16604 -t TCP_RR</span><br><span class="line">MIGRATED TCP REQUEST&#x2F;RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 192.168.122.41 () port 0 AF_INET : first burst 0</span><br><span class="line">Local &#x2F;Remote</span><br><span class="line">Socket Size   Request  Resp.   Elapsed  Trans.</span><br><span class="line">Send   Recv   Size     Size    Time     Rate</span><br><span class="line">bytes  Bytes  bytes    bytes   secs.    per sec</span><br><span class="line"></span><br><span class="line">16384  87380  1        1       30.00    27209.58</span><br><span class="line">计算延迟为 1&#x2F;27209.58 &#x3D; 0.0000367s</span><br></pre></td></tr></table></figure>

<p>关闭vhost-net的host → guest UDP_RR测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# netperf -l 30 -H 192.168.122.41 -p 16604 -t UDP_RR</span><br><span class="line">MIGRATED UDP REQUEST&#x2F;RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 192.168.122.41 () port 0 AF_INET : first burst 0</span><br><span class="line">Local &#x2F;Remote</span><br><span class="line">Socket Size   Request  Resp.   Elapsed  Trans.</span><br><span class="line">Send   Recv   Size     Size    Time     Rate</span><br><span class="line">bytes  Bytes  bytes    bytes   secs.    per sec</span><br><span class="line"></span><br><span class="line">212992 212992 1        1       30.00    27516.04</span><br></pre></td></tr></table></figure>

<p>计算延迟为 1/27516.04=0.0000363s</p>
<p>打开vhost-net的host → guest UDP_RR测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# netperf -l 30 -H 192.168.122.41 -p 16604 -t UDP_RR</span><br><span class="line">MIGRATED UDP REQUEST&#x2F;RESPONSE TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 192.168.122.41 () port 0 AF_INET : first burst 0</span><br><span class="line">Local &#x2F;Remote</span><br><span class="line">Socket Size   Request  Resp.   Elapsed  Trans.</span><br><span class="line">Send   Recv   Size     Size    Time     Rate</span><br><span class="line">bytes  Bytes  bytes    bytes   secs.    per sec</span><br><span class="line"></span><br><span class="line">212992 212992 1        1       30.00    37681.20</span><br></pre></td></tr></table></figure>

<p>计算延迟为 1/37681.20 = 0.0000265s</p>
<p>同样的测试一下guest→host方向的流量延迟，这里不列代码只记录测试结果</p>
<p>打开vhost-net guest→host TCP_RR 1/36030.14 = 0.0000278s</p>
<p>打开vhost-net guest→host UDP_RR 1/37690.97 = 0.0000265s</p>
<p>关闭vhost-net guest→host TCP_RR 1/26697.53 = 0.0000375s</p>
<p>关闭vhost-net guest→host UDP_RR 1/25850.89 = 0.0000387s</p>
<p>结果如下表</p>
<img src="/2021/12/28/hands-on-vhost-net-do-or-do-not-there-is-no-try/lantency.png" class="">

<p>使用strace统计系统调用，关闭vhost-net，测试iperf3的情况</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# strace -c -p 37518 # 进程的pid，统计结束后用ctrl+c结束</span><br><span class="line">strace: Process 37518 attached</span><br><span class="line">^Cstrace: Process 37518 detached</span><br><span class="line">% time     seconds  usecs&#x2F;call     calls    errors syscall</span><br><span class="line">------ ----------- ----------- --------- --------- ----------------</span><br><span class="line"> 41.63    1.137491           8    141188      2775 read</span><br><span class="line"> 28.66    0.783060           6    135331           ioctl</span><br><span class="line"> 27.16    0.742136           6    121757           writev</span><br><span class="line">  2.40    0.065491           8      8380           ppoll</span><br><span class="line">  0.13    0.003653           6       594       275 futex</span><br><span class="line">  0.01    0.000243           5        50           write</span><br><span class="line">  0.00    0.000020          20         1           clone</span><br><span class="line">  0.00    0.000012           3         4           sendmsg</span><br><span class="line">  0.00    0.000008           4         2           rt_sigprocmask</span><br><span class="line">------ ----------- ----------- --------- --------- ----------------</span><br><span class="line">100.00    2.732114                407307      3050 total</span><br></pre></td></tr></table></figure>

<p>打开vhost-net之后的iperf3测试时strace qemu的结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@host ~]# strace -c -p 27346</span><br><span class="line">strace: Process 27346 attached</span><br><span class="line">^Cstrace: Process 27346 detached</span><br><span class="line">% time     seconds  usecs&#x2F;call     calls    errors syscall</span><br><span class="line">------ ----------- ----------- --------- --------- ----------------</span><br><span class="line"> 99.96    6.819794      131150        52           ppoll</span><br><span class="line">  0.02    0.001416          10       136           write</span><br><span class="line">  0.01    0.000862          13        66           futex</span><br><span class="line">  0.00    0.000341           9        39           read</span><br><span class="line">  0.00    0.000083          10         8           sendmsg</span><br><span class="line">  0.00    0.000022          22         1           clone</span><br><span class="line">  0.00    0.000016           8         2           rt_sigprocmask</span><br><span class="line">------ ----------- ----------- --------- --------- ----------------</span><br><span class="line">100.00    6.822534                   304           total</span><br></pre></td></tr></table></figure>

<p>另外一个好方法就是看qmue发送了多少IOCTLs到KVM。因为每次I/O事件都需要qemu处理，qemu需要发送IOCTL给KVM来切换回VMX noroot的guest模式。我们可以通过strace分析qemu在每个syscall上花费的时间。根据上面两次strace的结果可以看到没打开vhost-net的情况，存在很多ioctl的调用，而打开vhost之后基本上只有ppoll。</p>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>在这篇文章里面我们完整了提供了一个创建一个QEMU + vhost-net的虚拟机，检查guest和host来理解这个架构的输入输出。我们也展示了性能是如何变化的。这个系列也到此为止。从 Introduction to virtio-networking and vhost-net 的总览到技术视角深入理解的 Deep dive into Virtio-networking and vhost-net 详细的解释了这些组件，现在展示完了如果配置，希望这些内容呢能够提供足够的资源给IT专家，架构师以及研发人员理解这个技术并开始和他一起工作。</p>
<p>下一个话题将会涉及 Userland networking and DPDK</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2021/12/28/hands-on-vhost-net-do-or-do-not-there-is-no-try/" data-id="cld1mheca005zyuwb5nnk6dee" data-title="Hands on vhost-net: Do. Or do not. There is no try" class="article-share-link">Share</a>
      
      
        <a href="/2021/12/28/hands-on-vhost-net-do-or-do-not-there-is-no-try/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2021/12/28/hands-on-vhost-net-do-or-do-not-there-is-no-try/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/architecture/" rel="tag">architecture</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/qemu/" rel="tag">qemu</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vhost-net/" rel="tag">vhost-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-net/" rel="tag">virtio-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-networking/" rel="tag">virtio-networking</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-deep-dive-virtio-networking-and-vhost-net" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/" class="article-date">
  <time class="dt-published" datetime="2021-12-22T11:17:19.000Z" itemprop="datePublished">2021-12-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/virtualization/">virtualization</a>►<a class="article-category-link" href="/categories/virtualization/translation/">translation</a>►<a class="article-category-link" href="/categories/virtualization/translation/virtio-networking/">virtio-networking</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/">deep dive virtio networking and vhost net</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在这篇文章里我们将会解释在 <a href="#">Post not found: introduction-virtio-networking-and-vhost-net [introduction]</a> 里描述的vhost-net架构，并通过技术视角来弄清楚所有东西是怎么协同工作的。这系列博客里的这部分内容是为了让你们更好的理解virtio-networking领域是如何将虚拟化和网络连接在一起的</p>
<p>本文主要主要是面向对有兴趣理解上一篇blog提到的vhost-net/virtio-net底层原理的架构师和研发人员</p>
<p>我们将从描述不同的virtio规范的标准组件和共享内存在hypervisor里如何组织的，QEMU如何模拟一个virtio网络设备以及一个guest使用根据virtio规范来实现一个虚拟化驱动来管理并和设备通讯的</p>
<p>在展示过QEMU virtio架构，我们将会分析I/O瓶颈和限制，同时我们将会用host的kernel来解决这个问题，同时最后给出一个宏观的vhost-net的架构</p>
<p>最后，我们将会展示如何通过在它所运行的host上使用OVS（一个开源的虚拟化，SDN，分布式交换机）连接虚拟机到外部网络</p>
<p>希望在读完这篇文章后，你能够理解vhost-net/virtio-net架构是如何工作的，同时能够理解这架构中每一个组件的目标和作用以及数据包是如何被发送和接收的</p>
<h2 id="Previous-Concepts"><a href="#Previous-Concepts" class="headerlink" title="Previous Concepts"></a>Previous Concepts</h2><p>在这个部分我们将会简短的介绍一些帮助你完全理解这篇文章需要知道的概念。对于精通这个问题的人是比较基础的内容，但主要是为了提供一个共同的基础</p>
<h3 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h3><p>让我们从最基础的开始。一个物理网卡（Nic，Network Interface Card）是一个真实的硬件组件，允许物理机连接到外部网络。网卡可以承担一些offload，比如代替CPU执行checksum计算，Segmentation Offload（把一个很大片的数据转换成很多切片，切片大小就是以太网MTU的大小）或者是Large Receive Offload（从CPU角度看，就是把很多数据包合成一个数据包）</p>
<p>另外我们还有tun/tap设备，虚拟化的用户态用来交换数据包的点对点网络设备。交换二层（ethernet frames）数据的叫做tap设备，如果交换 (IP packets)三层数据的就是tun设备。</p>
<p>当tun的kernel模块被加载之后会创建一个特殊的/dev/net/tun设备。进程可以创建tap设备，并发打开这个设备发送一些特殊的ioctl命令给这个设备。新创建的tap设备在。dev文件系统里存在一个名字，并且其他的进程可以打开，并发送接收Ethernet frames</p>
<h3 id="IPC，System-programming"><a href="#IPC，System-programming" class="headerlink" title="IPC，System programming"></a>IPC，System programming</h3><p>Unix sockets是一个在同一台物理机器上做进程间通信的有效方法。在这篇文章涉及的范围内，通讯的服务端监听了文件系统上一个路径下的Unix socket，因此一个客户端（client）可以连接到这个路径使用它。这样，进程间就可以交换消息了。注意，unix sockets也可以用来在进程间交换文件描述符。</p>
<p>eventfd是一个轻量级IPC的实现。虽然Unix sockets允许发送和接收各种消息，eventfd只是一个生产者可以修改，消费者可以读取的整型值。这个使得eventfd更适合用于等待通知机制，而不是传输信息的场景。</p>
<p>这两个IPC系统都为通信中的每个进程公开一个文件描述符。 fcntl调用对该文件描述符执行不同的操作，使它们成为非阻塞的（因此，如果没有可读取的内容，则读取操作会立即返回）。 ioctl调用遵循相同的模式，但实现特定于设备的操作，例如发送命令。</p>
<p>共享内存是我们要介绍的最后一个IPC方法。不同于提供一个进程间通讯的channel，共享内存使用进程的内存区域指向相同的内存页面，因此一个进程覆盖了这部分内存的修改也会影响其他进程之后的读操作。</p>
<h3 id="QEMU-and-device-emulation"><a href="#QEMU-and-device-emulation" class="headerlink" title="QEMU and device emulation"></a>QEMU and device emulation</h3><p>QEMU是一个host层的虚拟机模拟器，给guest提供了一系列不同的硬件和设备模型。对host来说，qemu是一个标准Linux可调度的标准进程，有自己的进程内存。在进程里QEMU分配了内存区域来给guest当作物理内存么，同时QEMU还要执行虚拟机的CPU指令</p>
<p>为了在裸机设备上执行I/O操作么，比如存储和网络，CPU必须给物理设备下发特殊的指令并访问特殊的内存区域，比如这个设备被映射到的内存区域</p>
<p>当guest访问这些内存区域的时候，控制面就返回到了执行设备透明模拟的guest的QEMU里</p>
<h3 id="KVM"><a href="#KVM" class="headerlink" title="KVM"></a>KVM</h3><p>Kernel-based Virtual Machine(KVM)是一个Linux内置的开源虚拟化技术。它为虚拟化软件提供硬件辅助，利用内置CPU虚拟化技术减少虚拟化开销（缓存、I/O、内存），提高安全性。</p>
<p>使用KVM，QEMU可以创建一个虚拟机，该虚拟机具有处理器识别的虚拟 CPU (vCPU)，运行native-speed指令。当特殊的比如需要和设备或者特殊内存交互的命令到达KVM的时候，vCPU会停下来，然后通知QEMU暂停的原因，然后hypervisor就会对这个事件作出反应了。</p>
<p>在常规的KVM操作里，hyervisor会打开/dev/kvm这个设备，然后通过ioctl和他通讯调用它创建虚拟机，增加CPU，增加内存（qemu分配，但是虚拟机看来是物理内存），发送CPU中断（外部设备引发的），等等。举个例子，某一个ioctl的命令运行了KVM vCPU，阻塞了QEMU而且vCPU需要等到它找到了需要硬件辅助的命令。那时ioctl就会返回（这个叫做vmexit）同时QEMU也能知道这个exit的原因（比如offending instruction)。</p>
<p>对一些特别的内存区域，KVM有类似的访问方式，把内存区域标记为只读或者完全不映射，通过KVM_EXIT_MMIO造成一个vmexit。</p>
<h3 id="The-virtio-specification"><a href="#The-virtio-specification" class="headerlink" title="The virtio specification"></a>The virtio specification</h3><h4 id="Virtio-specification-devices-and-drivers"><a href="#Virtio-specification-devices-and-drivers" class="headerlink" title="Virtio specification: devices and drivers"></a>Virtio specification: devices and drivers</h4><p>Virtio是一个虚拟机数据I/O的一个开放规范，提供了简单、有效、标准并且可拓展的虚拟设备机制，而不是固定在在每个环境或每个操作系统的机制。它主要基于guest可以和host共享内存以进行I/O来实现。</p>
<p>virtio规范基于两个元素：设备和驱动。在最经典的实现里，hypervisor通过一系列传输方法试将virtio设备暴露给guest。设计上他们在虚拟机内看起来是物理设备。</p>
<p>嘴常见的传输方法就是PCI或者PCIe总线。然而，这些设备在一些预定义好的guest内存地址是可用的（MMIO transport）。这些设备可以完全在没有物理设备或者是物理的兼容性接口的情况下被虚拟出来。</p>
<p>最典型最简单的暴露virtio设备的方法是通过PCI端口因为我们可以借用PCI已经是一个成熟并且在QEMU和Linux驱动自持的很好的协议。实际的PCI硬件通过特殊的物理内存地址范围和/或特殊的处理器指令暴露配置空间（比如，驱动可以通过这些内存范围读或者写设备的寄存器）。在虚拟机世界里，hypervisor能捕获访问这些内存范围并且执行设备模拟，暴露和真实设备相同的内存布局，并提供相同的返回。virtio标准也定义了PCI配置空间的布局，因此实现它是很简单的。</p>
<p>当guest驱动并使用PCI/PCI自动发现机制的时候，virtio设备通过PCI vendor ID和他们的PCI device ID标识自己。guest kernel通过使用这些标记来知道使用哪些驱动来处理这些设备。特别是，linux内核已经包含了virtio驱动程序。</p>
<p>virtio驱动必须能够分配给hypervisor和设备都能读写的内存区域，比如通过共享内存。我们把数据平面作为使用内存区域进行数据通讯的一个部分，同时控制平面主要是来配置他们。我们会在后续的文章里提供一个更深层次的virtio协议的实现细节以及内存布局。</p>
<p>virtio内核驱动共享了一个通用的传输专用接口（virtio-pci），并被用于实际的传输以及设备实现（比如virtio-net，virtio-scsi）</p>
<h4 id="Virtio-specification-virtqueues"><a href="#Virtio-specification-virtqueues" class="headerlink" title="Virtio specification: virtqueues"></a>Virtio specification: virtqueues</h4><p>Virtqueues是virtio设备的批量数据传输机制。每个设备可以有0个或者多个virtqueue <a href="xxxx">link</a>。它由guest分配的host可以访问并且可以读或者写的缓存队列组成。补充一下，virtio标准也定义了双向的通知：</p>
<ul>
<li>Available Buffer Notification：使用驱动来通知buffer就绪并可以被设备处理</li>
<li>Used Buffer Notification：被设备使用来通知已经处理完了一些buffers</li>
</ul>
<p>在PCI的场景，guest通过写一个特殊的内存地址发送available buffer notification，然后设备（这个场景里是QEMU）使用vCPU中断来发送used buffer notification。</p>
<p>virtio规范也允许notifications动态的启用或者停用。这种情况下设备和驱动可以批量的缓存通知或者主动的向virtqueues请求新的缓存。这个方法更适合高流量的场景。</p>
<p>总结一下，virtio驱动接口暴露了一下内容：</p>
<ul>
<li>Device’s feature bits（设备和guest需要协商的部分）</li>
<li>Status bits（状态位）</li>
<li>Configuration space（包含设备特殊信息，比如MAC地址）</li>
<li>Notification system（配置变更，缓存可用，缓存使用）</li>
<li>Zero or more virtqueues</li>
<li>Transport specific interface to the device</li>
</ul>
<h3 id="Networking-with-virtio-qemu-implementation"><a href="#Networking-with-virtio-qemu-implementation" class="headerlink" title="Networking with virtio: qemu implementation"></a>Networking with virtio: qemu implementation</h3><img src="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/2019-09-12-virtio-networking-fig1.png" class="">

<p><em>Figure 1: virtio-net on qemu</em></p>
<p>virtio网络设备是一个虚拟以太网卡，支持TX/RX（发送，接收）多队列。空的缓存被放在N virtqueues里用来接收数据包，往外送的数据包责备放到另外一个N virtqueues里面等待发送。另一个virtqueue被用于数据面之外的驱动和设备的通讯，比如控制高级过滤特性，设置mac地址，或者是一堆活跃的队列。像物理网卡一样，virtio设备支持很多特性比如offloading，能够让真实的host设备来处理。</p>
<p>为了发送数据包，驱动会发送给设备一个缓存包括metadata信息比如在要发送的packet frame上带有的数据包期望的offloading。驱动能够将这个缓存拆分成不同的条目，比如可以把这个metadata的header从packet frame上分离出来。</p>
<p>这些缓存被驱动管理，被映射给设备。因此这个情况下我们可以说这个设备实际上在hypervisor里（结合前面提到hypervisor）因为qemu能够访问所有的guest内存，所以有能力知道缓存的位置并能够对他们进行读写。</p>
<p>下面的流程图表示了virtio-net设备配置和使用virtio-net驱动发送数据包通过PCI和virtio-net设备通讯。在组装好要发送的数据包之后，出发了一个available buffer notification，把控制返回给QEMU然后它就能够把包通过TAP设备送出去</p>
<p>QEMU然后通知guest这些缓存操作执行完成了（读或者写）并且它通过把这些数据放到virtqueue然后发送一个used notification event来触发guest的vCPU中断。</p>
<p>接收数据包的过程和发送类似。唯一不同的是在接收数据包的场合，空缓存会提前被guest分配出来然后提供给设备一个可用缓存保证它能够把即将收到的数据写进去。</p>
<img src="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/2019-09-12-virtio-networking-fig2.png.jpg" class="">

<p><em>Figure 2: Qemu virtio sending buffer flow diagram</em></p>
<h2 id="Vhost-net"><a href="#Vhost-net" class="headerlink" title="Vhost-net"></a>Vhost-net</h2><p>vhost-net是一个内核驱动，实现了vhost协议的处理侧，用来实现一个高效的数据面。比如数据包转发，在这个实现里，qemu和vhost-net内核驱动（handler）使用ioctls来交换vhost消息和一大批叫做irqfd的类似eventfd的文件描述符然后ioeventfs被用来和guest交换通知</p>
<p>当vhost-net驱动被加载的时候，它会在/dev/vhost-net暴露一个字符设备。当qemu启用了vhost-net并启动后，它会打开这个设备并且一些ioctl调用初始化vhost-net实例。这是把vhost-net和hypervisor联系起来必不可少的步骤，准备virtio特性检查，然后给guest提供映射到vhost-net驱动的内存。</p>
<p>在初始化的过程里vhost-net驱动创建了一个内核线程叫做vhost-$pid这个$pid就是hypervisor（也就是qemu进程）的pid。这个线程也被叫做“vhost worker thread”</p>
<p>tag设备仍然被用于VM和host的通讯但是现在这个worker thread处理了这些I/O事件，比如它会不断poll驱动的通知或者是tap的事件并且做数据的转发。</p>
<p>Qemu分配了一个eventfd然后注册了到了vhost和KVM来实现通知的传递。vhost-pid内核线程poll这个eventfd，当guest写某一个特殊地址的时候KVM则会写这个eventfd。这个机制被叫做ioeventfd。用这个方法，对一个特别的guest内存地址简单的读写操作不需要再穿过钢轨的QEMU进程唤醒同时能够被直接路由到vhost worker thread。这也提供了异步的优势，也不再需要vCPU停下来（因此没必要立刻做上下文切换）</p>
<p>另一方面，qemu分配了另外的eventfd并再次注册他们到KVM和vhost来处理vCPU的直接中断注入。这个机制又叫irqfd，他们允许host通过写irqfd来注入vCPU中断到guest。同时也具有异步特性，不需要立刻做上下文切换</p>
<p>注意这些改动对virtio包处理后端对guest来说是完全透明的，guest仍然使用的是标准的virtio接口</p>
<p>下面的图展示了qemu数据路径的offloading到vhost-net内核驱动：</p>
<img src="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/2019-09-12-virtio-networking-fig3.png" class="">

<p><em>Figure 3: vhost-net block diagram</em></p>
<img src="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/2019-09-12-virtio-networking-fig4.png" class="">

<p><em>Figure 4: vhost-net sending buffer diagram flow</em></p>
<h3 id="Communication-with-the-outside-world"><a href="#Communication-with-the-outside-world" class="headerlink" title="Communication with the outside world"></a>Communication with the outside world</h3><p>guest能够和host通过tap设备通讯，然而还有一个遗留的问题就是guest如何和同一个host伤的其他vm或者是其他host上的vm通讯（使用internet通讯）</p>
<p>我们可以内核网络协议栈提供的转发和路由的机制，比如标准的Linux bridges。然而一个更加高级的解决方式就是一个全虚拟化的分发，管理交换机比如Open Virtual Switch</p>
<p>就像在总览篇说的一样，OVS的数据路径在这个场景里是作为内核模块运行的，ovs-switchd是一个用户态的控制管理守护进程然后ovsdb-server是一个转发数据库。</p>
<p>就像图上画的一样，OVS的数据路径在kernel运行然后在物理网卡和虚拟TAP设备间做包的forward：</p>
<img src="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/2019-09-12-virtio-networking-fig5.png" class="">

<p><em>Figure 5: Introduce OVS</em></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在这篇文章里，我们展示了virtio-net的架构是如何工作的，我们对每一个步骤做了详细的解剖并解释了每个组件的功能。</p>
<p>我们开始解释了默认的qemu IO设备如何通过提供给guest一个开放virtio标准的实现来运作的。我们接下来分析了guest如何和这些设备通过virtio驱动能够发送和接收数据包，发送和接收通知的</p>
<p>然后我们评价了在数据路径中有qemu的情况下需要切换上下文，然后展示了如何在host上使用vhost协议通过vhost-net内核驱动offload这些任务。我们也能够覆盖virtio通知在这个新设计下是如何工作的</p>
<p>最后我们展示了如何将VM连接到外部的非自己所在的host的世界。</p>
<p>在接下来的文章里我们将会提供一个使用之前学习到的解决方案里的不同组件完成关于vhost-net/virtio-net的架构实现。</p>
<p>如果你因为什么原因跳过了那篇文章，我们将在下一篇文章介绍一个新的用户态的使用DPDK的vhost处理协议。我们会列举他的优点，使用DPDK和用户态的观点，我们将建立第二个符合这些概念的架构。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2021/12/22/deep-dive-virtio-networking-and-vhost-net/" data-id="cld1mhea7000byuwb5tqr1j5u" data-title="deep dive virtio networking and vhost net" class="article-share-link">Share</a>
      
      
        <a href="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2021/12/22/deep-dive-virtio-networking-and-vhost-net/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/architecture/" rel="tag">architecture</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/qemu/" rel="tag">qemu</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vhost-net/" rel="tag">vhost-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-net/" rel="tag">virtio-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-networking/" rel="tag">virtio-networking</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-introduction-virtio-networking-and-vhost-net-md" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/12/15/introduction-virtio-networking-and-vhost-net-md/" class="article-date">
  <time class="dt-published" datetime="2021-12-15T06:42:40.000Z" itemprop="datePublished">2021-12-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/virtualization/">virtualization</a>►<a class="article-category-link" href="/categories/virtualization/translation/">translation</a>►<a class="article-category-link" href="/categories/virtualization/translation/virtio-networking/">virtio-networking</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/12/15/introduction-virtio-networking-and-vhost-net-md/">Introduction to virtio-networking and vhost-net</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>因为最近在看vdpa-blk，查资料的时候发现了红帽在19年写了一系列文章来介绍virtio网络入门的感觉和存储的加速思路差不多就拿来整理顺便做一下翻译。原文链接已附在文尾。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>virtio被开发作为云主机简化访问类似块设备和网络适配器的标准开放api。其中的virtio-net是一个虚拟以太网卡，是目前virtio支持过的最复杂的设备。本文会从virtio-networking的架构提供一个高级的基于建立host kernel和VM guest kernel的接口的解决方案。将介绍包括 KVM、qemu 和 libvirt 在内的基本组件，同时结合virtio spec和vhost协议以及Open vSwitch来讲云主机连接到外部世界。接下来描述的基于vhost-net/virtio-net的架构是许多virtio-networking架构中的第一个，这些架构将在一系列帖子中介绍，这些帖子因其性能、应用程序的易用性和实际部署而异。</p>
<p>读完这篇文章之后你应该能够清楚的了解提到的名词然后能够知道云主机中的应用如何发送数据包到其他云主机运行的应用以及如何发送到外部网络。这些术语也会作为下一片文章的基础。</p>
<h2 id="VIrtio-basic-building-blocks"><a href="#VIrtio-basic-building-blocks" class="headerlink" title="VIrtio basic building blocks"></a>VIrtio basic building blocks</h2><p>guest VM或者说guest是被安装，执行并托管在物理机器上的虚拟机。这个提供托管guest VM并提供资源功能的机器就叫做host。guest通过hypervisor拥有分离的运行在host操作系统之上的操作系统。举个例子，host会提供虚拟网卡给虚拟机，guest机器则把它当作一个真实的网卡使用，虽然事实上它只是个虚拟网卡。</p>
<p>下列的组件创建了接下来virtio需要提供链接的虚拟环境</p>
<ol>
<li>KVM - kernel based virtiual machine允许Linux提供hypervisor的功能支持运行多个隔离的虚拟环境又叫做guests。KVM提供了Linux作为hypervisor的基本功能。比如内存管理，调度，网络栈等都包含在了Linux kernel里。虚拟机通常是通过标准Linux调度去调度的进程，同时上面加在了一些专用的虚拟硬件比如网络适配器。</li>
<li>QEMU - A hosted virtual machine monitor。通过模拟给虚拟机提供一系列硬件和设备模型。QEMU 可以与 KVM 一起使用，利用硬件扩展以接近本机的速度运行虚拟机。guest通过qemu command line interface(CLI)运行。CLI 供了为QEMU指定所有必要配置选项的能力</li>
<li>Libvirt - 一个翻译xml格式配置到qemu CLI调用的接口。它提供了一个admin守护进程来配置子进程比如qemu，因此qemu不需要root特权。例如，当Openstack Nova想要启动一个VM时，它使用libvirt通过为每个VM调用一个qemu进程来为每个VM启动一个qemu进程。</li>
</ol>
<p>下图显示了这三个组件如何结合在一起：</p>


<p>host和guest都包含了kernel space和user space。可以根据图看到，KVM运行在host的内核态，而libvirt运行在host的用户态</p>
<p>guest虚拟机运行在qemu进程里，只是一个简单的运行在host用户态的继承并且和libvirt（用户态应用）以及KVM（内核态）通信</p>
<p>每一个虚拟机都会创建一个qemu进程，所以如果你创建N个虚拟机，你就会有N个虚拟机qemu进程，并且libvirt会和每一个进程通信</p>
<h2 id="Virtio-spec-and-the-vhost-protocol"><a href="#Virtio-spec-and-the-vhost-protocol" class="headerlink" title="Virtio spec and the vhost protocol"></a>Virtio spec and the vhost protocol</h2><p>当讨论vritio-networking的时候我们可以分开两层讨论</p>
<ol>
<li>控制平面 - 用于host和guest之间能力的协商，用于建立和终止数据平面</li>
<li>数据平面 - 用于host和guest之间传输实际的数据（packets）</li>
</ol>
<p>区分清楚这两个层级是很重要的，因为这些层级需要实现的要求不一样（比如性能）并且本文和后续文章也会展示很多不同的实现。</p>
<p>不过在以后的架构里最基础的要求，数据平面要求有尽可能好的性能来快速移动数据包，同时控制平面要求尽可能的有很好的拓展性来支持不同的设备和厂商</p>
<p>就像在开头提到的，virtio是被开发出来作为guest访问host设备的接口的。我们可以把virtio分成两部分：</p>
<ol>
<li>virtio spec - The virtio spec根据OASIS提到的。定义了如何创建一个介于host和guest之间的控制平面和数据平面。举个例子数据平面需要由缓存和环状布局组成，设置在spec里面详细说明的</li>
<li>vhost protocol - 一个为了提升性能允许virtio数据平面的实现被下放到其他元素（用户态进程或者内核模块）</li>
</ol>
<p>qemu进程中实现的virtio的控制平面是基于virtio spec的，然而数据平面不是。因此问题转变为了为什么数据平面没有像类似virtio spec中定义的那样实现在qemu进程里的？</p>
<p>答案是：性能问题</p>
<p>如果简单的按照virtio spec在qemu里实现了数据平面，我们就需要为每个从kernel进入guest的包切换上下文了反之亦然。这是一个代价很大的操作，增加了延迟同时也占用了更多的进程时间（提醒一下当前qemu只是一个Linux进程），所以我们想要尽可能的避免这个问题。</p>
<p>这就是为什么vhost protocol出现了，让我们能够去实现一个数据平面直接从host kernel到guest而不经过qemu进程</p>
<p>不过vhost协议本身仅描述了如何建立数据平面。任何实现被要求实现环状布局来描述数据缓存（包括host和guest的）以及真实发送/接受的数据包</p>
<p>就像之后的章节要介绍的内容那样，vhost协议可以被实现成内核态（vhost-net）或者是用户态（vhost-user）。本文描述的vhost-net/virtio-net架构专注于内核态的又被叫做vhost-net的实现</p>
<h2 id="The-vhost-net-virtio-net-architecture"><a href="#The-vhost-net-virtio-net-architecture" class="headerlink" title="The vhost-net/virtio-net architecture"></a>The vhost-net/virtio-net architecture</h2><p>当我们谈论关于virtio接口的时候我们有一个后端组件和一个前端组件</p>
<ul>
<li>后端组件是host侧的virtio接口</li>
<li>前端组件是guest侧的virtio接口</li>
</ul>
<p>在vhost-net/virtio-net架构的组件里是这样的</p>
<ul>
<li>vhost-net是运行在host内核空间</li>
<li>virtio-net运行在guest的内核空间</li>
</ul>
<p>下面的图展示了宏观上virtio的前端和后端是如何对应的：</p>


<p>有一些点需要说明：</p>
<ul>
<li>因为vhost-net和virtio-net都运行在host和guest的内核空间，我们也叫他们驱动（drivers）所以如果有人“vhost-net-driver”不太要感觉奇怪指的是同一个东西</li>
<li>在后端和前端之间有分离的控制平面和数据平面。就像前面解释过的，控制平面简单给vhost-net内核模块实现了virtio spec同时qemu进程进入guest的通讯最终到了virtio-net。Vhost-net使用了vhost协议建立了框架实现了使用共享内存的数据平面的host-guest内核包的直接转发的功能</li>
</ul>
<p>实际上数据面通讯  接收 receive（RX）和发送 transmit（TX）是通过vCPU专用队列完成的</p>
<p>每一个guest都能根据vCPU的数量关联起一堆RX/TX队列，这个队列对应每一个CPU。举一个更精确的例子比如有4 vCPUs会像这样（去掉控制平面简化一下这个图）：</p>


<h2 id="Virtio-networking-and-OVS"><a href="#Virtio-networking-and-OVS" class="headerlink" title="Virtio-networking and OVS"></a>Virtio-networking and OVS</h2><p>目前我们已经描述了如何guest能够通过virtio-networking接口把包送到host内核。为了转发这些包到其他运行在同一台host或者其他host的guest上，我们使用OVS</p>
<p>OVS是一个软交换机，能够给提供内核内包转发。它由用户态和内核态两个部分组成</p>
<ul>
<li>用户空间 - 包括一个数据库（ovsdb-server）和一个OVS deamon来管理和控制交换机（ovs-switched）</li>
<li>内核空间 - 包括ovs内核模块主要负责数据通路和转发平面</li>
</ul>
<p>OVS controller同时和数据库服务以及内核转发面通讯。为了使得包能够出入OVS我们使用了Linux端口。在我们的场景里，我们使用了一个端口连接OVS内核转发平面到一个物理网卡同时另外一个端口连接到vhost-net的后端。</p>
<p>注意我们是在描述简化过的场景。实际上可能会有多个网卡连接到OVS多个端口和虚拟机运行，。因此也需要很多端口连接到vhost-net的后端。</p>
<p>接着讲virtio-networking下面的图展示了OVS如何连接到virtio：</p>


<p>请注意所提到的用于将OVS连接到主机外部和vhost-net以及从vhost-net连接到virtio-ne 和在VM中运行的应用程序的端口。</p>
<p>这个总结了vhost-net/virio-net的基于host内核，guest内核和内核态OVS的架构的大概内容</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在这篇文章中，我们已经触及了 virtio-networking 生态系统的表面，向您介绍了 virtio-networking 使用的虚拟化和网络的基本构建组件。我们简短的涵盖了virtio spec和vhost protocol，回顾了用于实现 virtio 接口的前端和后端架构，并带您了解了 vhost-net（host内核）与 virtio-net（guest内核）通信的 vhost-net/virtio-net 架构。</p>
<p>我们在尝试解释事物时遇到的一个基本挑战是术语历史问题。例如，virtio-net既指virtio规范中的virtio网络设备实现，也指vhost-net/virtio-net架构中描述的guest内核前端。我们试图通过在上下文解释术语并使用virtio-net仅描述guest内核前端来解决这个问题。</p>
<p>正如将在后面的文章中解释的那样，基于使用DPDK和不同硬件offload技术的virtio规范网络设备还有其他实现，这些都在virtio-networking的保护伞下。</p>
<p>接下来的两篇文章会提供一个更加深入的关于vhost-net/virtio-net架构的理解。一篇文章将面向架构师，提供对vhost-net/virtio-net的技术深入研究，并解释数据平面和控制平面在实践中是如何实现的。面向开发人员的另一篇文章将是一个动手部分，包括Ansible脚本，以实现对vhost-net/virtio-net架构的试验。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>原文链接： <a target="_blank" rel="noopener" href="https://www.redhat.com/en/blog/introduction-virtio-networking-and-vhost-net">introduction-virtio-networking-and-vhost-net</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2021/12/15/introduction-virtio-networking-and-vhost-net-md/" data-id="cld1mhea9000fyuwb4r8l2iat" data-title="Introduction to virtio-networking and vhost-net" class="article-share-link">Share</a>
      
      
        <a href="/2021/12/15/introduction-virtio-networking-and-vhost-net-md/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2021/12/15/introduction-virtio-networking-and-vhost-net-md/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/qemu/" rel="tag">qemu</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/vhost-net/" rel="tag">vhost-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-net/" rel="tag">virtio-net</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/virtio-networking/" rel="tag">virtio-networking</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-qemu-ft-01" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/12/14/qemu-ft-01/" class="article-date">
  <time class="dt-published" datetime="2021-12-13T16:00:58.000Z" itemprop="datePublished">2021-12-14</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/virtualization/">virtualization</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/12/14/qemu-ft-01/">QEMU FT方案</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>因为对qemu的fault tolerance技术颇有兴趣，在查询资料时找到了一个IBM实现的MicroCheckpointing方案实现的ft查看文档并整理了一下这个ft方案的原理</p>
<h2 id="MicroCheckpointing"><a href="#MicroCheckpointing" class="headerlink" title="MicroCheckpointing"></a>MicroCheckpointing</h2><p>基本概念 Micro-Checkpoints (MC)  默认工作在QEMU虚拟机热迁移的逻辑上，可以被简单的理解为“在执行不会结束的热迁移”。比如，在一个确定的周期譬如0.01s去的循环里去执行如下逻辑：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. 在N ms之后stop vm（paused）</span><br><span class="line">2. 通过调用live migration的逻辑生成一个MC用于标识dirty memory并写入到qemu本地的暂存区 </span><br><span class="line">3. Resuse vm</span><br><span class="line">4. 传输MC到目标节点</span><br><span class="line">5. 重复1</span><br></pre></td></tr></table></figure>

<p>当发生failure的情况，目标节点的机器用最新的MC恢复guest</p>
<p>这里涉及的几个问题</p>
<h3 id="MC本身的设备I-O一致性"><a href="#MC本身的设备I-O一致性" class="headerlink" title="MC本身的设备I/O一致性"></a>MC本身的设备I/O一致性</h3><p>对qemu guest来说，设备I/O的一致性主要包括网络设备和存储设备</p>
<p>即在这N ms后vm stop，这段时间里，内部往外的网络包都会被cache住，直到MC被传输结束</p>
<p>存储设备则需要保证MC传输结束这段时间磁盘内容也被同步了，如果本来使用的就是共享磁盘则问题不大，同理也是需要磁盘I/O在MC之间也是被cache住的</p>
<p>这里有一点比较奇怪，就是既然vm已经处于stop状态，为什么还需要阻止网络包往外发（实际上应该没有网络包了）不过可以理解的是，这个目的是出于failure后恢复的考虑，这样恢复到目标节点之后，这些网络包都会被回复掉，保证MC的一致性</p>
<p>出于这些一致性考虑，MC的步骤需要作出如更改：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1. Insert a new Qdisc plug (Buffer A).</span><br><span class="line">2. 在N ms之后stop vm（paused）</span><br><span class="line">3. 通过调用live migration的逻辑生成一个MC用于标识dirty memory并写入到qemu本地的暂存区 </span><br><span class="line">4. Insert a *new* Qdisc plug (Buffer B). This buffers all new packets only.</span><br><span class="line">5. 立刻回复VM并继续往下运行保持继续工作 (making use of Buffer B).</span><br><span class="line">6. 传输MC到目标节点</span><br><span class="line">7. 等到传输结果被确认</span><br><span class="line">8. 确认传输成功</span><br><span class="line">9. Release the Qdisc plug for Buffer A.</span><br><span class="line">10. Qdisc Buffer B now becomes (symbolically rename) the most recent Buffer A</span><br><span class="line">11. 继续步骤2</span><br></pre></td></tr></table></figure>

<p>根据以上步骤，我们可以知道一个recent Buffer里面会buffer的数据包是上一次MC创建的时候VM resume开始到下一个MC传输完成的数据包，这个逻辑为什么能够保证网络一致呢</p>
<p>举一个具体的例子 比如当前VM在Buffer A里面保存的数据包P1，然后第一次执行MC得到了MC1，然后VM继续运行，创建了Buffer B来拦截新的数据包，到MC1传输完成之后P1被送出去了，Buffer A被清空，而这段时间里面由Buffer B拦截了P2，这个时候如果发生了Fail，目标的机器会恢复到MC1的状态，而因为Buffer B拦截了P2可知，MC1继续运行会发送P2，因此网络连接可以继续进行</p>
<h3 id="Failure-Recovery"><a href="#Failure-Recovery" class="headerlink" title="Failure Recovery"></a>Failure Recovery</h3><p>基于micro-checkpointing高频率的特性，每秒都会生成多个MC。即使错过了个别MC也没有关系，因为I/O Buffer保证了在下一个MC被传输完成之后才会继续运行。</p>
<p>因此判断出错的有以下两种情况：</p>
<ol>
<li><p>MC over TCP/IP: 一旦socket连接断开. 这问题可能出现在传输最后一个MC的时候流量太大或者是确认MC传输成功的请求超时等。</p>
</li>
<li><p>MC over RDMA: 因为无限带宽的逻辑没有提供任何底层的超时机制，这个实现给QEMU的RDMA migration protocol 增加了简单的keep-alive。如果发生了多少次keep-alive消息的丢失则认为发生了fail。</p>
</li>
</ol>
<p>在这两种情况下主备两端都能够通过一样的机制判断对方是不是挂掉了</p>
<p>如果主节点被判断挂掉了，备节点就会使用最近的MC恢复启动</p>
<p>如果备节点被判断挂掉了，就执行和live migrate相同的逻辑重新启动一个备份节点</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这个方案是一个很好的利用live migrate实现的ft方案</p>
<p>尤其是里面提到的如何保证网络一致，以及通过Qdisc保证网络包顺序，这个在云主机热迁移本身就有应用，即（paused时作为数据包的buffer）避免出现网络断开的情况</p>
<p>同时也解释了为什么恢复checkpoint之后依旧可以有连续的网络，当然考虑到tcp/udp的时候，这里可能对tcp来说更加友好，对使用udp的应用来说排队和保证顺序意义不那么大了</p>
<p>其次是创建checkpoint的时候需要保证I/O一致性需要不停的paused，并且要保证IO请求都sync，感觉也是很麻烦的事情</p>
<p>之后在抽时间看看Qdisc以及qemu虚拟机的IO请求路径，希望能对虚拟化的原理有更好的认识</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol>
<li><a target="_blank" rel="noopener" href="https://wiki.qemu.org/Features/MicroCheckpointing">https://wiki.qemu.org/Features/MicroCheckpointing</a></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2021/12/14/qemu-ft-01/" data-id="cld1mheau0014yuwb8dke0ejw" data-title="QEMU FT方案" class="article-share-link">Share</a>
      
      
        <a href="/2021/12/14/qemu-ft-01/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2021/12/14/qemu-ft-01/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ft/" rel="tag">ft</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/qemu/" rel="tag">qemu</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-new-start" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/12/13/new-start/" class="article-date">
  <time class="dt-published" datetime="2021-12-13T13:54:10.000Z" itemprop="datePublished">2021-12-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/12/13/new-start/">新的博客内容</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>新的博客会开始写一些技术文章，把最近研究过的东西整理上传</p>
<p>希望未来的一年能多做学习积累更进一步</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://hanayo.cn/2021/12/13/new-start/" data-id="cld1mheap000xyuwbfc8i3yok" data-title="新的博客内容" class="article-share-link">Share</a>
      
      
        <a href="/2021/12/13/new-start/#comments" class="article-comment-link">
          <span class="post-comments-count valine-comment-count" data-xid="/2021/12/13/new-start/" itemprop="commentCount"></span>
          Comments
        </a>
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/3/">&laquo; Prev</a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/arch-notes/">arch-notes</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/devops/">devops</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/languages/">languages</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/languages/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/languages/python/">python</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/linux/memory-management/">memory management</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/management/">management</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project-related-works/">project-related-works</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/">virtualization</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/edk2-ovmf/">edk2-ovmf</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/kvm/">kvm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/libvirt/">libvirt</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/translation/">translation</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/translation/virtio/">virtio</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/translation/virtio-networking/">virtio-networking</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/v2v/">v2v</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/virtualization/virtio-balloon/">virtio-balloon</a></li></ul></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/BSOD/" rel="tag">BSOD</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DPDK/" rel="tag">DPDK</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ElementTree/" rel="tag">ElementTree</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TDP/" rel="tag">TDP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TLB/" rel="tag">TLB</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/architecture/" rel="tag">architecture</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/code-reading/" rel="tag">code-reading</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cpu/" rel="tag">cpu</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/edk2-ovmf/" rel="tag">edk2-ovmf</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ft/" rel="tag">ft</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/interview/" rel="tag">interview</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/" rel="tag">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kernel/" rel="tag">kernel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kvm/" rel="tag">kvm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/libvirt/" rel="tag">libvirt</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/live-migration/" rel="tag">live-migration</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/maven/" rel="tag">maven</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nessus/" rel="tag">nessus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nexus/" rel="tag">nexus</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/others/" rel="tag">others</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/paper-reading/" rel="tag">paper-reading</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/perf/" rel="tag">perf</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/qemu/" rel="tag">qemu</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/reading-notes/" rel="tag">reading notes</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/security/" rel="tag">security</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/software-arch/" rel="tag">software-arch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/system-design/" rel="tag">system-design</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/v2v/" rel="tag">v2v</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vDPA/" rel="tag">vDPA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/vhost-net/" rel="tag">vhost-net</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/virt/" rel="tag">virt</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/virtio/" rel="tag">virtio</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/virtio-balloon/" rel="tag">virtio-balloon</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/virtio-net/" rel="tag">virtio-net</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/virtio-networking/" rel="tag">virtio-networking</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/windows/" rel="tag">windows</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/BSOD/" style="font-size: 10px;">BSOD</a> <a href="/tags/DPDK/" style="font-size: 12.86px;">DPDK</a> <a href="/tags/ElementTree/" style="font-size: 10px;">ElementTree</a> <a href="/tags/TDP/" style="font-size: 11.43px;">TDP</a> <a href="/tags/TLB/" style="font-size: 10px;">TLB</a> <a href="/tags/architecture/" style="font-size: 18.57px;">architecture</a> <a href="/tags/code-reading/" style="font-size: 10px;">code-reading</a> <a href="/tags/cpu/" style="font-size: 10px;">cpu</a> <a href="/tags/edk2-ovmf/" style="font-size: 10px;">edk2-ovmf</a> <a href="/tags/ft/" style="font-size: 10px;">ft</a> <a href="/tags/interview/" style="font-size: 10px;">interview</a> <a href="/tags/java/" style="font-size: 12.86px;">java</a> <a href="/tags/kernel/" style="font-size: 12.86px;">kernel</a> <a href="/tags/kvm/" style="font-size: 15.71px;">kvm</a> <a href="/tags/libvirt/" style="font-size: 11.43px;">libvirt</a> <a href="/tags/linux/" style="font-size: 15.71px;">linux</a> <a href="/tags/live-migration/" style="font-size: 10px;">live-migration</a> <a href="/tags/maven/" style="font-size: 10px;">maven</a> <a href="/tags/nessus/" style="font-size: 10px;">nessus</a> <a href="/tags/nexus/" style="font-size: 10px;">nexus</a> <a href="/tags/others/" style="font-size: 10px;">others</a> <a href="/tags/paper-reading/" style="font-size: 10px;">paper-reading</a> <a href="/tags/perf/" style="font-size: 10px;">perf</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/qemu/" style="font-size: 20px;">qemu</a> <a href="/tags/reading-notes/" style="font-size: 10px;">reading notes</a> <a href="/tags/security/" style="font-size: 10px;">security</a> <a href="/tags/software-arch/" style="font-size: 12.86px;">software-arch</a> <a href="/tags/system-design/" style="font-size: 12.86px;">system-design</a> <a href="/tags/v2v/" style="font-size: 10px;">v2v</a> <a href="/tags/vDPA/" style="font-size: 10px;">vDPA</a> <a href="/tags/vhost-net/" style="font-size: 17.14px;">vhost-net</a> <a href="/tags/virt/" style="font-size: 14.29px;">virt</a> <a href="/tags/virtio/" style="font-size: 15.71px;">virtio</a> <a href="/tags/virtio-balloon/" style="font-size: 10px;">virtio-balloon</a> <a href="/tags/virtio-net/" style="font-size: 17.14px;">virtio-net</a> <a href="/tags/virtio-networking/" style="font-size: 17.14px;">virtio-networking</a> <a href="/tags/windows/" style="font-size: 10px;">windows</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">February 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">November 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/10/">October 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/07/">July 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">June 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">April 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">December 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/03/01/cpu-feature-configuration-code-diving/">Cpu feature configuration code diving</a>
          </li>
        
          <li>
            <a href="/2023/02/23/packed-virtqueue-how-to-reduce-overhead-with-virtio/">Packed virtqueue: How to reduce overhead with virtio</a>
          </li>
        
          <li>
            <a href="/2023/02/22/virtqueues-and-virtio-ring-how-the-data-travels/">Virtqueues and virtio ring: How the data travels</a>
          </li>
        
          <li>
            <a href="/2023/02/22/virtio-devices-and-drivers-overview-the-headjack-and-the-phone/">Virtio devices and drivers overview: The headjack and the phone</a>
          </li>
        
          <li>
            <a href="/2023/02/03/windows-install-virtio-then-reboot-met-BSOD/">Windows install virtio then reboot met BSOD</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
        <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a></br>
      
      &copy; 2023 Alan Jager<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  
<script src="https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js"></script>

<script>
    var GUEST_INFO = ['nick','mail','link'];
    var guest_info = 'nick,mail,link'.split(',').filter(function(item){
        return GUEST_INFO.indexOf(item) > -1
    });
    var notify = '' == true;
    var verify = 'false' == true;
    new Valine({
        el: '.vcomment',
        notify: notify,
        verify: verify,
        appId: "r30r51B3r5JFqlxR88Jua6So-gzGzoHsz",
        appKey: "wnL9j38siXbLqBHGnWpzmVxv",
        placeholder: "Just go go",
        pageSize:'10',
        avatar:'mm',
        lang:'zh-cn'
    });
</script>

  </div>
</body>
</html>